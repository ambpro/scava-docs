{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the Scava documentation\n\n\nThis web site is the main documentation place for the \nEclipse Scava\n project.\n\n\nUseful links:\n\n\n\n\nEclipse Scava home project: \nEclipse Scava @ Eclipse\n\n\nEclipse Scava code repository: \ngithub.com/crossminer/scava\n\n\nEclipse Scava documentation repository: \ngithub.com/crossminer/scava-docs\n\n\n\n\nPlatform installation\n\n\n\n\nDocker-SCAVA\n How to build and run the Scava docker image.\n\n\nRunning the platform\n Quick start guide to get the Scava platform running from source on an Eclipse development environment.\n\n\nConfiguring the platform\n Quick start guide to present how to configure the platform using a configuration file.\n\n\nDocker-OSSMETER\n How to build and run the Ossmeter docker image.\n\n\n\n\nAdministration\n\n\n\n\nScava Administration\n The administration dashboard take care of managing Scava's services.\n\n\nAPI Gateway Configuration\n\n\nExtending MongoDB Data Model\n\n\n\n\nUsers\n\n\n\n\nScava metrics\n lists metrics computed by the various Scava Components.\n\n\nConsuming the REST services\n This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues\n\n\nREST API Documentation\n Reference documentation of REST services provided by the Scava platform.\n\n\nREST API Generation\n Tutorial about automatic generation of REST API Scava library using OpenAPI.\n\n\n\n\nDevelopment\n\n\n\n\nContributing\n Collection of Architectural and Technical guidelines dedicated to Scava developers.\n\n\nDevelopment guidelines\n Rules and guidelines used for the development of the Scava project.\n\n\nTesting Guidelines\n Collection of testing guidelines dedicated to Scava developers.\n\n\nRunning Scava in Eclipse\n How to setup and run the Scava Eclipse IDE plugin.\n\n\nHow to develop a metric provider\n Want to add a new metric provider? Here are some hints.\n\n\n\n\nArchitecture\n\n\n\n\nAPI Gateway Component\n The API Gateway allows to access to all Scava REST services using a centralised and securised common gateway.\n\n\nAuthentication Component\n The administration dashboard takes care of managing Scava's services.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-the-scava-documentation",
            "text": "This web site is the main documentation place for the  Eclipse Scava  project.  Useful links:   Eclipse Scava home project:  Eclipse Scava @ Eclipse  Eclipse Scava code repository:  github.com/crossminer/scava  Eclipse Scava documentation repository:  github.com/crossminer/scava-docs",
            "title": "Welcome to the Scava documentation"
        },
        {
            "location": "/#platform-installation",
            "text": "Docker-SCAVA  How to build and run the Scava docker image.  Running the platform  Quick start guide to get the Scava platform running from source on an Eclipse development environment.  Configuring the platform  Quick start guide to present how to configure the platform using a configuration file.  Docker-OSSMETER  How to build and run the Ossmeter docker image.",
            "title": "Platform installation"
        },
        {
            "location": "/#administration",
            "text": "Scava Administration  The administration dashboard take care of managing Scava's services.  API Gateway Configuration  Extending MongoDB Data Model",
            "title": "Administration"
        },
        {
            "location": "/#users",
            "text": "Scava metrics  lists metrics computed by the various Scava Components.  Consuming the REST services  This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues  REST API Documentation  Reference documentation of REST services provided by the Scava platform.  REST API Generation  Tutorial about automatic generation of REST API Scava library using OpenAPI.",
            "title": "Users"
        },
        {
            "location": "/#development",
            "text": "Contributing  Collection of Architectural and Technical guidelines dedicated to Scava developers.  Development guidelines  Rules and guidelines used for the development of the Scava project.  Testing Guidelines  Collection of testing guidelines dedicated to Scava developers.  Running Scava in Eclipse  How to setup and run the Scava Eclipse IDE plugin.  How to develop a metric provider  Want to add a new metric provider? Here are some hints.",
            "title": "Development"
        },
        {
            "location": "/#architecture",
            "text": "API Gateway Component  The API Gateway allows to access to all Scava REST services using a centralised and securised common gateway.  Authentication Component  The administration dashboard takes care of managing Scava's services.",
            "title": "Architecture"
        },
        {
            "location": "/admin/API-Gateway-Configuration/",
            "text": "API Gateway configuration\n\n\nWhen to use this guideline ?\n\n\nThis guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API.\n\n\nContext\n\n\nThe Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.\n\n\nRouting : Service Configuration\n\n\nTo reference a new remote REST API in the gateway, you have to add  2 new properties in the application.properties configuration file : the relative path of services  which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process.\n\n\nExamples :\n\n api gateway url = http://85.36.10.13:8080\n\n path = /administration/*\n\n\n url = http://85.36.10.12:8082/administration\n\n\nThe request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create\n\n\n\n\nid : \nzuul.routes.**servicename**.path\ndefault :\n NA\n\n\nRelative path of the incoming service which will be redirected. Example : /test1/**\n\n\n\n\n\n\n\nid : \nzuul.routes.**servicename**.url\ndefault :\n NA\n\n\nRedirection URL of the route. Example : http://127.0.0.1:8082/test1\n\n\n\n\n\nConfiguration file example\n\n\n# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2\n\n\n\n\nComments\n\n\nMore information about API Gateway configuration: \nAPI Gateway Component",
            "title": "API Gateway Configuration"
        },
        {
            "location": "/admin/API-Gateway-Configuration/#api-gateway-configuration",
            "text": "",
            "title": "API Gateway configuration"
        },
        {
            "location": "/admin/API-Gateway-Configuration/#when-to-use-this-guideline",
            "text": "This guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API.",
            "title": "When to use this guideline ?"
        },
        {
            "location": "/admin/API-Gateway-Configuration/#context",
            "text": "The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.",
            "title": "Context"
        },
        {
            "location": "/admin/API-Gateway-Configuration/#routing-service-configuration",
            "text": "To reference a new remote REST API in the gateway, you have to add  2 new properties in the application.properties configuration file : the relative path of services  which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process.  Examples :  api gateway url = http://85.36.10.13:8080  path = /administration/*   url = http://85.36.10.12:8082/administration  The request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create   id :  zuul.routes.**servicename**.path default :  NA  Relative path of the incoming service which will be redirected. Example : /test1/**    id :  zuul.routes.**servicename**.url default :  NA  Redirection URL of the route. Example : http://127.0.0.1:8082/test1",
            "title": "Routing : Service Configuration"
        },
        {
            "location": "/admin/API-Gateway-Configuration/#configuration-file-example",
            "text": "# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2",
            "title": "Configuration file example"
        },
        {
            "location": "/admin/API-Gateway-Configuration/#comments",
            "text": "More information about API Gateway configuration:  API Gateway Component",
            "title": "Comments"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/",
            "text": "When to use ?\n\n\nThis guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas.\n\n\nContext\n\n\nThe Scava platform use a MongoDb data base to store his data. We go through \nPONGO\n, a template based \nJava POJO generator\n to access \nMongoDB\n database. With Pongo we can define the data model which generates strongly-typed Java classes.\n\n\nIn this guideligne, we will present  : \n\n The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform.\n\n The access to MongoDb Document on from an external Java application.\n* How to preform basic CRUD operation with a PONGO Java data model.\n\n\nWe consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model : \nExtend MongoDB Data Model\n.\n\n\nYou want to access to MongoDB Document from an Eclipse Plugin ?\n\n\n1. Add a dependency to the Java Data Model\n\n\n\n\nEdit the plugin.xml file of your plugin.\n\n\nIn Dependency section, add a dependency to  the plugin which contained the data model you went to access.\n\n\nTo \norg.ossmeter.repository.model\n to access data related to project administration , metric execution process and authentification system.\n\n\nTo \norg.ossmeter.repository.model.'project delta manager'\n to access  configuration informations related to source codes managemeny tools.\n\n\nTo  \nspecific metric provider plugins\n  to access data related to a specific metric provider implementation contains his once data model.\n\n\n... others plugin which contained the data model\n\n\nIn Dependency section, add a dependency to \ncom.googlecode.pongo.runtime\n plugin\n\n\n\n\n\n\n2. Initiate a Connection to the MongoDb\n\n\nIn context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base.\n\n\n\n\nIn Dependency section of plugin.xml file , add a dependency to the \norg.ossmeter.platform\n plugin.\n\n\nYou can now create a new connection to the database using the Configuration service.\n\n\n\n\nMongo mongo = Configuration.getInstance().getMongoConnection(); \n\n\n\n\nYou want to access to MongoDB Document on from an External Java Application ?\n\n\n1. Add a dependency to the Java Data Model\n\n\n\n\nAdd to your java project a dependency to the jar which contained  the data model you went to access. You will have to deliver this jar with your application.\n\n\n\n\n\n\n\n\nAdd o your java project a dependency to the \npongo.jar\n jar file which can be download at this url : https://github.com/kolovos/pongo/releases\n\n\n\n\n2. Initiate a Connection to MongoDb\n\n\n// Define ServerAddress of the MongoDb database\nList<ServerAddress> mongoHostAddresses = new ArrayList<>();\nmongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1])));\n\n// Create Connection\nMongo mongo = new Mongo(mongoHostAddresses);\n\n\n\n\nOnce the connection to MongoDb has been created, you have to make the link  between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document.\n\n\nDB db =  mongo.getDB(\"databasename\");\n\n// Initiate the Project Java model\nProjectRepository = new ProjectRepository(db);\n\n\n\n\nBasic CRUD with a PONGO Java data model\n\n\n1. CREATE\n\n\n// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);\n\n\n\n\n2. READ\n\n\n// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to access object properties\nmetricprovider.getname();\n......\n\n\n\n\n3. UPDATE\n\n\n// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);\n\n\n\n\n4. DELETE\n\n\nMongo mongo = new Mongo();\nmongo.dropDatabase(\"databasename\");\n\n\nComment\n\n\nThis wiki has dealt with the access of MongoDB database using PONGO. To continue learning how to modify and make a new model with Pongo, we have another page here \nExtend MongoDB Data Model\n.",
            "title": "Access to MongoDB database using PONGO"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#when-to-use",
            "text": "This guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas.",
            "title": "When to use ?"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#context",
            "text": "The Scava platform use a MongoDb data base to store his data. We go through  PONGO , a template based  Java POJO generator  to access  MongoDB  database. With Pongo we can define the data model which generates strongly-typed Java classes.  In this guideligne, we will present  :   The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform.  The access to MongoDb Document on from an external Java application.\n* How to preform basic CRUD operation with a PONGO Java data model.  We consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model :  Extend MongoDB Data Model .",
            "title": "Context"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#you-want-to-access-to-mongodb-document-from-an-eclipse-plugin",
            "text": "",
            "title": "You want to access to MongoDB Document from an Eclipse Plugin ?"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#1-add-a-dependency-to-the-java-data-model",
            "text": "Edit the plugin.xml file of your plugin.  In Dependency section, add a dependency to  the plugin which contained the data model you went to access.  To  org.ossmeter.repository.model  to access data related to project administration , metric execution process and authentification system.  To  org.ossmeter.repository.model.'project delta manager'  to access  configuration informations related to source codes managemeny tools.  To   specific metric provider plugins   to access data related to a specific metric provider implementation contains his once data model.  ... others plugin which contained the data model  In Dependency section, add a dependency to  com.googlecode.pongo.runtime  plugin",
            "title": "1. Add a dependency to the Java Data Model"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#2-initiate-a-connection-to-the-mongodb",
            "text": "In context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base.   In Dependency section of plugin.xml file , add a dependency to the  org.ossmeter.platform  plugin.  You can now create a new connection to the database using the Configuration service.   Mongo mongo = Configuration.getInstance().getMongoConnection();",
            "title": "2. Initiate a Connection to the MongoDb"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#you-want-to-access-to-mongodb-document-on-from-an-external-java-application",
            "text": "",
            "title": "You want to access to MongoDB Document on from an External Java Application ?"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#1-add-a-dependency-to-the-java-data-model_1",
            "text": "Add to your java project a dependency to the jar which contained  the data model you went to access. You will have to deliver this jar with your application.     Add o your java project a dependency to the  pongo.jar  jar file which can be download at this url : https://github.com/kolovos/pongo/releases",
            "title": "1. Add a dependency to the Java Data Model"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#2-initiate-a-connection-to-mongodb",
            "text": "// Define ServerAddress of the MongoDb database\nList<ServerAddress> mongoHostAddresses = new ArrayList<>();\nmongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1])));\n\n// Create Connection\nMongo mongo = new Mongo(mongoHostAddresses);  Once the connection to MongoDb has been created, you have to make the link  between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document.  DB db =  mongo.getDB(\"databasename\");\n\n// Initiate the Project Java model\nProjectRepository = new ProjectRepository(db);",
            "title": "2. Initiate a Connection to MongoDb"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#basic-crud-with-a-pongo-java-data-model",
            "text": "",
            "title": "Basic CRUD with a PONGO Java data model"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#1-create",
            "text": "// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);",
            "title": "1. CREATE"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#2-read",
            "text": "// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to access object properties\nmetricprovider.getname();\n......",
            "title": "2. READ"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#3-update",
            "text": "// Connect the Data model to the database\nDB db =  mongo.getDB(\"databasename\");\nMetricProvider metricprovider = new MetricProvider(db);\n\n// Used accessors to intialise the object\nmetricprovider.setName(\"Metric1\").\n......\n\n// Create the Document\nmetricprovider.sync(true);",
            "title": "3. UPDATE"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#4-delete",
            "text": "Mongo mongo = new Mongo();\nmongo.dropDatabase(\"databasename\");",
            "title": "4. DELETE"
        },
        {
            "location": "/admin/Access-to-MongoDB-database-using-PONGO/#comment",
            "text": "This wiki has dealt with the access of MongoDB database using PONGO. To continue learning how to modify and make a new model with Pongo, we have another page here  Extend MongoDB Data Model .",
            "title": "Comment"
        },
        {
            "location": "/admin/SCAVA-Administration/",
            "text": "Scava Administration\n\n\nThe SCAVA administration dashboard take care of:\n\n\n\n\nProvide user administration feature, including user profile activation service and roles based authorization management.\n\n\nProvide services to analyse automatically open source software projects.\n\n\n\n\nAdministration Dashboard Installation\n\n\nPrerequired\n\n\nThe SCAVA administration dashboard  is front end web application based on Angular Framework. It can be executed in both Linux or Windows systems where it's required the installation of the following tools:\n\n\nNode.js\n\n\n\n\nDownload Node.js ver. 8.11.2 (includes npm 5.6.0) or above : https://nodejs.org/en/download/\n\n\n\n\nYarn Package Manager\n\n\n\n\nDownload Yarn ver. 1.7.0 or above : https://yarnpkg.com\n\n\n\n\nGet Started Scava Administration\n\n\nScava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart).\n\n\nScava Dashboard Deployment\n\n\nIn order to deploy the Scava Administration, you must to build and copy the output directory to a web server (For instance Apache HTTP Server).\n\n\nUsing the development profile:\n\n\n\n\nExecute the development build using the Angular CLI command line : \nng build\n.\n\n\nCopy everything within the output folder (dist/ by default) to a folder on the server.\n\n\nIf you copy the files into a server sub-folder, append the build flag, --base-href and set the \n appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to \n like this. or simply you can run: \nng build --base-href=/administration/\n\n\n\n\nUsing the production profile:\n\n\n\n\nYou can generate an optimized build with additional CLI command line flags: \nng build -- prod\n.\n\n\nCopy everything within the output folder (dist/ by default) to a folder on the server.\n\n\nIf you copy the files into a server sub-folder, append the build flag, --base-href and set the \n appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to \n like this. or simply you can run: \nng build --base-href=/administration/\n.",
            "title": "SCAVA Administration"
        },
        {
            "location": "/admin/SCAVA-Administration/#scava-administration",
            "text": "The SCAVA administration dashboard take care of:   Provide user administration feature, including user profile activation service and roles based authorization management.  Provide services to analyse automatically open source software projects.",
            "title": "Scava Administration"
        },
        {
            "location": "/admin/SCAVA-Administration/#administration-dashboard-installation",
            "text": "",
            "title": "Administration Dashboard Installation"
        },
        {
            "location": "/admin/SCAVA-Administration/#prerequired",
            "text": "The SCAVA administration dashboard  is front end web application based on Angular Framework. It can be executed in both Linux or Windows systems where it's required the installation of the following tools:",
            "title": "Prerequired"
        },
        {
            "location": "/admin/SCAVA-Administration/#nodejs",
            "text": "Download Node.js ver. 8.11.2 (includes npm 5.6.0) or above : https://nodejs.org/en/download/",
            "title": "Node.js"
        },
        {
            "location": "/admin/SCAVA-Administration/#yarn-package-manager",
            "text": "Download Yarn ver. 1.7.0 or above : https://yarnpkg.com",
            "title": "Yarn Package Manager"
        },
        {
            "location": "/admin/SCAVA-Administration/#get-started-scava-administration",
            "text": "Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart).",
            "title": "Get Started Scava Administration"
        },
        {
            "location": "/admin/SCAVA-Administration/#scava-dashboard-deployment",
            "text": "In order to deploy the Scava Administration, you must to build and copy the output directory to a web server (For instance Apache HTTP Server).",
            "title": "Scava Dashboard Deployment"
        },
        {
            "location": "/admin/SCAVA-Administration/#using-the-development-profile",
            "text": "Execute the development build using the Angular CLI command line :  ng build .  Copy everything within the output folder (dist/ by default) to a folder on the server.  If you copy the files into a server sub-folder, append the build flag, --base-href and set the   appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to   like this. or simply you can run:  ng build --base-href=/administration/",
            "title": "Using the development profile:"
        },
        {
            "location": "/admin/SCAVA-Administration/#using-the-production-profile",
            "text": "You can generate an optimized build with additional CLI command line flags:  ng build -- prod .  Copy everything within the output folder (dist/ by default) to a folder on the server.  If you copy the files into a server sub-folder, append the build flag, --base-href and set the   appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to   like this. or simply you can run:  ng build --base-href=/administration/ .",
            "title": "Using the production profile:"
        },
        {
            "location": "/architecture/API-Gateway-Component/",
            "text": "The API Gateway component\n\n\nThe Scava API Gateway :\n\n\n\n\nProvide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application).\n\n\nProvide a centralized mechanisms to secuerize Scava web services and manage authentication  required to access to this services.\n\n\n\n\nAPI Gateway Architecture\n\n\nThe API Gateway  is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change.\n\n\nThe API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the  API Gateway will manage the authentication for all services of the platform.  \n\n\n\n\nAuthentication Mechanism\n\n\nJSON Web Tokens\n\n\nThe Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.\n\n\nIn authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io)\n\n\n\n\nAuthentication Architecture\n\n\nIn Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway.\n\n\n\n\nAuthentication Flow\n\n\n\n\nTo obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.\n\n\nWhen the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.\n\n\n\n\n\n\nImplementation\n\n\nThe implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures.\n\n\nhttps://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul\n\n\nAPI Gateway Configuration\n\n\nThe Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.\n\n\nServer Configuration\n\n\n\n\nid : \nserver.port\ndefault :\n 8086\n\n\nPort of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port.\n\n\n\n\n\nJWT Security Configuration\n\n\n\n\nid : \napigateway.security.jwt.secret\ndefault :\n NA\n\n\nPrivate key pair which allow to sign jwt tokens using RSA.\n\n\n\n\n\n\n\nid : \napigateway.security.jwt.url\ndefault :\n /login\n\n\nURL Path of the authentication service.\n\n\n\n\n\n\n\nid : \napigateway.security.jwt.expiration\ndefault :\n 86400 (24H)\n\n\nPort of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port.\n\n\n\n\n\nRouting : Authentication Service Configuration\n\n\n\n\nid : \nzuul.routes.auth-center.path\ndefault :\n /api/authentication/**\n\n\nRelative path of the authentication service.\n\n\n\n\n\n\n\nid : \nzuul.routes.auth-center.url\ndefault :\n NA\n\n\nURL of the authentification server. Example: http://127.0.0.1:8081/ \n\n\n\n\n\n\n\nid : \nzuul.routes.auth-center.sensitiveHeaders\ndefault :\n Cookie,Set-Cookie\n\n\nSpecify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers.\n\n\n\n\n\n\n\nid : \nzuul.routes.auth-center.stripPrefix\ndefault :\n false\n\n\nSwitch off the stripping of the service-specific prefix from individual routes\n\n\n\n\n\nRouting : Service Configuration\n\n\n\n\nid : \nzuul.routes.**servicename**.path\ndefault :\n NA\n\n\nRelative path of the incoming service which will be redirected. Example : /test1/**\n\n\n\n\n\n\n\nid : \nzuul.routes.**servicename**.url\ndefault :\n NA\n\n\nRedirection URL of the route. Example : http://127.0.0.1:8082/test1\n\n\n\n\n\nConfiguration file example\n\n\n#API Gateway Port\nserver.port=8086\n\n# JWT Configuration\napigateway.security.jwt.secret=otherpeopledontknowit\napigateway.security.jwt.url=/api/authentication\napigateway.security.jwt.expiration=86400\n\n# Rooting Configuration : Authentication Service\nzuul.routes.auth-center.path=/api/authentication/**\nzuul.routes.auth-center.url=http://127.0.0.1:8081/\nzuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie\nzuul.routes.auth-center.stripPrefix=false\n\n\n# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2\n\n\n\n\nControl access API\n\n\nThe Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including:\n\n \u201cROLE_ADMIN\u201d\n\n \u201cROLE_PROJECT_MANAGER\u201d\n* \u201cROLE_USER\u201d\n\n\nBy the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d.\n\n\n# Filtering private restApi\n\nscava.routes.config.adminAccessApi[0]=/api/users\nscava.routes.config.adminAccessApi[1]=/api/user/**\n\nscava.routes.config.projectManagerAccessApi[0]=/administration/projects/create\nscava.routes.config.projectManagerAccessApi[1]=/administration/projects/import\nscava.routes.config.projectManagerAccessApi[2]=/administration/analysis/**\n\nscava.routes.config.userAccessApi[0]=/administration/projects\nscava.routes.config.userAccessApi[1]=/administration/projects/p/**\nscava.routes.config.userAccessApi[2]=/api/users/**\nscava.routes.config.userAccessApi[3]=/api/account\n\n\n\n\nPackaging Form Sources\n\n\nMaven Packaging\n\n\nmvn -Pprod install\n\n\n\n\nAPI Gateway Execution\n\n\n\n\ncomplete an put the \"application.properties\" configuration file in the execute directory.\n\n\nExecute the crossmeter-api-gateway-1.0.0.jar Jar.\n\n\n\n\njava -jar scava-api-gateway-1.0.0.jar\n\n\n\n\nClient Implementation\n\n\nHow to consume a Scava REST services ?\n \\\nThis guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.",
            "title": "API Gateway Component"
        },
        {
            "location": "/architecture/API-Gateway-Component/#the-api-gateway-component",
            "text": "The Scava API Gateway :   Provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application).  Provide a centralized mechanisms to secuerize Scava web services and manage authentication  required to access to this services.",
            "title": "The API Gateway component"
        },
        {
            "location": "/architecture/API-Gateway-Component/#api-gateway-architecture",
            "text": "The API Gateway  is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change.  The API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the  API Gateway will manage the authentication for all services of the platform.",
            "title": "API Gateway Architecture"
        },
        {
            "location": "/architecture/API-Gateway-Component/#authentication-mechanism",
            "text": "",
            "title": "Authentication Mechanism"
        },
        {
            "location": "/architecture/API-Gateway-Component/#json-web-tokens",
            "text": "The Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.  In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io)",
            "title": "JSON Web Tokens"
        },
        {
            "location": "/architecture/API-Gateway-Component/#authentication-architecture",
            "text": "In Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway.",
            "title": "Authentication Architecture"
        },
        {
            "location": "/architecture/API-Gateway-Component/#authentication-flow",
            "text": "To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.  When the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.",
            "title": "Authentication Flow"
        },
        {
            "location": "/architecture/API-Gateway-Component/#implementation",
            "text": "The implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures.  https://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul",
            "title": "Implementation"
        },
        {
            "location": "/architecture/API-Gateway-Component/#api-gateway-configuration",
            "text": "The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.",
            "title": "API Gateway Configuration"
        },
        {
            "location": "/architecture/API-Gateway-Component/#server-configuration",
            "text": "id :  server.port default :  8086  Port of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port.",
            "title": "Server Configuration"
        },
        {
            "location": "/architecture/API-Gateway-Component/#jwt-security-configuration",
            "text": "id :  apigateway.security.jwt.secret default :  NA  Private key pair which allow to sign jwt tokens using RSA.    id :  apigateway.security.jwt.url default :  /login  URL Path of the authentication service.    id :  apigateway.security.jwt.expiration default :  86400 (24H)  Port of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port.",
            "title": "JWT Security Configuration"
        },
        {
            "location": "/architecture/API-Gateway-Component/#routing-authentication-service-configuration",
            "text": "id :  zuul.routes.auth-center.path default :  /api/authentication/**  Relative path of the authentication service.    id :  zuul.routes.auth-center.url default :  NA  URL of the authentification server. Example: http://127.0.0.1:8081/     id :  zuul.routes.auth-center.sensitiveHeaders default :  Cookie,Set-Cookie  Specify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers.    id :  zuul.routes.auth-center.stripPrefix default :  false  Switch off the stripping of the service-specific prefix from individual routes",
            "title": "Routing : Authentication Service Configuration"
        },
        {
            "location": "/architecture/API-Gateway-Component/#routing-service-configuration",
            "text": "id :  zuul.routes.**servicename**.path default :  NA  Relative path of the incoming service which will be redirected. Example : /test1/**    id :  zuul.routes.**servicename**.url default :  NA  Redirection URL of the route. Example : http://127.0.0.1:8082/test1",
            "title": "Routing : Service Configuration"
        },
        {
            "location": "/architecture/API-Gateway-Component/#configuration-file-example",
            "text": "#API Gateway Port\nserver.port=8086\n\n# JWT Configuration\napigateway.security.jwt.secret=otherpeopledontknowit\napigateway.security.jwt.url=/api/authentication\napigateway.security.jwt.expiration=86400\n\n# Rooting Configuration : Authentication Service\nzuul.routes.auth-center.path=/api/authentication/**\nzuul.routes.auth-center.url=http://127.0.0.1:8081/\nzuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie\nzuul.routes.auth-center.stripPrefix=false\n\n\n# Rooting Configuration : Test1 Service\nzuul.routes.test1.path=/test1/**\nzuul.routes.test1.url=http://127.0.0.1:8082/test1\n\n# Rooting Configuration : Test2 Service\nzuul.routes.test2.path=/test2/**\nzuul.routes.test2.url=http://127.0.0.1:8083/test2",
            "title": "Configuration file example"
        },
        {
            "location": "/architecture/API-Gateway-Component/#control-access-api",
            "text": "The Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including:  \u201cROLE_ADMIN\u201d  \u201cROLE_PROJECT_MANAGER\u201d\n* \u201cROLE_USER\u201d  By the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d.  # Filtering private restApi\n\nscava.routes.config.adminAccessApi[0]=/api/users\nscava.routes.config.adminAccessApi[1]=/api/user/**\n\nscava.routes.config.projectManagerAccessApi[0]=/administration/projects/create\nscava.routes.config.projectManagerAccessApi[1]=/administration/projects/import\nscava.routes.config.projectManagerAccessApi[2]=/administration/analysis/**\n\nscava.routes.config.userAccessApi[0]=/administration/projects\nscava.routes.config.userAccessApi[1]=/administration/projects/p/**\nscava.routes.config.userAccessApi[2]=/api/users/**\nscava.routes.config.userAccessApi[3]=/api/account",
            "title": "Control access API"
        },
        {
            "location": "/architecture/API-Gateway-Component/#packaging-form-sources",
            "text": "Maven Packaging  mvn -Pprod install",
            "title": "Packaging Form Sources"
        },
        {
            "location": "/architecture/API-Gateway-Component/#api-gateway-execution",
            "text": "complete an put the \"application.properties\" configuration file in the execute directory.  Execute the crossmeter-api-gateway-1.0.0.jar Jar.   java -jar scava-api-gateway-1.0.0.jar",
            "title": "API Gateway Execution"
        },
        {
            "location": "/architecture/API-Gateway-Component/#client-implementation",
            "text": "How to consume a Scava REST services ?  \\\nThis guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.",
            "title": "Client Implementation"
        },
        {
            "location": "/architecture/Authentication-Component/",
            "text": "Authentication Component\n\n\nThe Scava Authentication service:\n\n\n\n\nProvides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform.\n\n\nProvides user management services, including user registration process, user profile editing and roles based authorization management.\n\n\n\n\nAuthentication API\n\n\nThe Authentication server is a component of The Scava platform which manages the authentication for all  services accessible behind the API Gateway.\n\n\n\n\nAuthenticate User\nPOST\n/api/authentication\n\n\nLogin as a registered user.\n\n\n\n\n\n\n\n### JSON Web Tokens (JWT)\n\nJSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are:\n* Header\n* Payload\n* Signature\n\nThis solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/).\n\n### JWT Authentication Implementation\n\n* Users have to login to the authentication service API using their credentials username and password.\n\n\u0002wzxhzdk:0\u0003\n\n* Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header.\n\n\n\n\n* The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway.\n\n\u0002wzxhzdk:1\u0003\n\n\n\n\n## User Management API\n\nThe Authentication component provides web services for CRUD user account.\n\n\n\n\nRegister User\nPOST\n/api/register\n\n\nRegister new user.\n\n\n\n\n\n\nActivate User\nGET\n/api/activate\n\n\nActivate the registered user.\n\n\n\n\n\n\nUpdate User\nPUT\n/api/users\n\n\nUpdate an existing user.\n\n\n\n\n\n\nRetrieve Users\nGET\n/api/users\n\n\nGet all registered users.\n\n\n\n\n\n\nRetrieve Login User\nGET\n/api/users/{login}\n\n\nGet the \"login\" user.\n\n\n\n\n\n\nDelete User\nDELETE\n/api/users/{login}\n\n\nDelete the \"login\" user.\n\n\n\n\n\nAuthentication Server Configuration\n\n\nThe Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component.\n\n\nServer Configuration\n\n\n\n\nid : \nserver.port\ndefault :\n 8085\n\n\nPort of the Authentication API server. Each REST request sent to the gateway must be adressed to this port.\n\n\n\n\n\nJWT Security Configuration\n\n\n\n\nid : \napigateway.security.jwt.secret\ndefault :\n NA\n\n\nPrivate key pair which allow to sign jwt tokens using RSA.\n\n\n\n\n\nDefault ADMIN configuration\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nscava.administration.username\n\n\nThe administrator username\n\n\nadmin\n\n\n\n\n\n\nscava.administration.password\n\n\nThe administrator password\n\n\nadmin\n\n\n\n\n\n\nscava.administration.admin-role\n\n\nThe admin role\n\n\nADMIN\n\n\n\n\n\n\nscava.administration.project-manager-role\n\n\nThe project manager role\n\n\nPROJECT_MANAGER\n\n\n\n\n\n\nscava.administration.project-user-role\n\n\nThe user role\n\n\nUSER\n\n\n\n\n\n\n\n\nMongodb Database Configuration\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nspring.data.mongodb.uri\n\n\nUrl of the MongoDB database server\n\n\nmongodb://localhost:27017\n\n\n\n\n\n\nspring.data.mongodb.database\n\n\nName of the MongoDB database\n\n\nscava\n\n\n\n\n\n\n\n\nMail Server configuration\n\n\nIn order to register new users, you have to configure a mail server.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nspring.mail.host\n\n\nUrl of the mail service\n\n\nsmtp.gmail.com\n\n\n\n\n\n\nspring.mail.port\n\n\nPort of the mail service\n\n\n587\n\n\n\n\n\n\nspring.mail.username\n\n\nLogin of the mail account\n\n\n\n\n\n\n\n\nspring.mail.password\n\n\nPassword of the mail account\n\n\n\n\n\n\n\n\nspring.mail.protocol\n\n\nmail protocole\n\n\nsmtp\n\n\n\n\n\n\nspring.mail.tls\n\n\n-\n\n\ntrue\n\n\n\n\n\n\nspring.mail.properties.mail.smtp.auth\n\n\n-\n\n\ntrue\n\n\n\n\n\n\nspring.mail.properties.mail.smtp.starttls.enable\n\n\n-\n\n\ntrue\n\n\n\n\n\n\nspring.mail.properties.mail.smtp.ssl.trust=\n\n\n-\n\n\nsmtp.gmail.com\n\n\n\n\n\n\n\n\nAdministration Dashboard Setting\n\n\n\n\nid : \nscava.administration.base-url\ndefault :\n http://localhost:4200\n\n\nThe SCAVA administration base URL to generate the activation account URL.\n\n\n\n\n\nPackaging From Sources\n\n\nMaven Packaging\n\n\nmvn -Pprod install\n\n\n\n\nAuthentication Server Execution\n\n\n\n\ncomplete an put the \"application.properties\" configuration file in the execution directory.\n\n\nExecute the scava-auth-service-1.0.0.jar Jar.\n\n\n\n\njava -jar scava-auth-service-1.0.0.jar",
            "title": "Authentication Component"
        },
        {
            "location": "/architecture/Authentication-Component/#authentication-component",
            "text": "The Scava Authentication service:   Provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform.  Provides user management services, including user registration process, user profile editing and roles based authorization management.",
            "title": "Authentication Component"
        },
        {
            "location": "/architecture/Authentication-Component/#authentication-api",
            "text": "The Authentication server is a component of The Scava platform which manages the authentication for all  services accessible behind the API Gateway.   Authenticate User POST /api/authentication  Login as a registered user.   \n\n### JSON Web Tokens (JWT)\n\nJSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are:\n* Header\n* Payload\n* Signature\n\nThis solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/).\n\n### JWT Authentication Implementation\n\n* Users have to login to the authentication service API using their credentials username and password.\n\n\u0002wzxhzdk:0\u0003\n\n* Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header. \n\n* The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway.\n\n\u0002wzxhzdk:1\u0003 \n\n## User Management API\n\nThe Authentication component provides web services for CRUD user account.  Register User POST /api/register  Register new user.    Activate User GET /api/activate  Activate the registered user.    Update User PUT /api/users  Update an existing user.    Retrieve Users GET /api/users  Get all registered users.    Retrieve Login User GET /api/users/{login}  Get the \"login\" user.    Delete User DELETE /api/users/{login}  Delete the \"login\" user.",
            "title": "Authentication API"
        },
        {
            "location": "/architecture/Authentication-Component/#authentication-server-configuration",
            "text": "The Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component.",
            "title": "Authentication Server Configuration"
        },
        {
            "location": "/architecture/Authentication-Component/#server-configuration",
            "text": "id :  server.port default :  8085  Port of the Authentication API server. Each REST request sent to the gateway must be adressed to this port.",
            "title": "Server Configuration"
        },
        {
            "location": "/architecture/Authentication-Component/#jwt-security-configuration",
            "text": "id :  apigateway.security.jwt.secret default :  NA  Private key pair which allow to sign jwt tokens using RSA.",
            "title": "JWT Security Configuration"
        },
        {
            "location": "/architecture/Authentication-Component/#default-admin-configuration",
            "text": "Property  Description  Default Value      scava.administration.username  The administrator username  admin    scava.administration.password  The administrator password  admin    scava.administration.admin-role  The admin role  ADMIN    scava.administration.project-manager-role  The project manager role  PROJECT_MANAGER    scava.administration.project-user-role  The user role  USER",
            "title": "Default ADMIN configuration"
        },
        {
            "location": "/architecture/Authentication-Component/#mongodb-database-configuration",
            "text": "Property  Description  Default Value      spring.data.mongodb.uri  Url of the MongoDB database server  mongodb://localhost:27017    spring.data.mongodb.database  Name of the MongoDB database  scava",
            "title": "Mongodb Database Configuration"
        },
        {
            "location": "/architecture/Authentication-Component/#mail-server-configuration",
            "text": "In order to register new users, you have to configure a mail server.     Property  Description  Default Value      spring.mail.host  Url of the mail service  smtp.gmail.com    spring.mail.port  Port of the mail service  587    spring.mail.username  Login of the mail account     spring.mail.password  Password of the mail account     spring.mail.protocol  mail protocole  smtp    spring.mail.tls  -  true    spring.mail.properties.mail.smtp.auth  -  true    spring.mail.properties.mail.smtp.starttls.enable  -  true    spring.mail.properties.mail.smtp.ssl.trust=  -  smtp.gmail.com",
            "title": "Mail Server configuration"
        },
        {
            "location": "/architecture/Authentication-Component/#administration-dashboard-setting",
            "text": "id :  scava.administration.base-url default :  http://localhost:4200  The SCAVA administration base URL to generate the activation account URL.",
            "title": "Administration Dashboard Setting"
        },
        {
            "location": "/architecture/Authentication-Component/#packaging-from-sources",
            "text": "Maven Packaging  mvn -Pprod install",
            "title": "Packaging From Sources"
        },
        {
            "location": "/architecture/Authentication-Component/#authentication-server-execution",
            "text": "complete an put the \"application.properties\" configuration file in the execution directory.  Execute the scava-auth-service-1.0.0.jar Jar.   java -jar scava-auth-service-1.0.0.jar",
            "title": "Authentication Server Execution"
        },
        {
            "location": "/deploy/Docker-Ossmeter/",
            "text": "Docker Ossmeter\n\n\nThis page lists information about the Ossmeter docker image. It should be noted that the generated image uses the old \nOssmeter binaries\n and will not be updated -- all new development will go into the Crossminer repository.\n\n\nAll images are stored on the \nCrossminer Docker-hub account\n.\n\n\nThe Docker image is composed of 4 services: \n\n\n\n\noss-web: corresponds to the service of OSSMETER platform website.\n\n\noss-app: service running api server and the orchestrator of OSSMETER slave instances.\n\n\noss-slave: service corresponding to the OSSMETER slaves responsible for the analysis of software projects. There can be several slaves serving the same master for load balancing.\n\n\noss-db: service responsible for the the storage of OSSMETER data. Uses a MongoDB image.\n\n\n\n\nThe database comes pre-populated with a project and a user. The loaded dump comes from \nmd2manoppello's repo\n. Login information:\n\n\n\n\nuser: \ndemo@crossminer.org\n\n\npassword: \ndemo18\n\n\n\n\nCustom quality is in the user object (demo@crossminer.org) stored in the users collection of users db. It resembles the \ndemo quality model\n.\n\n\nRunning the Ossmeter docker image\n\n\nThe easiest way to build the full stack is to run the docker-compose file:\n\n\n$ docker-compose up\n\n\n\n\nThis command will download the images and run them. The application is then available on \nlocalhost:9000\n.\n\n\nBuilding the Ossmeter docker image\n\n\nTwo containers actually need to be built. They can be built individually.\n\n\noss-platform\n\n\nBuild the image from the oss-platform directory:\n\n\n$ docker build -t bbaldassari/ossmeter-platform .\nSending build context to Docker daemon  3.072kB\nStep 1/5 : FROM openjdk:8-jdk\n\n\n\n\noss-web\n\n\nBuild the image from the oss-web directory:\n\n\n$ docker build -t bbaldassari/ossmeter-web .\nSending build context to Docker daemon  3.072kB\nStep 1/7 : FROM openjdk:8-jre-alpine\n\n\n\n\nContinuous integration\n\n\nWe use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the \n#ci\n channel in Slack.",
            "title": "Docker Ossmeter"
        },
        {
            "location": "/deploy/Docker-Ossmeter/#docker-ossmeter",
            "text": "This page lists information about the Ossmeter docker image. It should be noted that the generated image uses the old  Ossmeter binaries  and will not be updated -- all new development will go into the Crossminer repository.  All images are stored on the  Crossminer Docker-hub account .  The Docker image is composed of 4 services:    oss-web: corresponds to the service of OSSMETER platform website.  oss-app: service running api server and the orchestrator of OSSMETER slave instances.  oss-slave: service corresponding to the OSSMETER slaves responsible for the analysis of software projects. There can be several slaves serving the same master for load balancing.  oss-db: service responsible for the the storage of OSSMETER data. Uses a MongoDB image.   The database comes pre-populated with a project and a user. The loaded dump comes from  md2manoppello's repo . Login information:   user:  demo@crossminer.org  password:  demo18   Custom quality is in the user object (demo@crossminer.org) stored in the users collection of users db. It resembles the  demo quality model .",
            "title": "Docker Ossmeter"
        },
        {
            "location": "/deploy/Docker-Ossmeter/#running-the-ossmeter-docker-image",
            "text": "The easiest way to build the full stack is to run the docker-compose file:  $ docker-compose up  This command will download the images and run them. The application is then available on  localhost:9000 .",
            "title": "Running the Ossmeter docker image"
        },
        {
            "location": "/deploy/Docker-Ossmeter/#building-the-ossmeter-docker-image",
            "text": "Two containers actually need to be built. They can be built individually.",
            "title": "Building the Ossmeter docker image"
        },
        {
            "location": "/deploy/Docker-Ossmeter/#oss-platform",
            "text": "Build the image from the oss-platform directory:  $ docker build -t bbaldassari/ossmeter-platform .\nSending build context to Docker daemon  3.072kB\nStep 1/5 : FROM openjdk:8-jdk",
            "title": "oss-platform"
        },
        {
            "location": "/deploy/Docker-Ossmeter/#oss-web",
            "text": "Build the image from the oss-web directory:  $ docker build -t bbaldassari/ossmeter-web .\nSending build context to Docker daemon  3.072kB\nStep 1/7 : FROM openjdk:8-jre-alpine",
            "title": "oss-web"
        },
        {
            "location": "/deploy/Docker-Ossmeter/#continuous-integration",
            "text": "We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the  #ci  channel in Slack.",
            "title": "Continuous integration"
        },
        {
            "location": "/deploy/Docker-SCAVA/",
            "text": "Docker SCAVA\n\n\nThis page is about how to deploy a SCAVA instance on the behalf of Docker.\n\n\nAt the actual stage of the project, there two ways to get started with the docker images:\n1. Ready-to-use images are stored on the \nCrossminer Docker-hub account\n.\n1. Build them from the \nscava-deployment\n repository. They have to be built from various Dockerfile's and with help of a docker-compose file. At the time being, we're testing the \ndashboard-plus-admin branch\n.\n\n\nSummary of containers\n\n\nThe whole Docker stack consists of 11 services:\n\n\n\n\n\n\n\n\nDocker service name\n\n\nFull Name\n\n\nDefault port\n\n\nComments\n\n\n\n\n\n\n\n\n\n\nadmin-webapp\n\n\nAdministration UI\n\n\n80\n\n\nBuilt from /web-admin.\n\n\n\n\n\n\noss-app\n\n\nMetric Plateform\n\n\n8182\n\n\nBuilt from /metric-platform.\n\n\n\n\n\n\noss-db\n\n\nMongoDB (metrics storage)\n\n\n27017\n\n\nBuilt from /metric-platform. Can be used to connect a MongoDB visualisation tool.\n\n\n\n\n\n\nkb\n\n\nKnowledge base\n\n\n8080\n\n\nBuilt from KB.\n\n\n\n\n\n\nkb-db\n\n\nKnowledge base DB (based on MongoDB)\n\n\n27018\n\n\nBuilt from /KB-db. Can be used to connect a MongoDB visualisation tool.\n\n\n\n\n\n\napi-gw\n\n\nAPI Gateway\n\n\n8086\n\n\nBuilt from /api-gw.\n\n\n\n\n\n\nauth\n\n\nAuthentication\n\n\n8085\n\n\nBuilt from /auth.\n\n\n\n\n\n\nelasticsearch\n\n\nElasticSearch\n\n\n9200\n\n\nPulled from docker hub acsdocker/elasticsearch:6.3.1-secured. Can be used to connect an ElasticSearch visualisation tool.\n\n\n\n\n\n\nkibiter\n\n\nKibiter (Bitergia\u2019s customized Kibana)\n\n\n5601\n\n\nPulled from docker hub acsdocker/grimoirelab-kibiter:crossminer-6.3.1\n\n\n\n\n\n\ndashb-importer\n\n\nDashboard importer (to kibiter)\n\n\n\n\nNo port exposed on the host.\n\n\n\n\n\n\nprosoul\n\n\nProsoul Quality Model Viewer\n\n\n8000\n\n\nPulled from docker hub acsdocker/prosoul.\n\n\n\n\n\n\n\n\nBuilding the Docker images\n\n\nThe deployment setup is hosted in the \nscava-deployment\n repository. One needs to clone the repository locally in order to build and run the docker images.\n\n\nTo build all the required Docker images, simply go to the root of the cloned repository and issue the following command. This will rebuild all images, dismissing any cached layers.\n\n\n$ docker-compose -f docker-compose-build.yml build --no-cache\n\n\n\n\nThis will build required images and pull images hosted on docker hub.\n\n\nPrerequisites\n\n\nIn order to run Scava, you need to:\n\n\n\n\nEdit the hosts of your machine, creating a new record for the IP address: \n127.0.0.1\n with hostname: \nadmin-webapp\n\n\nEdit the \nexternalConfig.json\n file inside \nweb-admin/angular/\n docker image and put inside it the IP address of your host as follow:\n   {\n    \"SERVICE_URL\" :\"http://HOST-IP:8086\"\n   }\n\n\n\n\nRunning the locally built docker images\n\n\nTo run the locally built images, run the following command. Note that if the images are not available they will be rebuilt.\n\n\n$ docker-compose -f docker-compose-build.yml up\n\n\n\n\nAccess the administration web app by using the following address in the web browser: http://admin-webapp/\nFor login use user: admin  pass: admin\n\n\nRunning the pre-built docker images\n\n\nThe easiest way to run the full Scava setup is to use the docker images \nstored on Docker Hub\n. Use the \ndocker-compose-dockerhub.yml\n file to download all required images and start the stack:\n\n\n$ docker-compose -f docker-compose-dockerhub.yml up\n\n\n\n\nAccess the administration web app by using the following address in the web browser: http://admin-webapp/\n\n\nFor login use user: admin  pass: admin\n\n\nContinuous integration\n\n\nWe use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the \n#ci\n channel in Slack.",
            "title": "Docker SCAVA"
        },
        {
            "location": "/deploy/Docker-SCAVA/#docker-scava",
            "text": "This page is about how to deploy a SCAVA instance on the behalf of Docker.  At the actual stage of the project, there two ways to get started with the docker images:\n1. Ready-to-use images are stored on the  Crossminer Docker-hub account .\n1. Build them from the  scava-deployment  repository. They have to be built from various Dockerfile's and with help of a docker-compose file. At the time being, we're testing the  dashboard-plus-admin branch .",
            "title": "Docker SCAVA"
        },
        {
            "location": "/deploy/Docker-SCAVA/#summary-of-containers",
            "text": "The whole Docker stack consists of 11 services:     Docker service name  Full Name  Default port  Comments      admin-webapp  Administration UI  80  Built from /web-admin.    oss-app  Metric Plateform  8182  Built from /metric-platform.    oss-db  MongoDB (metrics storage)  27017  Built from /metric-platform. Can be used to connect a MongoDB visualisation tool.    kb  Knowledge base  8080  Built from KB.    kb-db  Knowledge base DB (based on MongoDB)  27018  Built from /KB-db. Can be used to connect a MongoDB visualisation tool.    api-gw  API Gateway  8086  Built from /api-gw.    auth  Authentication  8085  Built from /auth.    elasticsearch  ElasticSearch  9200  Pulled from docker hub acsdocker/elasticsearch:6.3.1-secured. Can be used to connect an ElasticSearch visualisation tool.    kibiter  Kibiter (Bitergia\u2019s customized Kibana)  5601  Pulled from docker hub acsdocker/grimoirelab-kibiter:crossminer-6.3.1    dashb-importer  Dashboard importer (to kibiter)   No port exposed on the host.    prosoul  Prosoul Quality Model Viewer  8000  Pulled from docker hub acsdocker/prosoul.",
            "title": "Summary of containers"
        },
        {
            "location": "/deploy/Docker-SCAVA/#building-the-docker-images",
            "text": "The deployment setup is hosted in the  scava-deployment  repository. One needs to clone the repository locally in order to build and run the docker images.  To build all the required Docker images, simply go to the root of the cloned repository and issue the following command. This will rebuild all images, dismissing any cached layers.  $ docker-compose -f docker-compose-build.yml build --no-cache  This will build required images and pull images hosted on docker hub.",
            "title": "Building the Docker images"
        },
        {
            "location": "/deploy/Docker-SCAVA/#prerequisites",
            "text": "In order to run Scava, you need to:   Edit the hosts of your machine, creating a new record for the IP address:  127.0.0.1  with hostname:  admin-webapp  Edit the  externalConfig.json  file inside  web-admin/angular/  docker image and put inside it the IP address of your host as follow:\n   {\n    \"SERVICE_URL\" :\"http://HOST-IP:8086\"\n   }",
            "title": "Prerequisites"
        },
        {
            "location": "/deploy/Docker-SCAVA/#running-the-locally-built-docker-images",
            "text": "To run the locally built images, run the following command. Note that if the images are not available they will be rebuilt.  $ docker-compose -f docker-compose-build.yml up  Access the administration web app by using the following address in the web browser: http://admin-webapp/\nFor login use user: admin  pass: admin",
            "title": "Running the locally built docker images"
        },
        {
            "location": "/deploy/Docker-SCAVA/#running-the-pre-built-docker-images",
            "text": "The easiest way to run the full Scava setup is to use the docker images  stored on Docker Hub . Use the  docker-compose-dockerhub.yml  file to download all required images and start the stack:  $ docker-compose -f docker-compose-dockerhub.yml up  Access the administration web app by using the following address in the web browser: http://admin-webapp/  For login use user: admin  pass: admin",
            "title": "Running the pre-built docker images"
        },
        {
            "location": "/deploy/Docker-SCAVA/#continuous-integration",
            "text": "We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the  #ci  channel in Slack.",
            "title": "Continuous integration"
        },
        {
            "location": "/deploy/Platform-configuration/",
            "text": "When starting the platform, you can pass a configuration file to control the behaviour of the platform:\n\n\n./eclipse -slave -config myconfiguration.properties\n\n\n\n\nThe configuration file is a typical Java properties file. The properties that can be configured are:\n\n\nidentifier=<your name>\n\n\nThe identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID. \n\n\nIf you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers.\n\n\nlog.type=console|file|rolling\n\n\nYou can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify \nfile\n or \nrolling\n, you must complete the \nlog.file.path\n or \nlog.rolling.path\n property as well. \n\n\nIf the property is not specified, it will default to the console logger.\n\n\nlog.file.path=<path>\n\n\nThe path to the file to store the log. E.g. \nlog.file.path=/tmp/lovelylog.log\n\n\nlog.rolling.path=<path>\n\n\nThe path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify \nlog.rolling.path=/tmp/mylovelylog.log\n, it will store files like so: \n/tmp/mylovelylog.log.2014-12-17-00\n and \n/tmp/mylovelylog.log.2014-12-17-12\n.\n\n\nmaven_executable=<path>\n\n\nThe path to where Maven is installed. E.g. \nmaven_executable=/usr/bin/mvn\n\n\nstorage_path=<path>\n\n\nThe path to where files should be stored. E.g. \nstorage_path=/mnt/ossmeter/\n\n\nmongo_hosts\n\n\nA comma-separated list of the hosts and ports in a replica set. E.g. \nua002:27017,ua009:27017,ua019:27017,ua020:27017",
            "title": "Platform configuration"
        },
        {
            "location": "/deploy/Platform-configuration/#identifieryour-name",
            "text": "The identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID.   If you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers.",
            "title": "identifier=&lt;your name&gt;"
        },
        {
            "location": "/deploy/Platform-configuration/#logtypeconsolefilerolling",
            "text": "You can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify  file  or  rolling , you must complete the  log.file.path  or  log.rolling.path  property as well.   If the property is not specified, it will default to the console logger.",
            "title": "log.type=console|file|rolling"
        },
        {
            "location": "/deploy/Platform-configuration/#logfilepathpath",
            "text": "The path to the file to store the log. E.g.  log.file.path=/tmp/lovelylog.log",
            "title": "log.file.path=&lt;path&gt;"
        },
        {
            "location": "/deploy/Platform-configuration/#logrollingpathpath",
            "text": "The path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify  log.rolling.path=/tmp/mylovelylog.log , it will store files like so:  /tmp/mylovelylog.log.2014-12-17-00  and  /tmp/mylovelylog.log.2014-12-17-12 .",
            "title": "log.rolling.path=&lt;path&gt;"
        },
        {
            "location": "/deploy/Platform-configuration/#maven_executablepath",
            "text": "The path to where Maven is installed. E.g.  maven_executable=/usr/bin/mvn",
            "title": "maven_executable=&lt;path&gt;"
        },
        {
            "location": "/deploy/Platform-configuration/#storage_pathpath",
            "text": "The path to where files should be stored. E.g.  storage_path=/mnt/ossmeter/",
            "title": "storage_path=&lt;path&gt;"
        },
        {
            "location": "/deploy/Platform-configuration/#mongo_hosts",
            "text": "A comma-separated list of the hosts and ports in a replica set. E.g.  ua002:27017,ua009:27017,ua019:27017,ua020:27017",
            "title": "mongo_hosts"
        },
        {
            "location": "/deploy/Running-the-platform/",
            "text": "Running the platform\n\n\nThis is a quick start guide to get the OSSMETER platform running from source.\n\n\nAlthough these instructions may apply to other versions of Eclipse, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package).\n\n\nA step-by-step video guide is also available at \nhttps://youtu.be/3Ry4KKfNdYg\n\n\nStart MongoDB\n\n\nYou can download MongoDB from the \nMongoDb website\n.\n\n\nInstructions for starting mongo can be found in the MongoDB \nmanual\n. For example:\n\n\nmongod --dbpath /data/db --port 27017\n\n\n\n\nGet the Code\n\n\nGet the latest version of the code, and checkout the \ndev\n branch. Please don't commit to the \nmaster\n branch: see the \nDevelopment Guidelines\n:\n\n\nIf you are using \nLinux / OS X\n:\n\n\ngit clone https://github.com/crossminer/scava.git scava\ncd scava\ngit checkout dev\n\n\n\n\nIf you are using \nWindows\n you need to do things differently due to Windows' long file name limit. In the Git shell:\n\n\nmkdir scava\ncd scava\ngit init\ngit config core.longpaths true\ngit add remote origin https://github.com/crossminer/scava.git\ngit fetch\ngit checkout dev\n\n\n\n\nSetup Eclipse\n\n\nOpen Eclipse and import all projects from the top level directory of the Scava code (\nFile -> Import -> Existing projects into workspace\n), and wait for all the projects to compile without errors.\n\n\nValidate and Run the Platform\n\n\nOpen \norg.ossmeter.platform.osgi/ossmeterfromfeature.product\n\n  * Click the \nValidate...\n icon in the top right of the product configuration editor (the icon is a piece of paper with a tick)\n  * If things do not validate, there's something wrong -- get in touch :) Problems related to \norg.eclipse.e4.core.di\n aren't critical.\n  * Then, click the \nExport an Eclipse product\n on the left of the \nValidate...\n button. Uncheck the \nGenerate p2 repository\n checkbox, select a destination directory and validate. After a while, the OSSMETER platform will be generated in the selected directory.\n  * The platform can then be run using the generated \neclipse\n binary; it accepts the following arguments:\n    * \n-apiServer\n: Starts up the client API on localhost:8182\n    * \n-worker ${id-worker}\n: Spawns a thread that analyses registered projects\n  * To get a full platform running, first launch a master thread, then a slave, and finally the API server.\n\n\nIf you are developing code for the Scava platform, be sure to check out the \nContributing\n.\n\n\nRun the api-gateway\n\n\n\n\nRight click on\n\nscava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java\n\n\nThen click on Run As -> Java Application\n\n\n\n\nRun the authentication service\n\n\n\n\nRight click on\n\nscava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java\n\n\nThen click on Run As -> Java Application\n\n\n\n\nRun the administration dashboard\n\n\nScava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart).\n\n\nThe following instructions show how to run the dashboard web app:\n  * Enter the \nadministration/scava-administration/\n directory within the scava repository.\n  * Run the web app on port 4200 using angular-cli: \nng serve\n\n  * Navigate to \nhttp://localhost:4200/",
            "title": "Running the platform"
        },
        {
            "location": "/deploy/Running-the-platform/#running-the-platform",
            "text": "This is a quick start guide to get the OSSMETER platform running from source.  Although these instructions may apply to other versions of Eclipse, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package).  A step-by-step video guide is also available at  https://youtu.be/3Ry4KKfNdYg",
            "title": "Running the platform"
        },
        {
            "location": "/deploy/Running-the-platform/#start-mongodb",
            "text": "You can download MongoDB from the  MongoDb website .  Instructions for starting mongo can be found in the MongoDB  manual . For example:  mongod --dbpath /data/db --port 27017",
            "title": "Start MongoDB"
        },
        {
            "location": "/deploy/Running-the-platform/#get-the-code",
            "text": "Get the latest version of the code, and checkout the  dev  branch. Please don't commit to the  master  branch: see the  Development Guidelines :  If you are using  Linux / OS X :  git clone https://github.com/crossminer/scava.git scava\ncd scava\ngit checkout dev  If you are using  Windows  you need to do things differently due to Windows' long file name limit. In the Git shell:  mkdir scava\ncd scava\ngit init\ngit config core.longpaths true\ngit add remote origin https://github.com/crossminer/scava.git\ngit fetch\ngit checkout dev",
            "title": "Get the Code"
        },
        {
            "location": "/deploy/Running-the-platform/#setup-eclipse",
            "text": "Open Eclipse and import all projects from the top level directory of the Scava code ( File -> Import -> Existing projects into workspace ), and wait for all the projects to compile without errors.",
            "title": "Setup Eclipse"
        },
        {
            "location": "/deploy/Running-the-platform/#validate-and-run-the-platform",
            "text": "Open  org.ossmeter.platform.osgi/ossmeterfromfeature.product \n  * Click the  Validate...  icon in the top right of the product configuration editor (the icon is a piece of paper with a tick)\n  * If things do not validate, there's something wrong -- get in touch :) Problems related to  org.eclipse.e4.core.di  aren't critical.\n  * Then, click the  Export an Eclipse product  on the left of the  Validate...  button. Uncheck the  Generate p2 repository  checkbox, select a destination directory and validate. After a while, the OSSMETER platform will be generated in the selected directory.\n  * The platform can then be run using the generated  eclipse  binary; it accepts the following arguments:\n    *  -apiServer : Starts up the client API on localhost:8182\n    *  -worker ${id-worker} : Spawns a thread that analyses registered projects\n  * To get a full platform running, first launch a master thread, then a slave, and finally the API server.  If you are developing code for the Scava platform, be sure to check out the  Contributing .",
            "title": "Validate and Run the Platform"
        },
        {
            "location": "/deploy/Running-the-platform/#run-the-api-gateway",
            "text": "Right click on scava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java  Then click on Run As -> Java Application",
            "title": "Run the api-gateway"
        },
        {
            "location": "/deploy/Running-the-platform/#run-the-authentication-service",
            "text": "Right click on scava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java  Then click on Run As -> Java Application",
            "title": "Run the authentication service"
        },
        {
            "location": "/deploy/Running-the-platform/#run-the-administration-dashboard",
            "text": "Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart).  The following instructions show how to run the dashboard web app:\n  * Enter the  administration/scava-administration/  directory within the scava repository.\n  * Run the web app on port 4200 using angular-cli:  ng serve \n  * Navigate to  http://localhost:4200/",
            "title": "Run the administration dashboard"
        },
        {
            "location": "/development/Component-Naming/",
            "text": "As consequence of our status of project hosted by the eclipse foundation, we have to follow a specific naming schema for all components implemented in context of SCAVA project.\n\n\nIn this section, \"component\" means big funcional componen of the SCAVA project.\n\n\n\n\n\n\n\n\nComponent\n\n\nComponentId\n\n\n\n\n\n\n\n\n\n\nDevOps Dashboard\n\n\ndashboard\n\n\n\n\n\n\nWorkflow Execution Engine\n\n\nworkflow\n\n\n\n\n\n\nKnowledge Base\n\n\nknowledgebase\n\n\n\n\n\n\nMetric Provider\n\n\nmetricprovider\n\n\n\n\n\n\nAdministration\n\n\nadministration\n\n\n\n\n\n\n\n\nProject Naming\n\n\n\n\nFor Eclipse Plugin \n\n\n\n\norg.eclipse.scava.{componentname}\n\n\n\n\n\n\nFor Maven Project : the name of the project if the ArtifactId\n\n\n\n\nIf your component is composed of one project :\n\n\n{component-name}\n\n\n\n\nIf your component is composed of several sub projects :\n\n\n{sub-component-name}\n ```\n\n## Source Code Namsespace\nAll sources must be nemspaces by : \n\n\n\n\norg.eclipse.scava.{componentname}\n\n\n\n## Maven Ids\nFor  the Maven projects:\n\n* If your component is composed of one project : \n\n\n\n\nGroup Id : org.eclipse.scava\nArtifactId : {component-name}\n\n\n\n* If your component is composed of several sub projects : \n\n\n\n\nGroup Id : org.eclipse.scava.{component-name}\nArtifactId : {sub-component-name}\n```",
            "title": "Component Naming"
        },
        {
            "location": "/development/Component-Naming/#project-naming",
            "text": "For Eclipse Plugin    org.eclipse.scava.{componentname}   For Maven Project : the name of the project if the ArtifactId   If your component is composed of one project :  {component-name}  If your component is composed of several sub projects :  {sub-component-name}\n ```\n\n## Source Code Namsespace\nAll sources must be nemspaces by :   org.eclipse.scava.{componentname}  \n## Maven Ids\nFor  the Maven projects:\n\n* If your component is composed of one project :   Group Id : org.eclipse.scava\nArtifactId : {component-name}  \n* If your component is composed of several sub projects :   Group Id : org.eclipse.scava.{component-name}\nArtifactId : {sub-component-name}\n```",
            "title": "Project Naming"
        },
        {
            "location": "/development/Contributing/",
            "text": "Contributing\n\n\nSubcategories\n\n\n\n\nSCAVA Repository Organisation\n Guideline describing how the SCAVA code repository is organised and how to add a new component in this repository.\n\n\nHow to name SCAVA components?\n Guideline describing naming constraints for a new scava component (component name, java namespace, maven artefact id and group id, etc.\n\n\nHow to name SCAVA REST services?\n This guideline provide naming rules for each REST services routes implemented by the SCAVA platform.\n\n\nHow to manage  Licensing\n Guideline describing licensing requirements for SCAVA components.\n\n\n\n\nTechnical Guidelines\n\n\nREST API\n\n\nEach implemented REST services must be documented (see /users directory): \nREST API DOCUMENTATION\n\n\n\n\nHow to configure the SCAVA Gateway in order to integrate a new  REST service\n Customers access SCAVA services through the SCAVA API Gateway. This guideline present how to configure the API Gateway to integrate new  remote service provider.\n\n\nHow to consume a SCAVA REST services\n This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues.\n\n\nHow to implement Restlet services\n Guideline which describe how to integrate and implement a REST service in SCAVA platform using the RESTLET framework.\n\n\n\n\nDATA ACCESS\n\n\n\n\n\n\nHow to access MongoDB database using PONGO\n Guideline which describe how to the access to MongoDB database using the  PONGO framework.\n\n\n\n\n\n\nHow to extend the SCAVA data model\n Guideline  which describe ways to extend the SCAVA Data Model, stored in a MongoDb database and based on the PONGO framework.\n\n\n\n\n\n\nOSGI\n\n\n\n\nHow to integrate OSGI service plugin in SCAVA Architecture\n Todo\n\n\nHow to communicate between OSGI plugin using JMS\n Todo",
            "title": "Contributing"
        },
        {
            "location": "/development/Contributing/#contributing",
            "text": "",
            "title": "Contributing"
        },
        {
            "location": "/development/Contributing/#subcategories",
            "text": "SCAVA Repository Organisation  Guideline describing how the SCAVA code repository is organised and how to add a new component in this repository.  How to name SCAVA components?  Guideline describing naming constraints for a new scava component (component name, java namespace, maven artefact id and group id, etc.  How to name SCAVA REST services?  This guideline provide naming rules for each REST services routes implemented by the SCAVA platform.  How to manage  Licensing  Guideline describing licensing requirements for SCAVA components.",
            "title": "Subcategories"
        },
        {
            "location": "/development/Contributing/#technical-guidelines",
            "text": "",
            "title": "Technical Guidelines"
        },
        {
            "location": "/development/Contributing/#rest-api",
            "text": "Each implemented REST services must be documented (see /users directory):  REST API DOCUMENTATION   How to configure the SCAVA Gateway in order to integrate a new  REST service  Customers access SCAVA services through the SCAVA API Gateway. This guideline present how to configure the API Gateway to integrate new  remote service provider.  How to consume a SCAVA REST services  This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues.  How to implement Restlet services  Guideline which describe how to integrate and implement a REST service in SCAVA platform using the RESTLET framework.",
            "title": "REST API"
        },
        {
            "location": "/development/Contributing/#data-access",
            "text": "How to access MongoDB database using PONGO  Guideline which describe how to the access to MongoDB database using the  PONGO framework.    How to extend the SCAVA data model  Guideline  which describe ways to extend the SCAVA Data Model, stored in a MongoDb database and based on the PONGO framework.",
            "title": "DATA ACCESS"
        },
        {
            "location": "/development/Contributing/#osgi",
            "text": "How to integrate OSGI service plugin in SCAVA Architecture  Todo  How to communicate between OSGI plugin using JMS  Todo",
            "title": "OSGI"
        },
        {
            "location": "/development/Development-Guidelines/",
            "text": "Development Guidelines\n\n\nIntroduction\n\n\nThis document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development.\n\n\nProcess overview\n\n\nThe development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system.\n\n\nIt is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests.\n\n\nThe different elements shown below are individually described in the remaining of the section.\n\n\n\nFigure 1. Overview of the CROSSMINER development process and tools\n\n\nSource Code Repository\n\n\nDifferent branches will be created in the repository. In particular, the \nmaster\n branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The \ndev\n branch contains the most recent code integrated by partners before release.\n\n\nOther branches will be created for the different features/components, which will be eventually merged in the \ndev\n branch. The name of each branch contains the id of the issue it is supposed to implement.\n\n\nAll source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository.\n\n\n\n\nCode: https://github.com/crossminer/scava\n\n\nProduct documentation: https://github.com/crossminer/scava-docs\n\n\nProduct deployment: https://github.com/crossminer/scava-deployment\n\n\n\n\nSince all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details).\n\n\nBy taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3.\n\n\n\nFigure 2: Branching model\n\n\nTests\n\n\nEach branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below.\n\n\n\nFigure 2: Explanatory master and branches\n\n\nBRANCH: Set of branches\nTEST: Set of tests\nINTGRATION_TEST < TEST\nUNIT_TEST < TEST\n\nfunction test: BRANCH -> TEST\n\ntest(master) -> test(cool-feature)\n    \u22c3 {itu, \u2026 itz}\n\n\n\n\nContinuous Integration Server\n\n\nContinuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus \nreduce cost and time\n.\n\n\nA complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all \nunit tests\n. \nIntegration tests\n are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities:\n\n\n\n\nAllow developers to write unit tests that can be executed automatically\n\n\nPerform automated tests against newly written code\n\n\nShow a list of tests that have passed and failed\n\n\nQuality analysis on source code\n\n\nPerform all the necessary actions to create a fully functioning build of the software when all tests have passed\n\n\n\n\nThe currently used CI server is available at http://ci5.castalia.camp:8080/\n\n\nDevelopment and Production environments\n\n\nAccording to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform.\n\n\nNaming and change conventions\n\n\nThe repository will contain the code of the high-level components shown in Table 1\n\n\n\n\n\n\n\n\nComponents\n\n\nCorresponding folder in repository\n\n\nLeader\n\n\n\n\n\n\n\n\n\n\nDevOps Dashboard\n\n\n\n\nBIT\n\n\n\n\n\n\nWorkflow Diagram Editor\n\n\n/crossflow\n\n\nYORK\n\n\n\n\n\n\nAdministration Web Application\n\n\n/administration\n\n\nSFT\n\n\n\n\n\n\nIDE\n\n\n/eclipse-based-ide\n\n\nFEA\n\n\n\n\n\n\nAPI Gateway\n\n\n/api-gateway\n\n\nSFT\n\n\n\n\n\n\nDevOps Backend\n\n\n\n\nBIT\n\n\n\n\n\n\nKnowledge Base\n\n\n/knowledge-base\n\n\nUDA\n\n\n\n\n\n\nProject Analyser\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nData Collector\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nProject Scheduler\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nMetric Providers\n\n\n/metric-platform\n\n\nYORK, CWI, AUEB, EHU\n\n\n\n\n\n\nData Storage\n\n\n/metric-platform\n\n\nSFT\n\n\n\n\n\n\nWeb-based dashboards\n\n\n/web-dashboards\n\n\nBIT\n\n\n\n\n\n\nTable 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Scava components architecture\n\n\nFor each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus:\n\n\n\n\nwhen developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons;\n\n\nwhen a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR.\n\n\n\n\nThe partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder:\n\n\n\n\nreadme.md\n : this \ufb01le will contain the entry-point to all documentation for the component.\n\n\nJenkinsfile\n: in order to compile the component, run tests, etc on the CI server.\n\n\ntest\n: this is a folder containing the unit and/or integration tests.\n\n\n\n\nAs a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base\n\n\nThe following items are typical practices when applying CI:\n\n Maintain a code repository\n\n Automate the build\n\n Make the build self-testing\n\n Everyone commits to the baseline every day\n\n Every commit (to baseline) should be built\n\n Keep the build fast\n\n Test in a clone of the production environment\n\n Make it easy to get the latest deliverables\n\n Everyone can see the results of the latest build\n\n Automate deployment\n\n\nCommunication and collaboration means\n\n\nThe development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of:\n\n\n\n\nSlack channel: http://crossminer.slack.com\n\n\nEclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev\n\n\nGitHub repository for documentation: https://github.com/crossminer/scava-docs/\n\n\n\n\nRemark\n\n\nI recall that we are running a research and innovative project. Reviewers that will evaluate our work will not be impressed by the fact that 100% of our tests succeed, that we perfectly adhere to a detailed code convention, etc. Of course, the community will do!!! Thus, we have to find a reasonably compromise:\n\n\n\n\nto have all the deliverables accepted !!!\n\n\nto develop in a way, which is preparatory to achieve our \u201cdreams\u201d\n\n\n\n\n\u201cBetter have a low test coverage than a part of our tests fail\u201d [cit. Boris] :-)",
            "title": "Development Guidelines"
        },
        {
            "location": "/development/Development-Guidelines/#development-guidelines",
            "text": "",
            "title": "Development Guidelines"
        },
        {
            "location": "/development/Development-Guidelines/#introduction",
            "text": "This document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development.",
            "title": "Introduction"
        },
        {
            "location": "/development/Development-Guidelines/#process-overview",
            "text": "The development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system.  It is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests.  The different elements shown below are individually described in the remaining of the section.  \nFigure 1. Overview of the CROSSMINER development process and tools",
            "title": "Process overview"
        },
        {
            "location": "/development/Development-Guidelines/#source-code-repository",
            "text": "Different branches will be created in the repository. In particular, the  master  branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The  dev  branch contains the most recent code integrated by partners before release.  Other branches will be created for the different features/components, which will be eventually merged in the  dev  branch. The name of each branch contains the id of the issue it is supposed to implement.  All source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository.   Code: https://github.com/crossminer/scava  Product documentation: https://github.com/crossminer/scava-docs  Product deployment: https://github.com/crossminer/scava-deployment   Since all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details).  By taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3.  \nFigure 2: Branching model",
            "title": "Source Code Repository"
        },
        {
            "location": "/development/Development-Guidelines/#tests",
            "text": "Each branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below.  \nFigure 2: Explanatory master and branches  BRANCH: Set of branches\nTEST: Set of tests\nINTGRATION_TEST < TEST\nUNIT_TEST < TEST\n\nfunction test: BRANCH -> TEST\n\ntest(master) -> test(cool-feature)\n    \u22c3 {itu, \u2026 itz}",
            "title": "Tests"
        },
        {
            "location": "/development/Development-Guidelines/#continuous-integration-server",
            "text": "Continuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus  reduce cost and time .  A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all  unit tests .  Integration tests  are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities:   Allow developers to write unit tests that can be executed automatically  Perform automated tests against newly written code  Show a list of tests that have passed and failed  Quality analysis on source code  Perform all the necessary actions to create a fully functioning build of the software when all tests have passed   The currently used CI server is available at http://ci5.castalia.camp:8080/",
            "title": "Continuous Integration Server"
        },
        {
            "location": "/development/Development-Guidelines/#development-and-production-environments",
            "text": "According to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform.",
            "title": "Development and Production environments"
        },
        {
            "location": "/development/Development-Guidelines/#naming-and-change-conventions",
            "text": "The repository will contain the code of the high-level components shown in Table 1     Components  Corresponding folder in repository  Leader      DevOps Dashboard   BIT    Workflow Diagram Editor  /crossflow  YORK    Administration Web Application  /administration  SFT    IDE  /eclipse-based-ide  FEA    API Gateway  /api-gateway  SFT    DevOps Backend   BIT    Knowledge Base  /knowledge-base  UDA    Project Analyser  /metric-platform  SFT    Data Collector  /metric-platform  SFT    Project Scheduler  /metric-platform  SFT    Metric Providers  /metric-platform  YORK, CWI, AUEB, EHU    Data Storage  /metric-platform  SFT    Web-based dashboards  /web-dashboards  BIT    Table 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes       \nFigure 2: Scava components architecture  For each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus:   when developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons;  when a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR.   The partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder:   readme.md  : this \ufb01le will contain the entry-point to all documentation for the component.  Jenkinsfile : in order to compile the component, run tests, etc on the CI server.  test : this is a folder containing the unit and/or integration tests.   As a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base  The following items are typical practices when applying CI:  Maintain a code repository  Automate the build  Make the build self-testing  Everyone commits to the baseline every day  Every commit (to baseline) should be built  Keep the build fast  Test in a clone of the production environment  Make it easy to get the latest deliverables  Everyone can see the results of the latest build  Automate deployment",
            "title": "Naming and change conventions"
        },
        {
            "location": "/development/Development-Guidelines/#communication-and-collaboration-means",
            "text": "The development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of:   Slack channel: http://crossminer.slack.com  Eclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev  GitHub repository for documentation: https://github.com/crossminer/scava-docs/",
            "title": "Communication and collaboration means"
        },
        {
            "location": "/development/Development-Guidelines/#remark",
            "text": "I recall that we are running a research and innovative project. Reviewers that will evaluate our work will not be impressed by the fact that 100% of our tests succeed, that we perfectly adhere to a detailed code convention, etc. Of course, the community will do!!! Thus, we have to find a reasonably compromise:   to have all the deliverables accepted !!!  to develop in a way, which is preparatory to achieve our \u201cdreams\u201d   \u201cBetter have a low test coverage than a part of our tests fail\u201d [cit. Boris] :-)",
            "title": "Remark"
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/",
            "text": "Extending MongoDB data model\n\n\nWhen to use ?\n\n\nIn this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order  the data layer of Scava platform during the evolution.\n\n\nContext\n\n\nThe Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class.\n\n\nEach MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model.\n\n\nThe current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins :\n\n\n\n\nThe Platform data model (org.ossmeter.repository.model)\n : Contains data related to project administration ,  metric execution process and authentification system.\n\n\nSource Code Repositroy managers (org.ossmeter.repository.model.'project delta manager')\n : Contains configuration informations related to source codes managemeny tools.\n\n\nMetric Providers data models (in each metric provider plugins)\n :  Each metric provider implementation contains his once data model.\n\n\n\n\nYou need to Extend an Existing Data Model ?\n\n\nThe first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same.\n\n\n1. Locate the *.emf file of this data model\n\n\nA presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model.\n\n\nEx : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf)\n\n\n2. Update the Data  Model description\n\n\nA data model description file contains a statical description of a MongoDb document.\n\n\n@db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n    attr String stackTrace;\n}\n...\n\n\n\n\nIf we would like to add one more attribute to the element \nProjectError\n, we could add it this way :\n\n\n@db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n        .\n        .\n    attr String stackTrace;\n+       attr String TestAttribute;\n}\n...\n\n\n\n\nYou can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines\n\n\n3. Generate the Java Class using the Pongo Tool.\n\n\n\n\nDownload the Pongo tool : https://github.com/kolovos/pongo/releases\n\n\nRun the Pongo generator from the command line as follows: \njava -jar pongo.jar  youremffile.emf\n\n\nReplace the existing Java class by the new generated java class.\n\n\n\n\nMore information about Pongo  : https://github.com/kolovos/pongo/wiki\n\n\nYou need to Create a new Data Model ?\n\n\nThe second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model.\n\n\nIn this case, we invite you to create a new plugin which will contain your data model.\n\n\n1. Create a new Eclipse Plug-In\n\n\n\n\nCreate a new Eclipse Plug-In Project (\n\n\nIn Eclipse Toolbar : File > New > Plug-In Project\n\n\nName of the project : org.scava.\nmycomponent\n.repository.model\n\n\nDisable the generation of an Activator Class / contribution to the ui\n\n\nEdit the MANIFEST.MF file\n\n\nIn Dependency : add a dependency to the \norg.eclipse.core.runtime\n plugin\n\n\nIn Dependency : add a dependency to the \ncom.googlecode.pongo.runtime\n plugin\n\n\nIn Dependency : add a dependency to the \norg.apache.commons.lang3\n plugin\n\n\nIn Extentions : reference an extension point named \ncom.googlecode.pongo.runtime.osgi\n\n\nIn source directory\n\n\nCreate a package named org.scava.\nmycomponent\n.repository.model\n\n\nIn this package create an emf file named \nmycomponent.emf\n\n\n\n\nA presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file :\n\n\n```package org.scava.mycomponent.repository.model;\n\n\n@db\nclass MyComponent {\n     ....\n}\n```\nYou can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines\n\n\n2. Generate the Java Class using the Pongo Tool.\n\n\n\n\nDownload the Pongo tool : https://github.com/kolovos/pongo/releases\n\n\nRun the Pongo generator from the command line as follows: \njava -jar pongo.jar  youremffile.emf\n\n\nAdd this class in your org.scava.\nmycomponent\n.repository.model package\n\n\n\n\nMore information about Pongo  : https://github.com/kolovos/pongo/wiki\n\n\nComment\n\n\nHere we learnt ways to modify model in the Scava platform. To know more about the access of data with the Pongo APIs \nlink here\n.",
            "title": "Extend MongoDB Data Model"
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/#extending-mongodb-data-model",
            "text": "",
            "title": "Extending MongoDB data model"
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/#when-to-use",
            "text": "In this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order  the data layer of Scava platform during the evolution.",
            "title": "When to use ?"
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/#context",
            "text": "The Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class.  Each MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model.  The current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins :   The Platform data model (org.ossmeter.repository.model)  : Contains data related to project administration ,  metric execution process and authentification system.  Source Code Repositroy managers (org.ossmeter.repository.model.'project delta manager')  : Contains configuration informations related to source codes managemeny tools.  Metric Providers data models (in each metric provider plugins)  :  Each metric provider implementation contains his once data model.",
            "title": "Context"
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/#you-need-to-extend-an-existing-data-model",
            "text": "The first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same.",
            "title": "You need to Extend an Existing Data Model ?"
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/#1-locate-the-emf-file-of-this-data-model",
            "text": "A presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model.  Ex : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf)",
            "title": "1. Locate the *.emf file of this data model"
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/#2-update-the-data-model-description",
            "text": "A data model description file contains a statical description of a MongoDb document.  @db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n    attr String stackTrace;\n}\n...  If we would like to add one more attribute to the element  ProjectError , we could add it this way :  @db\nclass ProjectRepository extends NamedElement {\n  val Project[*] projects;\n  val Role[*] roles;\n  .\n  val ProjectError[*] errors;\n}\n\n@customize\nclass ProjectError {\n    attr Date date;\n        .\n        .\n        .\n    attr String stackTrace;\n+       attr String TestAttribute;\n}\n...  You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines",
            "title": "2. Update the Data  Model description"
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/#3-generate-the-java-class-using-the-pongo-tool",
            "text": "Download the Pongo tool : https://github.com/kolovos/pongo/releases  Run the Pongo generator from the command line as follows:  java -jar pongo.jar  youremffile.emf  Replace the existing Java class by the new generated java class.   More information about Pongo  : https://github.com/kolovos/pongo/wiki",
            "title": "3. Generate the Java Class using the Pongo Tool."
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/#you-need-to-create-a-new-data-model",
            "text": "The second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model.  In this case, we invite you to create a new plugin which will contain your data model.",
            "title": "You need to Create a new Data Model ?"
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/#1-create-a-new-eclipse-plug-in",
            "text": "Create a new Eclipse Plug-In Project (  In Eclipse Toolbar : File > New > Plug-In Project  Name of the project : org.scava. mycomponent .repository.model  Disable the generation of an Activator Class / contribution to the ui  Edit the MANIFEST.MF file  In Dependency : add a dependency to the  org.eclipse.core.runtime  plugin  In Dependency : add a dependency to the  com.googlecode.pongo.runtime  plugin  In Dependency : add a dependency to the  org.apache.commons.lang3  plugin  In Extentions : reference an extension point named  com.googlecode.pongo.runtime.osgi  In source directory  Create a package named org.scava. mycomponent .repository.model  In this package create an emf file named  mycomponent.emf   A presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file :  ```package org.scava.mycomponent.repository.model;  @db\nclass MyComponent {\n     ....\n}\n```\nYou can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines",
            "title": "1. Create a new Eclipse Plug-In"
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/#2-generate-the-java-class-using-the-pongo-tool",
            "text": "Download the Pongo tool : https://github.com/kolovos/pongo/releases  Run the Pongo generator from the command line as follows:  java -jar pongo.jar  youremffile.emf  Add this class in your org.scava. mycomponent .repository.model package   More information about Pongo  : https://github.com/kolovos/pongo/wiki",
            "title": "2. Generate the Java Class using the Pongo Tool."
        },
        {
            "location": "/development/Extend-MongoDB-Data-Model/#comment",
            "text": "Here we learnt ways to modify model in the Scava platform. To know more about the access of data with the Pongo APIs  link here .",
            "title": "Comment"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/",
            "text": "How to Develop a Metric Provider\n\n\nIn this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a \ntransient\n metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a \nhistorical\n metric provider, will count the total number of commits over time. \n\n\nWe'll go through the steps above for each metric provider.\n\n\nPre-requisites\n\n\n\n\nEclipse \n\n\nThe \nEmfatic\n plug-in should be installed in your Eclipse\n\n\nThe \nPongo\n plug-in should be installed in your Eclipse\n\n\nThe OSSMETER source code should be in your workspace\n\n\n\n\nThe Transient Metric Provider\n\n\nThis metric provider will store a complete history of the commits in the version control system(s) used by a project. \n\n\n0. Setup\n\n\nCreate a new Plugin project in Eclipse.\n\n\n\n\nGo to File > New > Project... and select 'Plug-in project'\n\n\nGive the project an appropriate name. The OSSMETER naming convention is:\n\n\nTransient metrics: org.ossmeter.metricprovider.trans.(metric name)\n\n\nHistorical metrics: org.ossmeter.metricprovider.historical.(metric name)\n\n\n\n\n\n\nClick Next\n\n\nIf the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it\n\n\nClick Finish\n\n\nOpen up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.\n\n\n\n\n1. The data model\n\n\nWe define the data model using the Emfatic language. In your newly created plug-in, create a package called \norg.ossmeter.metricprovider.trans.commits.model\n. In that package create an empty file called \ncommits.emf\n. In this file, we will define our data model.\n\n\nFirst of all, we need to state the name of the package.\n\n\npackage org.ossmeter.metricprovider.trans.commits.model;\n\n\n\n\nThis is used by the Pongo code generator - the generated classes will be put in this package.\n\n\nWe then define the database for our model:\n\n\n@db(qualifiedCollectionNames=\"true\")\nclass Commits {\n    val Repository[*] repositories;\n    val Commit[*] commits;\n}\n\n\n\n\nThe \n@db\n annotation tells Pongo that this will be the container database for the data. Adding the \nqualifiedCollectionNames=true\n property will prepend the database name to all Mongo collections. \n\n\nThe \nCommits\n class above says that we want a database with two collections, name \nrepositories\n and \ncommits\n. If \nqualifiedCollectionNames\n is set to \ntrue\n, the collections will be named \nCommits.repositories\n and \nCommits.commits\n. \n\n\nWe now define the schema of the \nCommits.repositories\n collection:\n\n\n~~~java\nclass Repository {\n    @searchable\n    attr String url;\n    attr String repoType;\n    attr String revision;\n    attr int totalNumberOfCommits;\n    ref CommitData[*] commits;\n}\n\n\n\nThis class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection.\n\nThe `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time.\n\nEach commit is represented in the `Commits.commits` collection by the following model:\n\n~~~~java\nclass Commit {\n    @searchable\n    attr Date date;\n    attr String identifier;\n    attr String message;\n    attr String author;\n}\n\n\n\n\nFor each commit, we store its \ndate\n, \nidentifier\n (revision ID), the commit \nmessage\n, and the \nauthor\n. We also create an index on the \ndate\n to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated \nfindByDate(String date)\n or \nfindOneByDate(String date)\n methods).\n\n\nNow we need to use Pongo to generate code from this model. Right-click on the \ncommits.emf\n file and select \nPongo > Generate Pongos and plugin.xml\n. You should see some Java classes appear in your package.\n\n\nNow that we have our data model, we can implement the metric provider.\n\n\n2. The metric provider\n\n\nCreate a Java class called \norg.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider\n\n\nThis class should extend \nAbstractTransientMetricProvider\n and specify \nCommits\n for the generic argument:\n\n\npublic class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits>\n\n\n\n\nThe generic argument states that the metric provider stores objects conforming to the \nCommits\n data model. Note: You do not need to extend \nAbstractTransientMetricProvider\n and can implement \nITransientMetricProvider\n instead should you wish to.\n\n\nThere are a number of methods that need implementing. We will discuss each in turn.\n\n\nadapt(DB db)\n\n\nThis method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows:\n\n\n    @Override\n    public Commits adapt(DB db) {\n        return new Commits(db);\n    }\n\n\n\n\n\nThe next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider:\n\n\n    @Override\n    public String getShortIdentifier() { // This may be deprecated very soon\n        return \"transient-commits\"; \n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Commit History\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"The commit history of the project.\";\n    }\n\n\n\n\nThe next method allows you to declare whether the metric provider is applicable to a given project:\n\n\n@Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }\n\n\n\n\nOur metric applies to any project that has at least one VCS repository.\n\n\nFinally, we have the \nmeasure(...)\n method that performs the actual metric calculation:\n\n\n    @Override\n    public void measure(Project project, ProjectDelta delta, Commits db) {\n\n        for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) {\n            Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl());\n\n            if (repo == null) {\n                repo = new Repository();\n                repo.setUrl(repoDelta.getRepository().getUrl());\n                db.getRepositories().add(repo);\n            }\n\n            for (VcsCommit commit : repoDelta.getCommits()) {\n                Commit c = new Commit();\n                c.setDate(commit.getJavaDate());\n                c.setMessage(commit.getMessage());\n                c.setAuthor(commit.getAuthor());\n                c.setIdentifier(commit.getRevision());\n\n                repo.getCommits().add(c);\n                db.getCommits().add(c);\n            }\n        }\n        db.getCommits().sync();\n        db.getRepositories().sync();\n    }\n\n\n\n\nThe above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new \nCommit\n object is created for each commit in the delta.\n\n\n3. Make the metric provider discoverable\n\n\nMetric providers are registered with the OSSMETER platform using \nextension points\n:\n\n\n\n\nOpen up the plugin.xml file and select the 'Extensions' tab\n\n\nClick the Add button and from the list select \norg.ossmeter.platform.metricprovider\n\n\nOn the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.\n\n\nClick the 'Browse...' button and select the 'CommitsTransientMetricProvider' class\n\n\n\n\nNow everything is ready for the metric to be executed :)\n\n\nThe Historic Metric Provider\n\n\nThis metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value.  \n\n\n0. Setup\n\n\nCreate a new Plugin project in Eclipse.  \n\n\n\n\nGo to File > New > Project... and select 'Plug-in project'\n\n\nGive the project an appropriate name. The OSSMETER naming convention is:\n\n\nTransient metrics: org.ossmeter.metricprovider.trans.(metric name)\n\n\nHistorical metrics: org.ossmeter.metricprovider.historical.(metric name)\n\n\n\n\n\n\nClick Next\n\n\nIf the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it\n\n\nClick Finish\n\n\nOpen up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.\n\n\n\n\n1. The data model\n\n\nIn your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model:\n\n\npackage org.ossmeter.metricprovider.historic.commits.model;\n\nclass HistoricCommits {\n    attr int numberOfCommits;\n}\n\n\n\n\nThe data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed.\n\n\n2. The metric provider\n\n\nCreate a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider\n\n\nThis class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)):\n\n\npublic class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider\n\nThe generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to.\n\n\nThere are a number of methods that need implementing. We will discuss each in turn.\n\n\nFirst of all, complete the typical information-related methods:\n\n\n    @Override\n    public String getShortIdentifier() {\n        return \"historicalcommits\";\n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Historical commits\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"...\";\n    }\n\n\n\n\nNow complete the standard \nappliesTo\n method:\n\n\n    @Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }\n\n\n\n\nWe now need to specify a dependency on the transient metric provider that we just implemented.\n\n\n    @Override\n    public List<String> getIdentifiersOfUses() {\n        return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName());\n    }   \n\n\n\n\nThis tells the platform that we need access to the \nCommitsTransientMetricProvider\n database. The platform will assign this to the \nuses\n field that is available to the historical metric provider, as you'll see in the \nmeasure\n method:\n\n\n    @Override\n    public Pongo measure(Project project) {\n        Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0));\n\n        int commits = (int) transDb.getCommits().size();\n\n        HistoricCommits hist = new HistoricCommits();\n        hist.setNumberOfCommits(commits);\n\n        return hist;\n    }\n\n\n\n\nFirst of all, we get hold of the database of the transient commits metric provider, and simply count the size of the \ncommits\n collection. We save this in an instance of the \nHistoricCommits\n Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project.\n\n\n3. Make the metric provider discoverable\n\n\nThis process is the same as the transient metric provider:\n\n\n\n\nOpen up the plugin.xml file and select the 'Extensions' tab\n\n\nClick the Add button and from the list select \norg.ossmeter.platform.metricprovider\n\n\nOn the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.\n\n\nClick the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class\n\n\n\n\nNow everything is ready for both metrics to be executed :)\n\n\nBut first. Let's specify how we want this historical metric to be visualised.\n\n\n4. Define a MetVis visualisation specification\n\n\nMetVis\n is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis \nweb page\n has numerous examples of this.\n\n\nCreate a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider:\n\n\n{\n    \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\",\n    \"vis\" : [\n        {\n            \"id\" : \"historicalcommits\",\n            \"name\" : \"Commits over time\",\n            \"description\" : \"This metric shows when the projects commits occurred\",\n            \"type\" : \"LineChart\",\n            \"datatable\" : {\n                \"cols\" : [\n                    { \"name\" : \"Date\", \"field\" : \"$__date\" },\n                    { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" }\n                ]\n            },\n            \"x\" : \"Date\",\n            \"y\" : \"Commits\"\n        }\n    ]\n}\n\n\n\n\nThe \nmetricId\n field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use \n$__date\n. The final two fields (\nx\n and \ny\n) are references to column names in the datatable specification. The \ntype\n states that the data should be plotted as a line chart. You can test your MetVis specifications on the \nMetVis playpen\n.\n\n\n5. Make the visualisation specification discoverable\n\n\nAs with metric providers, visualisation specifications are registered using extension points.\n\n\n\n\nAdd the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project\n\n\nOpen up the plugin.xml file and select the 'Extensions' tab\n\n\nClick the Add button and from the list select \norg.ossmeter.platform.visualisation.metric\n\n\nOn the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.\n\n\nClick the 'Browse...' button and select the 'historicalcommits.json' file\n\n\n\n\nGood job.\n\n\nRunning the metric providers\n\n\nSee \nRunning from Source\n\n\nHomework\n\n\nAdapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.",
            "title": "How To Develop Metric Provider"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#how-to-develop-a-metric-provider",
            "text": "In this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a  transient  metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a  historical  metric provider, will count the total number of commits over time.   We'll go through the steps above for each metric provider.",
            "title": "How to Develop a Metric Provider"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#pre-requisites",
            "text": "Eclipse   The  Emfatic  plug-in should be installed in your Eclipse  The  Pongo  plug-in should be installed in your Eclipse  The OSSMETER source code should be in your workspace",
            "title": "Pre-requisites"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#the-transient-metric-provider",
            "text": "This metric provider will store a complete history of the commits in the version control system(s) used by a project.",
            "title": "The Transient Metric Provider"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#0-setup",
            "text": "Create a new Plugin project in Eclipse.   Go to File > New > Project... and select 'Plug-in project'  Give the project an appropriate name. The OSSMETER naming convention is:  Transient metrics: org.ossmeter.metricprovider.trans.(metric name)  Historical metrics: org.ossmeter.metricprovider.historical.(metric name)    Click Next  If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it  Click Finish  Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.",
            "title": "0. Setup"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#1-the-data-model",
            "text": "We define the data model using the Emfatic language. In your newly created plug-in, create a package called  org.ossmeter.metricprovider.trans.commits.model . In that package create an empty file called  commits.emf . In this file, we will define our data model.  First of all, we need to state the name of the package.  package org.ossmeter.metricprovider.trans.commits.model;  This is used by the Pongo code generator - the generated classes will be put in this package.  We then define the database for our model:  @db(qualifiedCollectionNames=\"true\")\nclass Commits {\n    val Repository[*] repositories;\n    val Commit[*] commits;\n}  The  @db  annotation tells Pongo that this will be the container database for the data. Adding the  qualifiedCollectionNames=true  property will prepend the database name to all Mongo collections.   The  Commits  class above says that we want a database with two collections, name  repositories  and  commits . If  qualifiedCollectionNames  is set to  true , the collections will be named  Commits.repositories  and  Commits.commits .   We now define the schema of the  Commits.repositories  collection:  ~~~java\nclass Repository {\n    @searchable\n    attr String url;\n    attr String repoType;\n    attr String revision;\n    attr int totalNumberOfCommits;\n    ref CommitData[*] commits;\n}  \nThis class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection.\n\nThe `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time.\n\nEach commit is represented in the `Commits.commits` collection by the following model:\n\n~~~~java\nclass Commit {\n    @searchable\n    attr Date date;\n    attr String identifier;\n    attr String message;\n    attr String author;\n}  For each commit, we store its  date ,  identifier  (revision ID), the commit  message , and the  author . We also create an index on the  date  to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated  findByDate(String date)  or  findOneByDate(String date)  methods).  Now we need to use Pongo to generate code from this model. Right-click on the  commits.emf  file and select  Pongo > Generate Pongos and plugin.xml . You should see some Java classes appear in your package.  Now that we have our data model, we can implement the metric provider.",
            "title": "1. The data model"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#2-the-metric-provider",
            "text": "Create a Java class called  org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider  This class should extend  AbstractTransientMetricProvider  and specify  Commits  for the generic argument:  public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits>  The generic argument states that the metric provider stores objects conforming to the  Commits  data model. Note: You do not need to extend  AbstractTransientMetricProvider  and can implement  ITransientMetricProvider  instead should you wish to.  There are a number of methods that need implementing. We will discuss each in turn.",
            "title": "2. The metric provider"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#adaptdb-db",
            "text": "This method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows:      @Override\n    public Commits adapt(DB db) {\n        return new Commits(db);\n    }  The next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider:      @Override\n    public String getShortIdentifier() { // This may be deprecated very soon\n        return \"transient-commits\"; \n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Commit History\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"The commit history of the project.\";\n    }  The next method allows you to declare whether the metric provider is applicable to a given project:  @Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }  Our metric applies to any project that has at least one VCS repository.  Finally, we have the  measure(...)  method that performs the actual metric calculation:      @Override\n    public void measure(Project project, ProjectDelta delta, Commits db) {\n\n        for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) {\n            Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl());\n\n            if (repo == null) {\n                repo = new Repository();\n                repo.setUrl(repoDelta.getRepository().getUrl());\n                db.getRepositories().add(repo);\n            }\n\n            for (VcsCommit commit : repoDelta.getCommits()) {\n                Commit c = new Commit();\n                c.setDate(commit.getJavaDate());\n                c.setMessage(commit.getMessage());\n                c.setAuthor(commit.getAuthor());\n                c.setIdentifier(commit.getRevision());\n\n                repo.getCommits().add(c);\n                db.getCommits().add(c);\n            }\n        }\n        db.getCommits().sync();\n        db.getRepositories().sync();\n    }  The above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new  Commit  object is created for each commit in the delta.",
            "title": "adapt(DB db)"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#3-make-the-metric-provider-discoverable",
            "text": "Metric providers are registered with the OSSMETER platform using  extension points :   Open up the plugin.xml file and select the 'Extensions' tab  Click the Add button and from the list select  org.ossmeter.platform.metricprovider  On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.  Click the 'Browse...' button and select the 'CommitsTransientMetricProvider' class   Now everything is ready for the metric to be executed :)",
            "title": "3. Make the metric provider discoverable"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#the-historic-metric-provider",
            "text": "This metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value.",
            "title": "The Historic Metric Provider"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#0-setup_1",
            "text": "Create a new Plugin project in Eclipse.     Go to File > New > Project... and select 'Plug-in project'  Give the project an appropriate name. The OSSMETER naming convention is:  Transient metrics: org.ossmeter.metricprovider.trans.(metric name)  Historical metrics: org.ossmeter.metricprovider.historical.(metric name)    Click Next  If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it  Click Finish  Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.",
            "title": "0. Setup"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#1-the-data-model_1",
            "text": "In your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model:  package org.ossmeter.metricprovider.historic.commits.model;\n\nclass HistoricCommits {\n    attr int numberOfCommits;\n}  The data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed.",
            "title": "1. The data model"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#2-the-metric-provider_1",
            "text": "Create a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider  This class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)):  public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider \nThe generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to.  There are a number of methods that need implementing. We will discuss each in turn.  First of all, complete the typical information-related methods:      @Override\n    public String getShortIdentifier() {\n        return \"historicalcommits\";\n    }\n\n    @Override\n    public String getFriendlyName() {\n        return \"Historical commits\";\n    }\n\n    @Override\n    public String getSummaryInformation() {\n        return \"...\";\n    }  Now complete the standard  appliesTo  method:      @Override\n    public boolean appliesTo(Project project) {\n        return project.getVcsRepositories().size() > 0;\n    }  We now need to specify a dependency on the transient metric provider that we just implemented.      @Override\n    public List<String> getIdentifiersOfUses() {\n        return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName());\n    }     This tells the platform that we need access to the  CommitsTransientMetricProvider  database. The platform will assign this to the  uses  field that is available to the historical metric provider, as you'll see in the  measure  method:      @Override\n    public Pongo measure(Project project) {\n        Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0));\n\n        int commits = (int) transDb.getCommits().size();\n\n        HistoricCommits hist = new HistoricCommits();\n        hist.setNumberOfCommits(commits);\n\n        return hist;\n    }  First of all, we get hold of the database of the transient commits metric provider, and simply count the size of the  commits  collection. We save this in an instance of the  HistoricCommits  Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project.",
            "title": "2. The metric provider"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#3-make-the-metric-provider-discoverable_1",
            "text": "This process is the same as the transient metric provider:   Open up the plugin.xml file and select the 'Extensions' tab  Click the Add button and from the list select  org.ossmeter.platform.metricprovider  On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.  Click the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class   Now everything is ready for both metrics to be executed :)  But first. Let's specify how we want this historical metric to be visualised.",
            "title": "3. Make the metric provider discoverable"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#4-define-a-metvis-visualisation-specification",
            "text": "MetVis  is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis  web page  has numerous examples of this.  Create a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider:  {\n    \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\",\n    \"vis\" : [\n        {\n            \"id\" : \"historicalcommits\",\n            \"name\" : \"Commits over time\",\n            \"description\" : \"This metric shows when the projects commits occurred\",\n            \"type\" : \"LineChart\",\n            \"datatable\" : {\n                \"cols\" : [\n                    { \"name\" : \"Date\", \"field\" : \"$__date\" },\n                    { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" }\n                ]\n            },\n            \"x\" : \"Date\",\n            \"y\" : \"Commits\"\n        }\n    ]\n}  The  metricId  field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use  $__date . The final two fields ( x  and  y ) are references to column names in the datatable specification. The  type  states that the data should be plotted as a line chart. You can test your MetVis specifications on the  MetVis playpen .",
            "title": "4. Define a MetVis visualisation specification"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#5-make-the-visualisation-specification-discoverable",
            "text": "As with metric providers, visualisation specifications are registered using extension points.   Add the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project  Open up the plugin.xml file and select the 'Extensions' tab  Click the Add button and from the list select  org.ossmeter.platform.visualisation.metric  On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button.  Click the 'Browse...' button and select the 'historicalcommits.json' file   Good job.",
            "title": "5. Make the visualisation specification discoverable"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#running-the-metric-providers",
            "text": "See  Running from Source",
            "title": "Running the metric providers"
        },
        {
            "location": "/development/How-To-Develop-Metric-Provider/#homework",
            "text": "Adapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.",
            "title": "Homework"
        },
        {
            "location": "/development/Implementing-Restlet-Service/",
            "text": "Implementing RESTLET services\n\n\nWhen to use this guideline ?\n\n\nThis guideline present how to create a new REST service using the RESTLET framework in the Scava platform.\n\n\nContext\n\n\nScava project manages REST services with the RESTLET framework.\n\n\nThe usage of Restlet framework  has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin :\n\n\n\n\norg.eclipse.crossmeter.platform.client.api.  \n\n\n\n\nThe RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in.\n\n\nYou want to access to create a new REST Service ?\n\n\n1. Create a new Route\n\n\nTo  register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service.\n\n\nNaming the Route\n\n\nThe routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service : \nNaming-Scava-REST-Services.html\n\n\nRegister the Route\n\n\nThe \norg.scava.platform.services\n plug-in contained the class \nPlatformRoute.java\n  responsible for declaring routes.\n\n\npackage org.scava.platform.services;\nimport org.restlet.Application;\nimport org.restlet.Restlet;\nimport org.restlet.routing.Router;\n\npublic class PlatformRoute extends Application {\n    @Override\n    public Restlet createInboundRoot() {\n        Router router = new Router(getContext());\n\n        router.attach(\"/\", PingResource.class);     \n        router.attach(\"/search\", SearchProjectResource.class);\n                ...\n                router.attach(\"/projects/p/{projectid}\", ProjectResource.class);\n        router.attach(\"/raw/metrics\", RawMetricListResource.class);\n        ...\n        return router;\n    }\n}\n\n\n\n\nRoute Example :\n\n\nrouter.attach(\"/raw/metrics\", RawMetricListResource.class);\n\n\n\n\n\n\n\"/raw/metrics\"\n: Represent the route URL.\n\n\n\"RawMetricListResource.class\"\n : Represent the class where the service to be implemented for this path.\n\n\n\n\nA route can contained some parameters. In this case, parameters are identified by a name with curly brackets \n{}\n.\n\n\n2. Implement the Service\n\n\nA service implementation is a Java class which extend the \nServerResource\n class provided by the RESTLET framework.\nTo create a new service create a new Class :\n\n Named \"\nServiceName\n\" + Resource.  Ex : ProjectCreationResource.java\n\n On a namespace based on the  route. Ex : org.scava.platform.services.administration for platform administration services.\n* Who extend the org.restlet.resource.ServerResource class.\n\n\nGET Service\n\n\nTo implement a service of type GET, create a new method :\n\n Based on the following signature : public final Representation represent()\n\n Add the @Get(\"json\") annotation\n\n\n\n@Get(\"json\")\npublic final Representation represent() {\n  // Initialise Response Header\n  Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\");\n   if (responseHeaders == null) {\n    responseHeaders = new Series(Header.class);\n     getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders);\n  }\n  responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\"));\n  responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\"));\n\n  // Get Route parameter if required {projectid}\n  String projectId = (String) getRequest().getAttributes().get(\"projectid\");\n\n\n  try {\n    ....\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_OK);\n    return new StringRepresentation(...);\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n    return rep;\n  }\n}\n\n\n\n\n\nYou can also extend the \nAbstractApiResource\n , a service class  provided by the platform and dedicate to services who request a connection to MongoDb database instead of the \nServerResource\n. In this case you will have to implement the doRepresent() method.\n\n\npublic class RawMetricListResource extends AbstractApiResource {\n    public Representation doRepresent() {\n        ObjectNode res = mapper.createObjectNode();\n\n        ArrayNode metrics = mapper.createArrayNode();\n        res.put(\"metrics\", metrics);\n        ...\n        return Util.createJsonRepresentation(res);\n    }\n}\n\n\n\n\nPOST Service\n\n\nTo implement a service of type POST, crate a new method :\n\n Based on the following signature : public Representation \nmyServiceName\n (Representation entity)\n\n Add the @Post annotation\n\n\n@Post\npublic Representation myServiceName(Representation entity) {\n  try {\n    // Read Json Datas\n    JsonNode json = mapper.readTree(entity.getText());\n\n    ...\n\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_CREATED);\n    return new StringRepresentation(...);\n\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n   return rep;\n  }\n}\n\n\n\n\nDELETE Service\n\n\nTo do ....\n\n\n3. Document the Service\n\n\nThe REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to  allow an easy integration, this contract must be documented :\n\n\n4. Test the Service\n\n\nTo do ....\n\n\nComment",
            "title": "Implementing Restlet Service"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#implementing-restlet-services",
            "text": "",
            "title": "Implementing RESTLET services"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#when-to-use-this-guideline",
            "text": "This guideline present how to create a new REST service using the RESTLET framework in the Scava platform.",
            "title": "When to use this guideline ?"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#context",
            "text": "Scava project manages REST services with the RESTLET framework.  The usage of Restlet framework  has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin :   org.eclipse.crossmeter.platform.client.api.     The RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in.",
            "title": "Context"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#you-want-to-access-to-create-a-new-rest-service",
            "text": "",
            "title": "You want to access to create a new REST Service ?"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#1-create-a-new-route",
            "text": "To  register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service.",
            "title": "1. Create a new Route"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#naming-the-route",
            "text": "The routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service :  Naming-Scava-REST-Services.html",
            "title": "Naming the Route"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#register-the-route",
            "text": "The  org.scava.platform.services  plug-in contained the class  PlatformRoute.java   responsible for declaring routes.  package org.scava.platform.services;\nimport org.restlet.Application;\nimport org.restlet.Restlet;\nimport org.restlet.routing.Router;\n\npublic class PlatformRoute extends Application {\n    @Override\n    public Restlet createInboundRoot() {\n        Router router = new Router(getContext());\n\n        router.attach(\"/\", PingResource.class);     \n        router.attach(\"/search\", SearchProjectResource.class);\n                ...\n                router.attach(\"/projects/p/{projectid}\", ProjectResource.class);\n        router.attach(\"/raw/metrics\", RawMetricListResource.class);\n        ...\n        return router;\n    }\n}  Route Example :  router.attach(\"/raw/metrics\", RawMetricListResource.class);   \"/raw/metrics\" : Represent the route URL.  \"RawMetricListResource.class\"  : Represent the class where the service to be implemented for this path.   A route can contained some parameters. In this case, parameters are identified by a name with curly brackets  {} .",
            "title": "Register the Route"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#2-implement-the-service",
            "text": "A service implementation is a Java class which extend the  ServerResource  class provided by the RESTLET framework.\nTo create a new service create a new Class :  Named \" ServiceName \" + Resource.  Ex : ProjectCreationResource.java  On a namespace based on the  route. Ex : org.scava.platform.services.administration for platform administration services.\n* Who extend the org.restlet.resource.ServerResource class.",
            "title": "2. Implement the Service"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#get-service",
            "text": "To implement a service of type GET, create a new method :  Based on the following signature : public final Representation represent()  Add the @Get(\"json\") annotation  \n@Get(\"json\")\npublic final Representation represent() {\n  // Initialise Response Header\n  Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\");\n   if (responseHeaders == null) {\n    responseHeaders = new Series(Header.class);\n     getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders);\n  }\n  responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\"));\n  responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\"));\n\n  // Get Route parameter if required {projectid}\n  String projectId = (String) getRequest().getAttributes().get(\"projectid\");\n\n\n  try {\n    ....\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_OK);\n    return new StringRepresentation(...);\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n    return rep;\n  }\n}  You can also extend the  AbstractApiResource  , a service class  provided by the platform and dedicate to services who request a connection to MongoDb database instead of the  ServerResource . In this case you will have to implement the doRepresent() method.  public class RawMetricListResource extends AbstractApiResource {\n    public Representation doRepresent() {\n        ObjectNode res = mapper.createObjectNode();\n\n        ArrayNode metrics = mapper.createArrayNode();\n        res.put(\"metrics\", metrics);\n        ...\n        return Util.createJsonRepresentation(res);\n    }\n}",
            "title": "GET Service"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#post-service",
            "text": "To implement a service of type POST, crate a new method :  Based on the following signature : public Representation  myServiceName  (Representation entity)  Add the @Post annotation  @Post\npublic Representation myServiceName(Representation entity) {\n  try {\n    // Read Json Datas\n    JsonNode json = mapper.readTree(entity.getText());\n\n    ...\n\n    // Provide Result\n    getResponse().setStatus(Status.SUCCESS_CREATED);\n    return new StringRepresentation(...);\n\n  } catch (IOException e) {\n    StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\");\n    rep.setMediaType(MediaType.APPLICATION_JSON);\n    getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST);\n   return rep;\n  }\n}",
            "title": "POST Service"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#delete-service",
            "text": "To do ....",
            "title": "DELETE Service"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#3-document-the-service",
            "text": "The REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to  allow an easy integration, this contract must be documented :",
            "title": "3. Document the Service"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#4-test-the-service",
            "text": "To do ....",
            "title": "4. Test the Service"
        },
        {
            "location": "/development/Implementing-Restlet-Service/#comment",
            "text": "",
            "title": "Comment"
        },
        {
            "location": "/development/Licensing/",
            "text": "Licencing for Scava\n\n\nContent\n\n\nThe Scava project is licensed under \nEclipse Public License - v 2.0\n license.\n\n\nAs consequence of our status of project hosted by the eclipse foundation, all Scava components  must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header.\n\n\n\"Eclipse Public License\" licensing file\n\n\nThe text below must be integrated to the root folder of your project on a text file name \"LICENSE\".\n\n\nEclipse Public License - v 2.0\n\n    THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE\n    PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION\n    OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT.\n\n1. DEFINITIONS\n\n\"Contribution\" means:\n\n  a) in the case of the initial Contributor, the initial content\n     Distributed under this Agreement, and\n\n  b) in the case of each subsequent Contributor:\n     i) changes to the Program, and\n     ii) additions to the Program;\n  where such changes and/or additions to the Program originate from\n  and are Distributed by that particular Contributor. A Contribution\n  \"originates\" from a Contributor if it was added to the Program by\n  such Contributor itself or anyone acting on such Contributor's behalf.\n  Contributions do not include changes or additions to the Program that\n  are not Modified Works.\n\n\"Contributor\" means any person or entity that Distributes the Program.\n\n\"Licensed Patents\" mean patent claims licensable by a Contributor which\nare necessarily infringed by the use or sale of its Contribution alone\nor when combined with the Program.\n\n\"Program\" means the Contributions Distributed in accordance with this\nAgreement.\n\n\"Recipient\" means anyone who receives the Program under this Agreement\nor any Secondary License (as applicable), including Contributors.\n\n\"Derivative Works\" shall mean any work, whether in Source Code or other\nform, that is based on (or derived from) the Program and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship.\n\n\"Modified Works\" shall mean any work in Source Code or other form that\nresults from an addition to, deletion from, or modification of the\ncontents of the Program, including, for purposes of clarity any new file\nin Source Code form that contains any contents of the Program. Modified\nWorks shall not include works that contain only declarations,\ninterfaces, types, classes, structures, or files of the Program solely\nin each case in order to link to, bind by name, or subclass the Program\nor Modified Works thereof.\n\n\"Distribute\" means the acts of a) distributing or b) making available\nin any manner that enables the transfer of a copy.\n\n\"Source Code\" means the form of a Program preferred for making\nmodifications, including but not limited to software source code,\ndocumentation source, and configuration files.\n\n\"Secondary License\" means either the GNU General Public License,\nVersion 2.0, or any later versions of that license, including any\nexceptions or additional permissions as identified by the initial\nContributor.\n\n2. GRANT OF RIGHTS\n\n  a) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free copyright\n  license to reproduce, prepare Derivative Works of, publicly display,\n  publicly perform, Distribute and sublicense the Contribution of such\n  Contributor, if any, and such Derivative Works.\n\n  b) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free patent\n  license under Licensed Patents to make, use, sell, offer to sell,\n  import and otherwise transfer the Contribution of such Contributor,\n  if any, in Source Code or other form. This patent license shall\n  apply to the combination of the Contribution and the Program if, at\n  the time the Contribution is added by the Contributor, such addition\n  of the Contribution causes such combination to be covered by the\n  Licensed Patents. The patent license shall not apply to any other\n  combinations which include the Contribution. No hardware per se is\n  licensed hereunder.\n\n  c) Recipient understands that although each Contributor grants the\n  licenses to its Contributions set forth herein, no assurances are\n  provided by any Contributor that the Program does not infringe the\n  patent or other intellectual property rights of any other entity.\n  Each Contributor disclaims any liability to Recipient for claims\n  brought by any other entity based on infringement of intellectual\n  property rights or otherwise. As a condition to exercising the\n  rights and licenses granted hereunder, each Recipient hereby\n  assumes sole responsibility to secure any other intellectual\n  property rights needed, if any. For example, if a third party\n  patent license is required to allow Recipient to Distribute the\n  Program, it is Recipient's responsibility to acquire that license\n  before distributing the Program.\n\n  d) Each Contributor represents that to its knowledge it has\n  sufficient copyright rights in its Contribution, if any, to grant\n  the copyright license set forth in this Agreement.\n\n  e) Notwithstanding the terms of any Secondary License, no\n  Contributor makes additional grants to any Recipient (other than\n  those set forth in this Agreement) as a result of such Recipient's\n  receipt of the Program under the terms of a Secondary License\n  (if permitted under the terms of Section 3).\n\n3. REQUIREMENTS\n\n3.1 If a Contributor Distributes the Program in any form, then:\n\n  a) the Program must also be made available as Source Code, in\n  accordance with section 3.2, and the Contributor must accompany\n  the Program with a statement that the Source Code for the Program\n  is available under this Agreement, and informs Recipients how to\n  obtain it in a reasonable manner on or through a medium customarily\n  used for software exchange; and\n\n  b) the Contributor may Distribute the Program under a license\n  different than this Agreement, provided that such license:\n     i) effectively disclaims on behalf of all other Contributors all\n     warranties and conditions, express and implied, including\n     warranties or conditions of title and non-infringement, and\n     implied warranties or conditions of merchantability and fitness\n     for a particular purpose;\n\n     ii) effectively excludes on behalf of all other Contributors all\n     liability for damages, including direct, indirect, special,\n     incidental and consequential damages, such as lost profits;\n\n     iii) does not attempt to limit or alter the recipients' rights\n     in the Source Code under section 3.2; and\n\n     iv) requires any subsequent distribution of the Program by any\n     party to be under a license that satisfies the requirements\n     of this section 3.\n\n3.2 When the Program is Distributed as Source Code:\n\n  a) it must be made available under this Agreement, or if the\n  Program (i) is combined with other material in a separate file or\n  files made available under a Secondary License, and (ii) the initial\n  Contributor attached to the Source Code the notice described in\n  Exhibit A of this Agreement, then the Program may be made available\n  under the terms of such Secondary Licenses, and\n\n  b) a copy of this Agreement must be included with each copy of\n  the Program.\n\n3.3 Contributors may not remove or alter any copyright, patent,\ntrademark, attribution notices, disclaimers of warranty, or limitations\nof liability (\"notices\") contained within the Program from any copy of\nthe Program which they Distribute, provided that Contributors may add\ntheir own appropriate notices.\n\n4. COMMERCIAL DISTRIBUTION\n\nCommercial distributors of software may accept certain responsibilities\nwith respect to end users, business partners and the like. While this\nlicense is intended to facilitate the commercial use of the Program,\nthe Contributor who includes the Program in a commercial product\noffering should do so in a manner which does not create potential\nliability for other Contributors. Therefore, if a Contributor includes\nthe Program in a commercial product offering, such Contributor\n(\"Commercial Contributor\") hereby agrees to defend and indemnify every\nother Contributor (\"Indemnified Contributor\") against any losses,\ndamages and costs (collectively \"Losses\") arising from claims, lawsuits\nand other legal actions brought by a third party against the Indemnified\nContributor to the extent caused by the acts or omissions of such\nCommercial Contributor in connection with its distribution of the Program\nin a commercial product offering. The obligations in this section do not\napply to any claims or Losses relating to any actual or alleged\nintellectual property infringement. In order to qualify, an Indemnified\nContributor must: a) promptly notify the Commercial Contributor in\nwriting of such claim, and b) allow the Commercial Contributor to control,\nand cooperate with the Commercial Contributor in, the defense and any\nrelated settlement negotiations. The Indemnified Contributor may\nparticipate in any such claim at its own expense.\n\nFor example, a Contributor might include the Program in a commercial\nproduct offering, Product X. That Contributor is then a Commercial\nContributor. If that Commercial Contributor then makes performance\nclaims, or offers warranties related to Product X, those performance\nclaims and warranties are such Commercial Contributor's responsibility\nalone. Under this section, the Commercial Contributor would have to\ndefend claims against the other Contributors related to those performance\nclaims and warranties, and if a court requires any other Contributor to\npay any damages as a result, the Commercial Contributor must pay\nthose damages.\n\n5. NO WARRANTY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\"\nBASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR\nIMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF\nTITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR\nPURPOSE. Each Recipient is solely responsible for determining the\nappropriateness of using and distributing the Program and assumes all\nrisks associated with its exercise of rights under this Agreement,\nincluding but not limited to the risks and costs of program errors,\ncompliance with applicable laws, damage to or loss of data, programs\nor equipment, and unavailability or interruption of operations.\n\n6. DISCLAIMER OF LIABILITY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS\nSHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST\nPROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE\nEXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n7. GENERAL\n\nIf any provision of this Agreement is invalid or unenforceable under\napplicable law, it shall not affect the validity or enforceability of\nthe remainder of the terms of this Agreement, and without further\naction by the parties hereto, such provision shall be reformed to the\nminimum extent necessary to make such provision valid and enforceable.\n\nIf Recipient institutes patent litigation against any entity\n(including a cross-claim or counterclaim in a lawsuit) alleging that the\nProgram itself (excluding combinations of the Program with other software\nor hardware) infringes such Recipient's patent(s), then such Recipient's\nrights granted under Section 2(b) shall terminate as of the date such\nlitigation is filed.\n\nAll Recipient's rights under this Agreement shall terminate if it\nfails to comply with any of the material terms or conditions of this\nAgreement and does not cure such failure in a reasonable period of\ntime after becoming aware of such noncompliance. If all Recipient's\nrights under this Agreement terminate, Recipient agrees to cease use\nand distribution of the Program as soon as reasonably practicable.\nHowever, Recipient's obligations under this Agreement and any licenses\ngranted by Recipient relating to the Program shall continue and survive.\n\nEveryone is permitted to copy and distribute copies of this Agreement,\nbut in order to avoid inconsistency the Agreement is copyrighted and\nmay only be modified in the following manner. The Agreement Steward\nreserves the right to publish new versions (including revisions) of\nthis Agreement from time to time. No one other than the Agreement\nSteward has the right to modify this Agreement. The Eclipse Foundation\nis the initial Agreement Steward. The Eclipse Foundation may assign the\nresponsibility to serve as the Agreement Steward to a suitable separate\nentity. Each new version of the Agreement will be given a distinguishing\nversion number. The Program (including Contributions) may always be\nDistributed subject to the version of the Agreement under which it was\nreceived. In addition, after a new version of the Agreement is published,\nContributor may elect to Distribute the Program (including its\nContributions) under the new version.\n\nExcept as expressly stated in Sections 2(a) and 2(b) above, Recipient\nreceives no rights or licenses to the intellectual property of any\nContributor under this Agreement, whether expressly, by implication,\nestoppel or otherwise. All rights in the Program not expressly granted\nunder this Agreement are reserved. Nothing in this Agreement is intended\nto be enforceable by any entity that is not a Contributor or Recipient.\nNo third-party beneficiary rights are created under this Agreement.\n\nExhibit A - Form of Secondary Licenses Notice\n\n\"This Source Code may also be made available under the following\nSecondary Licenses when the conditions for such availability set forth\nin the Eclipse Public License, v. 2.0 are satisfied: {name license(s),\nversion(s), and exceptions or additional permissions here}.\"\n\n  Simply including a copy of this Agreement, including this Exhibit A\n  is not sufficient to license the Source Code under Secondary Licenses.\n\n  If it is not possible or desirable to put the notice in a particular\n  file, then You may include the notice in a location (such as a LICENSE\n  file in a relevant directory) where a recipient would be likely to\n  look for such a notice.\n\n  You may add additional accurate notices of copyright ownership.\n\n\n\n\nSource File header\n\n\nAll sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...).\n\n\nOptionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization.\n\n\nExample of Java license header file\n\n\n/*******************************************************************************\n * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"}\n * This program and the accompanying materials are made\n * available under the terms of the Eclipse Public License 2.0\n * which is available at https://www.eclipse.org/legal/epl-2.0/\n *\n * SPDX-License-Identifier: EPL-2.0\n ******************************************************************************/\n\n\n\n\nComment\n\n\nn Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files.\n\n\nThis plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.",
            "title": "Licensing"
        },
        {
            "location": "/development/Licensing/#licencing-for-scava",
            "text": "",
            "title": "Licencing for Scava"
        },
        {
            "location": "/development/Licensing/#content",
            "text": "The Scava project is licensed under  Eclipse Public License - v 2.0  license.  As consequence of our status of project hosted by the eclipse foundation, all Scava components  must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header.",
            "title": "Content"
        },
        {
            "location": "/development/Licensing/#eclipse-public-license-licensing-file",
            "text": "The text below must be integrated to the root folder of your project on a text file name \"LICENSE\".  Eclipse Public License - v 2.0\n\n    THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE\n    PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION\n    OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT.\n\n1. DEFINITIONS\n\n\"Contribution\" means:\n\n  a) in the case of the initial Contributor, the initial content\n     Distributed under this Agreement, and\n\n  b) in the case of each subsequent Contributor:\n     i) changes to the Program, and\n     ii) additions to the Program;\n  where such changes and/or additions to the Program originate from\n  and are Distributed by that particular Contributor. A Contribution\n  \"originates\" from a Contributor if it was added to the Program by\n  such Contributor itself or anyone acting on such Contributor's behalf.\n  Contributions do not include changes or additions to the Program that\n  are not Modified Works.\n\n\"Contributor\" means any person or entity that Distributes the Program.\n\n\"Licensed Patents\" mean patent claims licensable by a Contributor which\nare necessarily infringed by the use or sale of its Contribution alone\nor when combined with the Program.\n\n\"Program\" means the Contributions Distributed in accordance with this\nAgreement.\n\n\"Recipient\" means anyone who receives the Program under this Agreement\nor any Secondary License (as applicable), including Contributors.\n\n\"Derivative Works\" shall mean any work, whether in Source Code or other\nform, that is based on (or derived from) the Program and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship.\n\n\"Modified Works\" shall mean any work in Source Code or other form that\nresults from an addition to, deletion from, or modification of the\ncontents of the Program, including, for purposes of clarity any new file\nin Source Code form that contains any contents of the Program. Modified\nWorks shall not include works that contain only declarations,\ninterfaces, types, classes, structures, or files of the Program solely\nin each case in order to link to, bind by name, or subclass the Program\nor Modified Works thereof.\n\n\"Distribute\" means the acts of a) distributing or b) making available\nin any manner that enables the transfer of a copy.\n\n\"Source Code\" means the form of a Program preferred for making\nmodifications, including but not limited to software source code,\ndocumentation source, and configuration files.\n\n\"Secondary License\" means either the GNU General Public License,\nVersion 2.0, or any later versions of that license, including any\nexceptions or additional permissions as identified by the initial\nContributor.\n\n2. GRANT OF RIGHTS\n\n  a) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free copyright\n  license to reproduce, prepare Derivative Works of, publicly display,\n  publicly perform, Distribute and sublicense the Contribution of such\n  Contributor, if any, and such Derivative Works.\n\n  b) Subject to the terms of this Agreement, each Contributor hereby\n  grants Recipient a non-exclusive, worldwide, royalty-free patent\n  license under Licensed Patents to make, use, sell, offer to sell,\n  import and otherwise transfer the Contribution of such Contributor,\n  if any, in Source Code or other form. This patent license shall\n  apply to the combination of the Contribution and the Program if, at\n  the time the Contribution is added by the Contributor, such addition\n  of the Contribution causes such combination to be covered by the\n  Licensed Patents. The patent license shall not apply to any other\n  combinations which include the Contribution. No hardware per se is\n  licensed hereunder.\n\n  c) Recipient understands that although each Contributor grants the\n  licenses to its Contributions set forth herein, no assurances are\n  provided by any Contributor that the Program does not infringe the\n  patent or other intellectual property rights of any other entity.\n  Each Contributor disclaims any liability to Recipient for claims\n  brought by any other entity based on infringement of intellectual\n  property rights or otherwise. As a condition to exercising the\n  rights and licenses granted hereunder, each Recipient hereby\n  assumes sole responsibility to secure any other intellectual\n  property rights needed, if any. For example, if a third party\n  patent license is required to allow Recipient to Distribute the\n  Program, it is Recipient's responsibility to acquire that license\n  before distributing the Program.\n\n  d) Each Contributor represents that to its knowledge it has\n  sufficient copyright rights in its Contribution, if any, to grant\n  the copyright license set forth in this Agreement.\n\n  e) Notwithstanding the terms of any Secondary License, no\n  Contributor makes additional grants to any Recipient (other than\n  those set forth in this Agreement) as a result of such Recipient's\n  receipt of the Program under the terms of a Secondary License\n  (if permitted under the terms of Section 3).\n\n3. REQUIREMENTS\n\n3.1 If a Contributor Distributes the Program in any form, then:\n\n  a) the Program must also be made available as Source Code, in\n  accordance with section 3.2, and the Contributor must accompany\n  the Program with a statement that the Source Code for the Program\n  is available under this Agreement, and informs Recipients how to\n  obtain it in a reasonable manner on or through a medium customarily\n  used for software exchange; and\n\n  b) the Contributor may Distribute the Program under a license\n  different than this Agreement, provided that such license:\n     i) effectively disclaims on behalf of all other Contributors all\n     warranties and conditions, express and implied, including\n     warranties or conditions of title and non-infringement, and\n     implied warranties or conditions of merchantability and fitness\n     for a particular purpose;\n\n     ii) effectively excludes on behalf of all other Contributors all\n     liability for damages, including direct, indirect, special,\n     incidental and consequential damages, such as lost profits;\n\n     iii) does not attempt to limit or alter the recipients' rights\n     in the Source Code under section 3.2; and\n\n     iv) requires any subsequent distribution of the Program by any\n     party to be under a license that satisfies the requirements\n     of this section 3.\n\n3.2 When the Program is Distributed as Source Code:\n\n  a) it must be made available under this Agreement, or if the\n  Program (i) is combined with other material in a separate file or\n  files made available under a Secondary License, and (ii) the initial\n  Contributor attached to the Source Code the notice described in\n  Exhibit A of this Agreement, then the Program may be made available\n  under the terms of such Secondary Licenses, and\n\n  b) a copy of this Agreement must be included with each copy of\n  the Program.\n\n3.3 Contributors may not remove or alter any copyright, patent,\ntrademark, attribution notices, disclaimers of warranty, or limitations\nof liability (\"notices\") contained within the Program from any copy of\nthe Program which they Distribute, provided that Contributors may add\ntheir own appropriate notices.\n\n4. COMMERCIAL DISTRIBUTION\n\nCommercial distributors of software may accept certain responsibilities\nwith respect to end users, business partners and the like. While this\nlicense is intended to facilitate the commercial use of the Program,\nthe Contributor who includes the Program in a commercial product\noffering should do so in a manner which does not create potential\nliability for other Contributors. Therefore, if a Contributor includes\nthe Program in a commercial product offering, such Contributor\n(\"Commercial Contributor\") hereby agrees to defend and indemnify every\nother Contributor (\"Indemnified Contributor\") against any losses,\ndamages and costs (collectively \"Losses\") arising from claims, lawsuits\nand other legal actions brought by a third party against the Indemnified\nContributor to the extent caused by the acts or omissions of such\nCommercial Contributor in connection with its distribution of the Program\nin a commercial product offering. The obligations in this section do not\napply to any claims or Losses relating to any actual or alleged\nintellectual property infringement. In order to qualify, an Indemnified\nContributor must: a) promptly notify the Commercial Contributor in\nwriting of such claim, and b) allow the Commercial Contributor to control,\nand cooperate with the Commercial Contributor in, the defense and any\nrelated settlement negotiations. The Indemnified Contributor may\nparticipate in any such claim at its own expense.\n\nFor example, a Contributor might include the Program in a commercial\nproduct offering, Product X. That Contributor is then a Commercial\nContributor. If that Commercial Contributor then makes performance\nclaims, or offers warranties related to Product X, those performance\nclaims and warranties are such Commercial Contributor's responsibility\nalone. Under this section, the Commercial Contributor would have to\ndefend claims against the other Contributors related to those performance\nclaims and warranties, and if a court requires any other Contributor to\npay any damages as a result, the Commercial Contributor must pay\nthose damages.\n\n5. NO WARRANTY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\"\nBASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR\nIMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF\nTITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR\nPURPOSE. Each Recipient is solely responsible for determining the\nappropriateness of using and distributing the Program and assumes all\nrisks associated with its exercise of rights under this Agreement,\nincluding but not limited to the risks and costs of program errors,\ncompliance with applicable laws, damage to or loss of data, programs\nor equipment, and unavailability or interruption of operations.\n\n6. DISCLAIMER OF LIABILITY\n\nEXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT\nPERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS\nSHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST\nPROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE\nEXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGES.\n\n7. GENERAL\n\nIf any provision of this Agreement is invalid or unenforceable under\napplicable law, it shall not affect the validity or enforceability of\nthe remainder of the terms of this Agreement, and without further\naction by the parties hereto, such provision shall be reformed to the\nminimum extent necessary to make such provision valid and enforceable.\n\nIf Recipient institutes patent litigation against any entity\n(including a cross-claim or counterclaim in a lawsuit) alleging that the\nProgram itself (excluding combinations of the Program with other software\nor hardware) infringes such Recipient's patent(s), then such Recipient's\nrights granted under Section 2(b) shall terminate as of the date such\nlitigation is filed.\n\nAll Recipient's rights under this Agreement shall terminate if it\nfails to comply with any of the material terms or conditions of this\nAgreement and does not cure such failure in a reasonable period of\ntime after becoming aware of such noncompliance. If all Recipient's\nrights under this Agreement terminate, Recipient agrees to cease use\nand distribution of the Program as soon as reasonably practicable.\nHowever, Recipient's obligations under this Agreement and any licenses\ngranted by Recipient relating to the Program shall continue and survive.\n\nEveryone is permitted to copy and distribute copies of this Agreement,\nbut in order to avoid inconsistency the Agreement is copyrighted and\nmay only be modified in the following manner. The Agreement Steward\nreserves the right to publish new versions (including revisions) of\nthis Agreement from time to time. No one other than the Agreement\nSteward has the right to modify this Agreement. The Eclipse Foundation\nis the initial Agreement Steward. The Eclipse Foundation may assign the\nresponsibility to serve as the Agreement Steward to a suitable separate\nentity. Each new version of the Agreement will be given a distinguishing\nversion number. The Program (including Contributions) may always be\nDistributed subject to the version of the Agreement under which it was\nreceived. In addition, after a new version of the Agreement is published,\nContributor may elect to Distribute the Program (including its\nContributions) under the new version.\n\nExcept as expressly stated in Sections 2(a) and 2(b) above, Recipient\nreceives no rights or licenses to the intellectual property of any\nContributor under this Agreement, whether expressly, by implication,\nestoppel or otherwise. All rights in the Program not expressly granted\nunder this Agreement are reserved. Nothing in this Agreement is intended\nto be enforceable by any entity that is not a Contributor or Recipient.\nNo third-party beneficiary rights are created under this Agreement.\n\nExhibit A - Form of Secondary Licenses Notice\n\n\"This Source Code may also be made available under the following\nSecondary Licenses when the conditions for such availability set forth\nin the Eclipse Public License, v. 2.0 are satisfied: {name license(s),\nversion(s), and exceptions or additional permissions here}.\"\n\n  Simply including a copy of this Agreement, including this Exhibit A\n  is not sufficient to license the Source Code under Secondary Licenses.\n\n  If it is not possible or desirable to put the notice in a particular\n  file, then You may include the notice in a location (such as a LICENSE\n  file in a relevant directory) where a recipient would be likely to\n  look for such a notice.\n\n  You may add additional accurate notices of copyright ownership.",
            "title": "\"Eclipse Public License\" licensing file"
        },
        {
            "location": "/development/Licensing/#source-file-header",
            "text": "All sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...).  Optionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization.  Example of Java license header file  /*******************************************************************************\n * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"}\n * This program and the accompanying materials are made\n * available under the terms of the Eclipse Public License 2.0\n * which is available at https://www.eclipse.org/legal/epl-2.0/\n *\n * SPDX-License-Identifier: EPL-2.0\n ******************************************************************************/",
            "title": "Source File header"
        },
        {
            "location": "/development/Licensing/#comment",
            "text": "n Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files.  This plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.",
            "title": "Comment"
        },
        {
            "location": "/development/Naming-Scava-REST-Services/",
            "text": "Naming Scava REST services\n\n\nWhen to use this guideline ?\n\n\nThis guideline present how to define the route of a new REST service provided by the Scava platform.\n\n\nContext\n\n\nThe REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of  platform services , we need to used a common naming schema for all REST services provided by the platform.\n\n\nHow to name a REST service ?\n\n\n\n\n/{\ncomponentid\n}/{\ncategoryname\n}/{\nservicename\n}\n\n\n\n\n\n\n/\ncomponentid\n/\n: Name of the Architectural component which provide the service\n\n\n/\ncategoryname\n/ (Optional)\n : optional category of the service\n\n\n/\nservicename\n/\n : Name of the rest service\n\n\n\n\nComponent\n\n\n\n\n\n\n\n\nComponent\n\n\nComponentId\n\n\n\n\n\n\n\n\n\n\nDevOps Dashboard\n\n\ndashboard\n\n\n\n\n\n\nWorkflow Execution Engine\n\n\nworkflow\n\n\n\n\n\n\nKnowledge Base\n\n\nknowledgebase\n\n\n\n\n\n\nMetric Provider\n\n\nmetricprovider\n\n\n\n\n\n\nAdministration\n\n\nadministration\n\n\n\n\n\n\n\n\nComment",
            "title": "Naming Scava REST Services"
        },
        {
            "location": "/development/Naming-Scava-REST-Services/#naming-scava-rest-services",
            "text": "",
            "title": "Naming Scava REST services"
        },
        {
            "location": "/development/Naming-Scava-REST-Services/#when-to-use-this-guideline",
            "text": "This guideline present how to define the route of a new REST service provided by the Scava platform.",
            "title": "When to use this guideline ?"
        },
        {
            "location": "/development/Naming-Scava-REST-Services/#context",
            "text": "The REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of  platform services , we need to used a common naming schema for all REST services provided by the platform.",
            "title": "Context"
        },
        {
            "location": "/development/Naming-Scava-REST-Services/#how-to-name-a-rest-service",
            "text": "/{ componentid }/{ categoryname }/{ servicename }    / componentid / : Name of the Architectural component which provide the service  / categoryname / (Optional)  : optional category of the service  / servicename /  : Name of the rest service",
            "title": "How to name a REST service ?"
        },
        {
            "location": "/development/Naming-Scava-REST-Services/#component",
            "text": "Component  ComponentId      DevOps Dashboard  dashboard    Workflow Execution Engine  workflow    Knowledge Base  knowledgebase    Metric Provider  metricprovider    Administration  administration",
            "title": "Component"
        },
        {
            "location": "/development/Naming-Scava-REST-Services/#comment",
            "text": "",
            "title": "Comment"
        },
        {
            "location": "/development/Repository-Organisation/",
            "text": "The SCAVA code repository  is organized by functional components with one package for each of this components. \n\n\nGeneral organisation\n\n\n\n\nmetric-platform \n\n\nplatform : Core projects of the metric platform\n\n\nplatform-extensions : Extensions of the metric platform\n\n\nmetric-providers : Metric Providers implementations projects\n\n\nfactoids : Factoids implementations projects\n\n\ntests : Test projects related to the metric-platform\n\n\nweb-dashboards : DevOps Dashboard and DevOpsDashboard components \n\n\nknowledge-base : Knowledge Base implementation\n\n\nWorkflow Execution Engine : Workflow Execution Engine implementation\n\n\neclipse-based-ide : SCAVA Eclipse plugin\n\n\napi-gateway :  API Gateway implementation.\n\n\nadministration : Administration component implementation\n\n\nmingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project. \n\n\n\n\nComments",
            "title": "Repository Organisation"
        },
        {
            "location": "/development/Repository-Organisation/#general-organisation",
            "text": "metric-platform   platform : Core projects of the metric platform  platform-extensions : Extensions of the metric platform  metric-providers : Metric Providers implementations projects  factoids : Factoids implementations projects  tests : Test projects related to the metric-platform  web-dashboards : DevOps Dashboard and DevOpsDashboard components   knowledge-base : Knowledge Base implementation  Workflow Execution Engine : Workflow Execution Engine implementation  eclipse-based-ide : SCAVA Eclipse plugin  api-gateway :  API Gateway implementation.  administration : Administration component implementation  mingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project.",
            "title": "General organisation"
        },
        {
            "location": "/development/Repository-Organisation/#comments",
            "text": "",
            "title": "Comments"
        },
        {
            "location": "/development/Running-Scava-in-Eclipse/",
            "text": "This page gives \ngeneral guidelines for running Ossmeter platform in Eclipse\n\n\nThe screeshot below will be used as a reference : \n\n\n\n\n(Eclipse version used in screenshot : Eclipse Java EE IDE for Web developers Luna 2 (4.4.2) )\n\n\n1) We compile with \nOssmeterfromfeature.product\n file in \norg.ossmeter.platform.osgi\n (highlighted above)\n2) In the file we put program arguements : \n-apiServer\n, \n-master\n, \n-slave\n. More information on https://github.com/crossminer/crossminer/wiki/Running-the-platform\n3) We must validate with the button on corner top right (highlighted above). This will add the required packages.\n4) If the required packages are still missing, go to \nRun Configurations -> Plug-ins\n and click on \nAdd Required Plugins\n.\n\n\n\n\n5) You could also check the \ngeneral information\n, if its the same (as in the screenshot below).\n\n\n\n\nThese were some of the things for running the platform. If there is any issue still, do not hesitate to contact us.",
            "title": "Running Scava in Eclipse"
        },
        {
            "location": "/development/Testing-Guidelines/",
            "text": "Knowledge Base\n\n\nThis builds needs some configuration to run successfully. \n\n\nIn the application.properties files:\n\n \n/knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties\n\n\n \n/knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties\n\n\nEdit the following parameters:\n\n\nlucene.index.folder=/tmp/scava_lucene/\negit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57\n\n\n\n\nTo generate the GitHub access token you need to go to \nyour own GitHub account\n and create a new one. Once it's done simply restart the tests, they should pass.",
            "title": "Testing Guidelines"
        },
        {
            "location": "/development/Testing-Guidelines/#knowledge-base",
            "text": "This builds needs some configuration to run successfully.   In the application.properties files:   /knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties    /knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties  Edit the following parameters:  lucene.index.folder=/tmp/scava_lucene/\negit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57  To generate the GitHub access token you need to go to  your own GitHub account  and create a new one. Once it's done simply restart the tests, they should pass.",
            "title": "Knowledge Base"
        },
        {
            "location": "/users/Consuming-REST-Services/",
            "text": "Consuming REST services\n\n\nWhen to use ?\n\n\nThis guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications.\n\n\nREST API Reference\n\n\nThe reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]].\n\n\nAPI Gateway\n\n\nThe Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway.\n\n\nAll web service request form clients have to go through the gateway.\n\n\n\n\nThe api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform.\n\n\nPlatform Authentication\n\n\nThe CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io).\n1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.\n1. When the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.\n\n\nAuthentication in Java\n\n\nRetrieve a Web Tokens from authentication service\n\n\nprivate String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException {\n  // Authentication Service URI\n  URL url = new URL(\"http://localhost:8086/api/authentication\");\n\n  // AUthentication Request\n  HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n  connection.setDoOutput(true);\n  connection.setRequestMethod(\"POST\");\n  connection.setRequestProperty(\"Content-Type\", \"application/json\");\n\n  String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\";\n  OutputStream os = connection.getOutputStream();\n  os.write(input.getBytes());\n  os.flush();\n\n  if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) {\n    throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode());\n  }\n\n  connection.disconnect();\n\n  // A JWT Token is return in the Header of the response\n  return connection.getHeaderField(\"Authorization\");\n}\n\n\n\n\nREST Service Call\n\n\ncurl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication\n\n\n\n\nService Consumption\n\n\nTo consume a REST service provided by the integrated platform, the client must include the Token in the header of his request.\n\n\n```java\n// Service URL\nURL url = new URL(\"http://localhost:8086/api/users\");\nHttpURLConnection connection = (HttpURLConnection) url.openConnection();\nconnection.setRequestMethod(\"GET\");\n\n\n// Add Token to the request header\nconnection.setRequestProperty(\"Authorization\",token);\n```\n\n\nComment",
            "title": "Consuming REST Services"
        },
        {
            "location": "/users/Consuming-REST-Services/#consuming-rest-services",
            "text": "",
            "title": "Consuming REST services"
        },
        {
            "location": "/users/Consuming-REST-Services/#when-to-use",
            "text": "This guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications.",
            "title": "When to use ?"
        },
        {
            "location": "/users/Consuming-REST-Services/#rest-api-reference",
            "text": "The reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]].",
            "title": "REST API Reference"
        },
        {
            "location": "/users/Consuming-REST-Services/#api-gateway",
            "text": "The Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway.  All web service request form clients have to go through the gateway.   The api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform.",
            "title": "API Gateway"
        },
        {
            "location": "/users/Consuming-REST-Services/#platform-authentication",
            "text": "The CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io).\n1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests.\n1. When the client request a specific service, the api gateway valivate the token from the authentication  service. If the token is valide, the api gateway transmite the request to the related service.  Authentication in Java  Retrieve a Web Tokens from authentication service  private String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException {\n  // Authentication Service URI\n  URL url = new URL(\"http://localhost:8086/api/authentication\");\n\n  // AUthentication Request\n  HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n  connection.setDoOutput(true);\n  connection.setRequestMethod(\"POST\");\n  connection.setRequestProperty(\"Content-Type\", \"application/json\");\n\n  String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\";\n  OutputStream os = connection.getOutputStream();\n  os.write(input.getBytes());\n  os.flush();\n\n  if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) {\n    throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode());\n  }\n\n  connection.disconnect();\n\n  // A JWT Token is return in the Header of the response\n  return connection.getHeaderField(\"Authorization\");\n}  REST Service Call  curl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication",
            "title": "Platform Authentication"
        },
        {
            "location": "/users/Consuming-REST-Services/#service-consumption",
            "text": "To consume a REST service provided by the integrated platform, the client must include the Token in the header of his request.  ```java\n// Service URL\nURL url = new URL(\"http://localhost:8086/api/users\");\nHttpURLConnection connection = (HttpURLConnection) url.openConnection();\nconnection.setRequestMethod(\"GET\");  // Add Token to the request header\nconnection.setRequestProperty(\"Authorization\",token);\n```",
            "title": "Service Consumption"
        },
        {
            "location": "/users/Consuming-REST-Services/#comment",
            "text": "",
            "title": "Comment"
        },
        {
            "location": "/users/REST-API-Documentation/",
            "text": "REST API Documentation\n\n\nProject Administration API\n\n\nThe Administration API provides services related to platform administration, user management  and registration of projects to  be analyse by the platform.\n\n\n\n\nBase URL : \n\"/administration/\"\n\n\n\n\nProject Management\n\n\nManage registration of projects analysed by the platform.\n\n\n\n\nProjects List\nGET\n \n /administration/projects/{page}\n\n\nRetrieve a list of monitored projects.\n\n\n\n\n\n\n\nProject Search\nGET\n \n /administration/projects/search\n\n\nRetrieve a list of monitored projects base on a search query.\n\n\n\n\n\n\n\nProject Info\nGET\n \n /administration/projects/{projectId}\n\n\nRetrieve the metadata of a specific project.\n\n\n\n\n\n\n\nImport Project\nPOST\n \n /administration/projects/import\n\n\nImport a new project to be analyzed by the platform. Take as input the URL of the repository of the project to import.\n\n\n\n\n\n\n\nRegister Project\nPOST\n \n /administration/projects/register\n\n\nRegister a new project to be analyzed by the platform. Take as input the definition of project to analyse.\n\n\n\n\n\nProject Analysis\n\n\nAnalyse projects registered on the platform.\n\n\n\n\nCreate Analysis Task\nPOST\n \n /administration/task/create\n\n\nCreate a new analysis task.\n\n\n\n\n\n\n\nUpdate Analysis Task\nPUT\n \n /administration/task/update\n\n\nUpdate an analysis task.\n\n\n\n\n\n\n\nStart Analysis Task\nPOST\n \n /administration/task/start\n\n\nStart an analysis task.\n\n\n\n\n\n\n\nStop Analysis Task\nPOST\n \n /administration/task/stop\n\n\nStop an analysis task.\n\n\n\n\n\n\n\nReset Analysis Task\nPOST\n \n /administration/task/reset\n\n\nReset an analysis task.\n\n\n\n\n\n\n\nDelete Analysis Task\nDELETE\n \n /administration/task/delete/{analysisTaskId}\n\n\nDelete an analysis task.\n\n\n\n\n\n\n\nAnalysis Tasks List\nGET\n \n /analysis/tasks\n\n\nRetrieve a list of analysis tasks.\n\n\n\n\n\n\n\nAnalysis Task\nGET\n \n/analysis/task/{analysistaskid}\n\n\nRetrieve an analysis task by Id.\n\n\n\n\n\n\n\nAnalysis Tasks List By Project\nGET\n \n /analysis/tasks/project/{projectid}\n\n\nRetrieve a list of analysis tasks by project.\n\n\n\n\n\n\n\nAnalysis Tasks Status By Project\nGET\n \n /analysis/tasks/status/project/{projectid}\n\n\nRetrieve a gloabl status of the analysis tasks by project.\n\n\n\n\n\n\n\nWorkers List\nGET\n \n /analysis/workers\n\n\nRetrieve a list of workers.\n\n\n\n\n\n\n\nMetric Providers List\nGET\n \n /analysis/metricproviders\n\n\nRetrieve a list of Metric Providers.\n\n\n\n\n\n\n\nPromote Analysis Task\nDELETE\n \n /analysis/task/promote/{analysisTaskId}\n\n\nPush up an analysis task in Workers Queue.\n\n\n\n\n\n\n\nDemote Analysis Task\nDELETE\n \n /analysis/task/demote/{analysisTaskId}\n\n\nPush down an analysis task in Workers Queue.\n\n\n\n\n\n\n\nPush Analysis Task On Worker\nDELETE\n \n /analysis/task/pushOnWorker/{analysisTaskId}/w/{workerId}\n\n\nForce the execution of an analysis task by a worker.\n\n\n\n\n\nMetrics Provider API\n\n\nAccess to Mertics collected  by the Metrics Provider component.\n\n\n\n\nList of Metrics\nGET\n \n /metricprovider/metrics\n\n\nRetrieve a list of the visualisable metric providers supported by the plat-\nform. It includes information about how the metric provider should be visualised.\n\n\n\n\n\n\n\nList of Raw Metrics\nGET\n \n /metricprovider/raw/metrics\n\n\nRetrieve a list of the all of the metric providers supported by the plat-\nform.\n\n\n\n\n\n\n\nList of Metric Visualisation\nGET\n \n /metricprovider/projects/p/{projectId}/m/{metricId}\n\n\nRetrieve the visualisation of a specific project metric provider. Visuali-\nsations are defined in the MetVis language.\n\n\n\n\n\n\n\nList of Factoids\nGET\n \n /metricprovider/factoids\n\n\nRetrieve a list of factoids supported by the platform.\n\n\n\n\n\n\n\nList of Project Factoids\nGET\n \n /metricprovider/projects/p/{projectId}/f\n\n\nRetrieve the data of a specific factoid for a project.\n\n\n\n\n\n\n\nMetric Raw  Data\nGET\n \n /metricprovider/raw/projects/p/{projectId}/m/{metricId}\n\n\nRetrieve the raw data of a project\u2019s metric provider. This is essentially a\nJSON dump of the metric provider\u2019s MongoDB collection.\n\n\n\n\n\nWorkflow Execution Engine API\n\n\nTODO\n\n\nKnowledge Base API\n\n\n\n\nList of analyzed artifacts\nGET\n \n /api/artifacts\n\n\nRetrieve a list of monitored artifacts.\n\n\n\n\n\n\n\nList of similar artifacts\nGET\n \n\n/api/recommendation/similar/p/{id}/m/{sim_method}/n/{num}\n\n\n\nThis resource is used to retrieve projects that are similar to a given project.\n\n\n\n\n\nDevOps Dashboard API",
            "title": "REST API Documentation"
        },
        {
            "location": "/users/REST-API-Documentation/#rest-api-documentation",
            "text": "",
            "title": "REST API Documentation"
        },
        {
            "location": "/users/REST-API-Documentation/#project-administration-api",
            "text": "The Administration API provides services related to platform administration, user management  and registration of projects to  be analyse by the platform.   Base URL :  \"/administration/\"",
            "title": "Project Administration API"
        },
        {
            "location": "/users/REST-API-Documentation/#project-management",
            "text": "Manage registration of projects analysed by the platform.   Projects List GET    /administration/projects/{page}  Retrieve a list of monitored projects.    Project Search GET    /administration/projects/search  Retrieve a list of monitored projects base on a search query.    Project Info GET    /administration/projects/{projectId}  Retrieve the metadata of a specific project.    Import Project POST    /administration/projects/import  Import a new project to be analyzed by the platform. Take as input the URL of the repository of the project to import.    Register Project POST    /administration/projects/register  Register a new project to be analyzed by the platform. Take as input the definition of project to analyse.",
            "title": "Project Management"
        },
        {
            "location": "/users/REST-API-Documentation/#project-analysis",
            "text": "Analyse projects registered on the platform.   Create Analysis Task POST    /administration/task/create  Create a new analysis task.    Update Analysis Task PUT    /administration/task/update  Update an analysis task.    Start Analysis Task POST    /administration/task/start  Start an analysis task.    Stop Analysis Task POST    /administration/task/stop  Stop an analysis task.    Reset Analysis Task POST    /administration/task/reset  Reset an analysis task.    Delete Analysis Task DELETE    /administration/task/delete/{analysisTaskId}  Delete an analysis task.    Analysis Tasks List GET    /analysis/tasks  Retrieve a list of analysis tasks.    Analysis Task GET   /analysis/task/{analysistaskid}  Retrieve an analysis task by Id.    Analysis Tasks List By Project GET    /analysis/tasks/project/{projectid}  Retrieve a list of analysis tasks by project.    Analysis Tasks Status By Project GET    /analysis/tasks/status/project/{projectid}  Retrieve a gloabl status of the analysis tasks by project.    Workers List GET    /analysis/workers  Retrieve a list of workers.    Metric Providers List GET    /analysis/metricproviders  Retrieve a list of Metric Providers.    Promote Analysis Task DELETE    /analysis/task/promote/{analysisTaskId}  Push up an analysis task in Workers Queue.    Demote Analysis Task DELETE    /analysis/task/demote/{analysisTaskId}  Push down an analysis task in Workers Queue.    Push Analysis Task On Worker DELETE    /analysis/task/pushOnWorker/{analysisTaskId}/w/{workerId}  Force the execution of an analysis task by a worker.",
            "title": "Project Analysis"
        },
        {
            "location": "/users/REST-API-Documentation/#metrics-provider-api",
            "text": "Access to Mertics collected  by the Metrics Provider component.   List of Metrics GET    /metricprovider/metrics  Retrieve a list of the visualisable metric providers supported by the plat-\nform. It includes information about how the metric provider should be visualised.    List of Raw Metrics GET    /metricprovider/raw/metrics  Retrieve a list of the all of the metric providers supported by the plat-\nform.    List of Metric Visualisation GET    /metricprovider/projects/p/{projectId}/m/{metricId}  Retrieve the visualisation of a specific project metric provider. Visuali-\nsations are defined in the MetVis language.    List of Factoids GET    /metricprovider/factoids  Retrieve a list of factoids supported by the platform.    List of Project Factoids GET    /metricprovider/projects/p/{projectId}/f  Retrieve the data of a specific factoid for a project.    Metric Raw  Data GET    /metricprovider/raw/projects/p/{projectId}/m/{metricId}  Retrieve the raw data of a project\u2019s metric provider. This is essentially a\nJSON dump of the metric provider\u2019s MongoDB collection.",
            "title": "Metrics Provider API"
        },
        {
            "location": "/users/REST-API-Documentation/#workflow-execution-engine-api",
            "text": "TODO",
            "title": "Workflow Execution Engine API"
        },
        {
            "location": "/users/REST-API-Documentation/#knowledge-base-api",
            "text": "List of analyzed artifacts GET    /api/artifacts  Retrieve a list of monitored artifacts.    List of similar artifacts GET   \n/api/recommendation/similar/p/{id}/m/{sim_method}/n/{num}  This resource is used to retrieve projects that are similar to a given project.",
            "title": "Knowledge Base API"
        },
        {
            "location": "/users/REST-API-Documentation/#devops-dashboard-api",
            "text": "",
            "title": "DevOps Dashboard API"
        },
        {
            "location": "/users/REST-API-Generation/",
            "text": "REST API Generation\n\n\nREST API Tutorial files\n\n\nInstall\n\n\nFirst of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install.\n\n\nExecute the following steps.\n\n\n\n\nClone repository: \nhttps://github.com/patrickneubauer/crossminer-workflow\n\n\nImport projects from the cloned repo to empty Eclipse workspace:\n\n\nImport root via \nMaven > Existing Maven Projects\n,\n\n\nthen the rest via \nGeneral > Existing projects into Workspace\n \nwithout\n checking \nSearch for nested projects\n.\n\n\nInstall new software packages:\n\n\nInstall Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling \n[web page]\n\n\nInstall Epsilon (Stable): http://download.eclipse.org/epsilon/updates/ \n[update site]\n\n\nUpdate Epsilon (Interim): http://download.eclipse.org/epsilon/interim/ \n[update site]\n\n\nInstall Emfatic: http://download.eclipse.org/emfatic/update/ \n[update site]\n\n\nNote:\n I didn't see the the feature until \nGroup items by category\n was unchecked.\n\n\nInstall GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ \n[update site]\n\n\nImport the given json projects (\norg.eclipse.epsilon.emc.json\n and \norg.eclipse.epsilon.emc.json.dt\n) into the workspace via \nGeneral > Existing projects into Workspace\n.\n\n\nNow right-click on \norg.eclipse.crossmeter.workflow project\n and select \nMaven > Update Project....\n\n\nCopy the give \nM2M_Environment_oxygen.launch\n (for Windows 10) to \norg.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator\n and overwrite the old one.\n\n\nRefresh project \norg.eclipse.crossmeter.workflow.restmule.generator\n in Eclipse.\n\n\nBefore running the \n.launch\n file, right-click on it, \nRun as > Run configurations....\n Here go to \nPlug-ins\n tab and click on \nAdd Required Plug-ins\n.\n\n\nNow you can run the \n.launch\n file TBA:How?\n\n\nIn the Runtime Eclipse import the \norg.eclipse.crossmeter.workflow.restmule.generator\n project as Maven Project.\n\n\nRun \ngenerateFromOAS.launch\n as you can see in the videos https://youtu.be/BJXuozHJPeg.\n\n\nNow, you should see it generating the project.\n\n\nFor further steps, watch Patrick's videos .\n\n\ngithub client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg\n\n\ngithub client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI\n\n\ngithub client generation example \u2014 https://youtu.be/ltSNnSZRETA\n\n\nkafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc\n\n\nkafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4\n\n\nkafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA\n\n\nMDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk\n\n\n\n\nExample\n\n\nI've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following: \n/square/{number}\n. You can replace the \n{number}\n with any number. The repsonse for this request is a \nJSON\n object, which looks like this:\n\n\n{\n    \"squared\": {squaredValue}\n}\n\n\n\n\nIt contains only one field, named \nsquared\n, which has the value of the square of the given number. From the specification the generator will provide us a \nJava Project\n which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines:\n\n\nITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings).\nint number = 7; //The number to be squared.\nIData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server.\nNumberValue numberValue; //NumberValue is the class which represents the object received from the server\nnumberValue = squared.observe().blockingSingle(); //Get the actually received object from the response.\nSystem.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer.\n\n\n\n\nEverything, including the \nIEntityApi\n interface and the \nNumberValue\n class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through \nJava\n to the objects/field inside of the received \nJSON\n object. And once again, they are all generated from the OpenAPI specification.\n\n\nSo, to generate the mentioned \nJava Project\n, you have to put your OpenAPI specification file into the \nschemas\n folder inside the \norg.eclipse.crossmeter.workflow.restmule.generator\n project. I'll call mine as \nTestAPI.json\n, and it's made up of the following:\n\n\n{\n  \"swagger\": \"2.0\",\n  \"schemes\": [\n    \"http\"\n  ],\n  \"host\": \"localhost:8080\",\n  \"basePath\": \"/\",\n  \"info\": {\n    \"description\": \"This is a test API, only for demonstration of the generator.\",\n    \"termsOfService\": \"\",\n    \"title\": \"TestAPI\",\n    \"version\": \"v1\"\n  },\n  \"consumes\": [\n    \"application/json\"\n  ],\n  \"produces\": [\n    \"application/json\"\n  ],\n  \"securityDefinitions\": {\n\n  },\n  \"paths\": {\n    \"/square/{number}\": {\n      \"get\": {\n        \"description\": \"Square a number.\",\n        \"parameters\": [\n          {\n            \"description\": \"The number to be squared\",\n            \"in\": \"path\",\n            \"name\": \"number\",\n            \"required\": true,\n            \"type\": \"integer\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"OK\",\n            \"schema\": {\n              \"$ref\": \"#/definitions/NumberValue\"\n            }\n          }\n        }\n      }\n    }\n  },\n  \"definitions\": {\n    \"NumberValue\": {\n      \"properties\": {\n        \"squared\": {\n          \"type\": \"integer\"\n        }\n      },\n      \"type\": \"object\"\n    }\n  }\n}\n\n\n\n\nYou can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own.\nThen we have to set the generator to use our new schema. To do this, open the \nbuild.xml\n in the generator project and modify the \napi\n and \njson.model.file\n property to \ntestapi\n and \nschemas/TestAPI.json\n, respectively.\n\n\n<!--API Variables -->\n<property name=\"api\" value=\"testapi\" />\n<property name=\"json.model.file\" value=\"schemas/TestAPI.json\" />\n\n\n\n\nWe are almost done, but we have to take one more small step. We have to provide an \n.eol\n file in the \nepsilon/util/fix/\n folder in the generator project. Its name must be the same as the value of the \napi\n property in the previous step, so for this example we use the \ntestapi.eol\n. The content of this file:\n\n\nimport \"../restmule.eol\";\n\nvar api = RestMule!API.all.first();\n\n// RATE LIMITS\n\nvar search = new RestMule!RatePolicyScope;\nsearch.scope = \"Search\";\nvar entity = new RestMule!RatePolicyScope;\nentity.scope = \"Entity\";\n\n// RATE POLICY\n\nvar policy = new RestMule!RatePolicy;\npolicy.scopes.add(entity);\npolicy.scopes.add(search);\n\nvar reset = new RestMule!ResponseHeader;\nvar resetInt = new RestMule!TInteger;\nresetInt.label = \"X-RateLimit-Reset\";\nreset.type = resetInt;\npolicy.reset = reset;\n\nvar limit = new RestMule!ResponseHeader;\nvar limitInt = new RestMule!TInteger;\nlimitInt.label = \"X-RateLimit-Limit\";\nlimit.type = limitInt;\npolicy.limit = limit;\n\nvar remaining = new RestMule!ResponseHeader;\nvar remainingInt = new RestMule!TInteger;\nremainingInt.label = \"X-RateLimit-Remaining\";\nremaining.type = remainingInt;\npolicy.remaining = remaining;\n\napi.ratePolicy = policy;\n\n// PAGINATION\n\nvar pagination= new RestMule!PaginationPolicy;\npagination.start = 1;\npagination.max = 10;\npagination.increment = 1;\npagination.maxPerIteration = 100;\n\nvar perIteration = new RestMule!Query;\nperIteration.description = \"Items per page\";\nperIteration.required = false;\nvar type = new RestMule!TInteger;\ntype.label = \"per_page\";\ntype.name = type.label;\nperIteration.type = type;\npagination.perIteration = perIteration;\n\nvar page = new RestMule!Query;\npage.description = \"Page identifier\";\npage.required = false;\nvar type1 = new RestMule!TInteger;\ntype1.label = \"page\";\ntype1.name = type1.label;\npage.type = type1;\npagination.page = page;\n\nvar link = new RestMule!ResponseHeader;\nlink.description = \"Page links\";\nvar format = new RestMule!TFormattedString;\nformat.label = \"Link\";\nformat.name = format.label;\nlink.type = format;\npagination.links = link;\n\napi.pagination = pagination;\n\n// WRAPPER\n\nvar wrapper = new RestMule!Wrapper;\nwrapper.name = \"Wrapper\";\nvar items = new RestMule!ListType;\nitems.label = \"items\";\nwrapper.items = items;\nwrapper.totalLabel= \"total_count\";\nwrapper.incompleteLabel = \"incomplete_results\";\napi.pageWrapper = wrapper;\n\n// ADD RATE & WRAPPER TO REQUESTS (FIXME)\n\nfor (r in RestMule!Request.all){\n    if (r.parent.path.startsWith(\"/search\")){\n        r.scope = search;\n    } else {\n        r.scope = entity;\n    }\n    r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader)));\n    for (resp in r.responses.select(s|s.responseType <> null)){\n        resp.unwrap();\n    }\n}\n\n/*  //////////\n * OPERATIONS\n *//////////\noperation RestMule!ObjectType hasWrapper() : Boolean {\n    var wrapper = RestMule!Wrapper.all.first;\n    var lists = self.listFields.collect(a|a.label);\n    return (not lists.isEmpty()) and lists\n        .includes(wrapper.items.label);\n}\n\noperation RestMule!Response unwrap() : RestMule!ObjectType{\n    if (self.responseType.instanceOf(ObjectType)){\n        if (self.responseType.hasWrapper()){\n            (\"Unwrapping : \"+ self.responseType.name).println;\n            var wrapper = RestMule!Wrapper.all.first;\n            var name = self.responseType.name.println;\n            self.responseType = self.responseType.println.listFields\n                .select(b| b.label == wrapper.items.label).first.elements.first;            \n            self.responseType.name = name;\n            self.pageWrapped = true;\n            self.responseType.description = \"UNWRAPPED: \" + self.responseType.description;\n        }\n    }\n}\n\n\n\n\nAfter this, we can run the generator and it will generate our \nJava Project\n from the specification.\n\n\nIn the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.",
            "title": "REST API Generation"
        },
        {
            "location": "/users/REST-API-Generation/#rest-api-generation",
            "text": "REST API Tutorial files",
            "title": "REST API Generation"
        },
        {
            "location": "/users/REST-API-Generation/#install",
            "text": "First of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install.  Execute the following steps.   Clone repository:  https://github.com/patrickneubauer/crossminer-workflow  Import projects from the cloned repo to empty Eclipse workspace:  Import root via  Maven > Existing Maven Projects ,  then the rest via  General > Existing projects into Workspace   without  checking  Search for nested projects .  Install new software packages:  Install Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling  [web page]  Install Epsilon (Stable): http://download.eclipse.org/epsilon/updates/  [update site]  Update Epsilon (Interim): http://download.eclipse.org/epsilon/interim/  [update site]  Install Emfatic: http://download.eclipse.org/emfatic/update/  [update site]  Note:  I didn't see the the feature until  Group items by category  was unchecked.  Install GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/  [update site]  Import the given json projects ( org.eclipse.epsilon.emc.json  and  org.eclipse.epsilon.emc.json.dt ) into the workspace via  General > Existing projects into Workspace .  Now right-click on  org.eclipse.crossmeter.workflow project  and select  Maven > Update Project....  Copy the give  M2M_Environment_oxygen.launch  (for Windows 10) to  org.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator  and overwrite the old one.  Refresh project  org.eclipse.crossmeter.workflow.restmule.generator  in Eclipse.  Before running the  .launch  file, right-click on it,  Run as > Run configurations....  Here go to  Plug-ins  tab and click on  Add Required Plug-ins .  Now you can run the  .launch  file TBA:How?  In the Runtime Eclipse import the  org.eclipse.crossmeter.workflow.restmule.generator  project as Maven Project.  Run  generateFromOAS.launch  as you can see in the videos https://youtu.be/BJXuozHJPeg.  Now, you should see it generating the project.  For further steps, watch Patrick's videos .  github client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg  github client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI  github client generation example \u2014 https://youtu.be/ltSNnSZRETA  kafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc  kafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4  kafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA  MDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk",
            "title": "Install"
        },
        {
            "location": "/users/REST-API-Generation/#example",
            "text": "I've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following:  /square/{number} . You can replace the  {number}  with any number. The repsonse for this request is a  JSON  object, which looks like this:  {\n    \"squared\": {squaredValue}\n}  It contains only one field, named  squared , which has the value of the square of the given number. From the specification the generator will provide us a  Java Project  which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines:  ITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings).\nint number = 7; //The number to be squared.\nIData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server.\nNumberValue numberValue; //NumberValue is the class which represents the object received from the server\nnumberValue = squared.observe().blockingSingle(); //Get the actually received object from the response.\nSystem.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer.  Everything, including the  IEntityApi  interface and the  NumberValue  class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through  Java  to the objects/field inside of the received  JSON  object. And once again, they are all generated from the OpenAPI specification.  So, to generate the mentioned  Java Project , you have to put your OpenAPI specification file into the  schemas  folder inside the  org.eclipse.crossmeter.workflow.restmule.generator  project. I'll call mine as  TestAPI.json , and it's made up of the following:  {\n  \"swagger\": \"2.0\",\n  \"schemes\": [\n    \"http\"\n  ],\n  \"host\": \"localhost:8080\",\n  \"basePath\": \"/\",\n  \"info\": {\n    \"description\": \"This is a test API, only for demonstration of the generator.\",\n    \"termsOfService\": \"\",\n    \"title\": \"TestAPI\",\n    \"version\": \"v1\"\n  },\n  \"consumes\": [\n    \"application/json\"\n  ],\n  \"produces\": [\n    \"application/json\"\n  ],\n  \"securityDefinitions\": {\n\n  },\n  \"paths\": {\n    \"/square/{number}\": {\n      \"get\": {\n        \"description\": \"Square a number.\",\n        \"parameters\": [\n          {\n            \"description\": \"The number to be squared\",\n            \"in\": \"path\",\n            \"name\": \"number\",\n            \"required\": true,\n            \"type\": \"integer\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"OK\",\n            \"schema\": {\n              \"$ref\": \"#/definitions/NumberValue\"\n            }\n          }\n        }\n      }\n    }\n  },\n  \"definitions\": {\n    \"NumberValue\": {\n      \"properties\": {\n        \"squared\": {\n          \"type\": \"integer\"\n        }\n      },\n      \"type\": \"object\"\n    }\n  }\n}  You can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own.\nThen we have to set the generator to use our new schema. To do this, open the  build.xml  in the generator project and modify the  api  and  json.model.file  property to  testapi  and  schemas/TestAPI.json , respectively.  <!--API Variables -->\n<property name=\"api\" value=\"testapi\" />\n<property name=\"json.model.file\" value=\"schemas/TestAPI.json\" />  We are almost done, but we have to take one more small step. We have to provide an  .eol  file in the  epsilon/util/fix/  folder in the generator project. Its name must be the same as the value of the  api  property in the previous step, so for this example we use the  testapi.eol . The content of this file:  import \"../restmule.eol\";\n\nvar api = RestMule!API.all.first();\n\n// RATE LIMITS\n\nvar search = new RestMule!RatePolicyScope;\nsearch.scope = \"Search\";\nvar entity = new RestMule!RatePolicyScope;\nentity.scope = \"Entity\";\n\n// RATE POLICY\n\nvar policy = new RestMule!RatePolicy;\npolicy.scopes.add(entity);\npolicy.scopes.add(search);\n\nvar reset = new RestMule!ResponseHeader;\nvar resetInt = new RestMule!TInteger;\nresetInt.label = \"X-RateLimit-Reset\";\nreset.type = resetInt;\npolicy.reset = reset;\n\nvar limit = new RestMule!ResponseHeader;\nvar limitInt = new RestMule!TInteger;\nlimitInt.label = \"X-RateLimit-Limit\";\nlimit.type = limitInt;\npolicy.limit = limit;\n\nvar remaining = new RestMule!ResponseHeader;\nvar remainingInt = new RestMule!TInteger;\nremainingInt.label = \"X-RateLimit-Remaining\";\nremaining.type = remainingInt;\npolicy.remaining = remaining;\n\napi.ratePolicy = policy;\n\n// PAGINATION\n\nvar pagination= new RestMule!PaginationPolicy;\npagination.start = 1;\npagination.max = 10;\npagination.increment = 1;\npagination.maxPerIteration = 100;\n\nvar perIteration = new RestMule!Query;\nperIteration.description = \"Items per page\";\nperIteration.required = false;\nvar type = new RestMule!TInteger;\ntype.label = \"per_page\";\ntype.name = type.label;\nperIteration.type = type;\npagination.perIteration = perIteration;\n\nvar page = new RestMule!Query;\npage.description = \"Page identifier\";\npage.required = false;\nvar type1 = new RestMule!TInteger;\ntype1.label = \"page\";\ntype1.name = type1.label;\npage.type = type1;\npagination.page = page;\n\nvar link = new RestMule!ResponseHeader;\nlink.description = \"Page links\";\nvar format = new RestMule!TFormattedString;\nformat.label = \"Link\";\nformat.name = format.label;\nlink.type = format;\npagination.links = link;\n\napi.pagination = pagination;\n\n// WRAPPER\n\nvar wrapper = new RestMule!Wrapper;\nwrapper.name = \"Wrapper\";\nvar items = new RestMule!ListType;\nitems.label = \"items\";\nwrapper.items = items;\nwrapper.totalLabel= \"total_count\";\nwrapper.incompleteLabel = \"incomplete_results\";\napi.pageWrapper = wrapper;\n\n// ADD RATE & WRAPPER TO REQUESTS (FIXME)\n\nfor (r in RestMule!Request.all){\n    if (r.parent.path.startsWith(\"/search\")){\n        r.scope = search;\n    } else {\n        r.scope = entity;\n    }\n    r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader)));\n    for (resp in r.responses.select(s|s.responseType <> null)){\n        resp.unwrap();\n    }\n}\n\n/*  //////////\n * OPERATIONS\n *//////////\noperation RestMule!ObjectType hasWrapper() : Boolean {\n    var wrapper = RestMule!Wrapper.all.first;\n    var lists = self.listFields.collect(a|a.label);\n    return (not lists.isEmpty()) and lists\n        .includes(wrapper.items.label);\n}\n\noperation RestMule!Response unwrap() : RestMule!ObjectType{\n    if (self.responseType.instanceOf(ObjectType)){\n        if (self.responseType.hasWrapper()){\n            (\"Unwrapping : \"+ self.responseType.name).println;\n            var wrapper = RestMule!Wrapper.all.first;\n            var name = self.responseType.name.println;\n            self.responseType = self.responseType.println.listFields\n                .select(b| b.label == wrapper.items.label).first.elements.first;            \n            self.responseType.name = name;\n            self.pageWrapped = true;\n            self.responseType.description = \"UNWRAPPED: \" + self.responseType.description;\n        }\n    }\n}  After this, we can run the generator and it will generate our  Java Project  from the specification.  In the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.",
            "title": "Example"
        },
        {
            "location": "/users/Scava-Metrics/",
            "text": "Metrics computed by Scava\n\n\nMetric-platform\n\n\nOSGi metrics\n are defined in:\n\n\n\n\nhttps://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.osgi/src/OSGi.rsc\n\n\n\n\nList of metrics:\n\n\n\n\nallOSGiBundleDependencies -- Retrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies).\n\n\nallOSGiPackageDependencies -- Retrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).\n\n\nallOSGiDynamicImportedPackages -- Retrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file.\n\n\nnumberOSGiPackageDependencies -- Retrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).\n\n\nnumberOSGiBundleDependencies -- Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies).\n\n\nunusedOSGiImportedPackages -- Retrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell).\n\n\nratioUnusedOSGiImportedPackages -- Retrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages.\n\n\nratioUsedOSGiImportedPackages -- Retrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code.\n\n\nnumberOSGiSplitImportedPackages -- Retrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest.\n\n\nnumberOSGiSplitExportedPackages -- Retrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest..\n\n\nunversionedOSGiRequiredBundles -- Retrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header.\n\n\nratioUnversionedOSGiRequiredBundles -- Retrieves the ratio of unversioned OSGi required bundles.\n\n\nunversionedOSGiImportedPackages -- Retrieves the set of unversioned OSGi imported packages (declared in the Import-Package header). If returned value != {} there is a smell in the Manifest.\n\n\nratioUnversionedOSGiImportedPackages -- Retrieves the ratio of unversioned OSGi imported packages.\n\n\nunversionedOSGiExportedPackages -- Retrieves the set of unversioned OSGi exported packages (declared in the Export-Package header). If returned value != {} there is a smell in the Manifest.\n\n\nratioUnversionedOSGiExportedPackages -- Retrieves the ratio of unversioned OSGi exported packages.\n\n\n\n\nMaven metrics\n are defined in \n\n\n\n\nhttps://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.maven/src/Maven.rsc\n\n\n\n\nList of metrics: \n\n\n\n\nallMavenDependencies -- Retrieves all the Maven dependencies.\n\n\nallOptionalMavenDependencies -- Retrieves all the optional Maven dependencies.\n\n\nnumberMavenDependencies -- Retrieves the number of Maven dependencies.\n\n\nnumberUniqueMavenDependencies -- Retrieves the number of unique Maven dependencies.\n\n\nratioOptionalMavenDependencies -- Retrieves the ratio of optional Maven dependencies.\n\n\nisUsingTycho -- Checks if the current project is a Tycho project.\n\n\n\n\nIDE plugin\n\n\nAll metrics are defined in \nmetrics_definition_ide_plugin.docx\n\n\n\n\nscava-lib-usage -- Level of using CROSSMINER library change function.\n\n\nscava-search-usage -- Level of using CROSSMINER search function.\n\n\nscava-search-success -- Level of successfull free text serach using CROSSMINER plug-in.\n\n\nmodifictaion-rate -- Rate of changes applied to document.\n\n\ngui-usage-rate -- Rate of activly using the GUI of Eclipse.\n\n\ntesting-rate -- Count of executing tests.\n\n\nworking-time -- Average working time for Java source files.\n\n\nfile-access-rate -- Average count of Java source files open and brought to top.\n\n\n\n\nNatural Language Processing\n\n\nAll metrics are defined in \nmetrics_definition_nlp.docx\n.\n\n\nIssue tracking\n metrics:\n\n\n\n\nNumber of bugs: It indicates how many bugs, through the time, are in total all the issue tracking systems related to a project.\n\n\nNumber of comments: This metric indicates, through the time, the evolution in the quantity of comments for all the bugs related to a project.\n\n\nEmotions: This metric expresses, grosso modo, the emotions found in the issue tracker for a given project.\n\n\nNumber new bugs: It indicates how many bugs are created every delta through a lapse of time.\n\n\nNumber of new users: How many users are new in the bug tracking system.\n\n\nOpen time: This metric shows the average open time of an issue.\n\n\nNumber of patches: It indicates the number of patches located in the issue tracking system.\n\n\nNumber of request and replies: From all the comments in the issue tracking system, how many of them are replies and how many are requests.\n\n\nResponse time: It presents the average time to reply a comment in an issue tracker.\n\n\nSentiments: Which sentiments are found in a issue tracking system for a project.\n\n\nSeverity: The severity level of the bugs found in the issue tracker. This metric classifies a bug report into 1 of 7 severity levels such as Blocker, Critical, Major, Normal, Minor, Trivial or Enhancement.\n\n\nStatus: It indicates the issues status, like open or closed.\n\n\nTopics: Using clustering methods, this metric indicates the topics that are discussed through the time.\n\n\nUnasnwered bugs: It indicates how many issues do not have any reply.\n\n\nUsers: How many users are found in a issue tracker system.\n\n\n\n\nNewsgroup\n metrics\n\n\n\n\nNumber of articles; It determines the number of articles found in a newsgroup.\n\n\nEmotions: Which emotions are found in the newgroup.\n\n\nNumber of threads: How many threads a newsgroup contains. In other words, how many different branches have been created since the first email was sent to the newsgroup.\n\n\nNumber of new users: How many users make use of the newsgroup.\n\n\nNumber of request and replies: From the total number of emails belonging to the newsgroup, how many of them are request and how many are replies.\n\n\nResponse time: The average time to reply a message.\n\n\nSentiments: The sentiments found in the newsgroup.\n\n\nSeverity: The severity level of the messages exchanges in the newsgroup. The severity is measured in terms of how severe an issue is.\n\n\nNumber of new threads: How many threads are created per delta.\n\n\nTopics: Using the clustering algorithm, we determine which are the most frequent topics discussed in the newsgroup.\n\n\nNumber of unanswered topics: How many messages haven\u2019t been answered.\n\n\nNumber of users: Number of users that make use of the newsgroup.\n\n\n\n\nForums\n metrics\n\n\n\n\nNumber of posts: For every topic (i.e. forum thread), how many posts in total exist.\n\n\nEmotions: Which are the emotions located in the forum.\n\n\nNumber of topics: How many forum threads contain the forum.\n\n\nNumber of request and replies: From all the posts in the forum, how many are request and how many are reply.\n\n\nSentiments: Which are the sentiments that can be found in a forum.\n\n\nSeverity: In the case the forums are used to express issues, we can determine their severity.\n\n\nNumber of new topics: Number of new forum threads created.\n\n\nTopics: Number of content topics, i.e. subjects, are discussed in the forum. \n\n\nNumber of unanswered topics: Number of forums threads which do not have a reply.",
            "title": "Scava Metrics"
        },
        {
            "location": "/users/Scava-Metrics/#metrics-computed-by-scava",
            "text": "",
            "title": "Metrics computed by Scava"
        },
        {
            "location": "/users/Scava-Metrics/#metric-platform",
            "text": "OSGi metrics  are defined in:   https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.osgi/src/OSGi.rsc   List of metrics:   allOSGiBundleDependencies -- Retrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies).  allOSGiPackageDependencies -- Retrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).  allOSGiDynamicImportedPackages -- Retrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file.  numberOSGiPackageDependencies -- Retrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies).  numberOSGiBundleDependencies -- Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies).  unusedOSGiImportedPackages -- Retrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell).  ratioUnusedOSGiImportedPackages -- Retrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages.  ratioUsedOSGiImportedPackages -- Retrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code.  numberOSGiSplitImportedPackages -- Retrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest.  numberOSGiSplitExportedPackages -- Retrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest..  unversionedOSGiRequiredBundles -- Retrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header.  ratioUnversionedOSGiRequiredBundles -- Retrieves the ratio of unversioned OSGi required bundles.  unversionedOSGiImportedPackages -- Retrieves the set of unversioned OSGi imported packages (declared in the Import-Package header). If returned value != {} there is a smell in the Manifest.  ratioUnversionedOSGiImportedPackages -- Retrieves the ratio of unversioned OSGi imported packages.  unversionedOSGiExportedPackages -- Retrieves the set of unversioned OSGi exported packages (declared in the Export-Package header). If returned value != {} there is a smell in the Manifest.  ratioUnversionedOSGiExportedPackages -- Retrieves the ratio of unversioned OSGi exported packages.   Maven metrics  are defined in    https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.maven/src/Maven.rsc   List of metrics:    allMavenDependencies -- Retrieves all the Maven dependencies.  allOptionalMavenDependencies -- Retrieves all the optional Maven dependencies.  numberMavenDependencies -- Retrieves the number of Maven dependencies.  numberUniqueMavenDependencies -- Retrieves the number of unique Maven dependencies.  ratioOptionalMavenDependencies -- Retrieves the ratio of optional Maven dependencies.  isUsingTycho -- Checks if the current project is a Tycho project.",
            "title": "Metric-platform"
        },
        {
            "location": "/users/Scava-Metrics/#ide-plugin",
            "text": "All metrics are defined in  metrics_definition_ide_plugin.docx   scava-lib-usage -- Level of using CROSSMINER library change function.  scava-search-usage -- Level of using CROSSMINER search function.  scava-search-success -- Level of successfull free text serach using CROSSMINER plug-in.  modifictaion-rate -- Rate of changes applied to document.  gui-usage-rate -- Rate of activly using the GUI of Eclipse.  testing-rate -- Count of executing tests.  working-time -- Average working time for Java source files.  file-access-rate -- Average count of Java source files open and brought to top.",
            "title": "IDE plugin"
        },
        {
            "location": "/users/Scava-Metrics/#natural-language-processing",
            "text": "All metrics are defined in  metrics_definition_nlp.docx .  Issue tracking  metrics:   Number of bugs: It indicates how many bugs, through the time, are in total all the issue tracking systems related to a project.  Number of comments: This metric indicates, through the time, the evolution in the quantity of comments for all the bugs related to a project.  Emotions: This metric expresses, grosso modo, the emotions found in the issue tracker for a given project.  Number new bugs: It indicates how many bugs are created every delta through a lapse of time.  Number of new users: How many users are new in the bug tracking system.  Open time: This metric shows the average open time of an issue.  Number of patches: It indicates the number of patches located in the issue tracking system.  Number of request and replies: From all the comments in the issue tracking system, how many of them are replies and how many are requests.  Response time: It presents the average time to reply a comment in an issue tracker.  Sentiments: Which sentiments are found in a issue tracking system for a project.  Severity: The severity level of the bugs found in the issue tracker. This metric classifies a bug report into 1 of 7 severity levels such as Blocker, Critical, Major, Normal, Minor, Trivial or Enhancement.  Status: It indicates the issues status, like open or closed.  Topics: Using clustering methods, this metric indicates the topics that are discussed through the time.  Unasnwered bugs: It indicates how many issues do not have any reply.  Users: How many users are found in a issue tracker system.   Newsgroup  metrics   Number of articles; It determines the number of articles found in a newsgroup.  Emotions: Which emotions are found in the newgroup.  Number of threads: How many threads a newsgroup contains. In other words, how many different branches have been created since the first email was sent to the newsgroup.  Number of new users: How many users make use of the newsgroup.  Number of request and replies: From the total number of emails belonging to the newsgroup, how many of them are request and how many are replies.  Response time: The average time to reply a message.  Sentiments: The sentiments found in the newsgroup.  Severity: The severity level of the messages exchanges in the newsgroup. The severity is measured in terms of how severe an issue is.  Number of new threads: How many threads are created per delta.  Topics: Using the clustering algorithm, we determine which are the most frequent topics discussed in the newsgroup.  Number of unanswered topics: How many messages haven\u2019t been answered.  Number of users: Number of users that make use of the newsgroup.   Forums  metrics   Number of posts: For every topic (i.e. forum thread), how many posts in total exist.  Emotions: Which are the emotions located in the forum.  Number of topics: How many forum threads contain the forum.  Number of request and replies: From all the posts in the forum, how many are request and how many are reply.  Sentiments: Which are the sentiments that can be found in a forum.  Severity: In the case the forums are used to express issues, we can determine their severity.  Number of new topics: Number of new forum threads created.  Topics: Number of content topics, i.e. subjects, are discussed in the forum.   Number of unanswered topics: Number of forums threads which do not have a reply.",
            "title": "Natural Language Processing"
        }
    ]
}