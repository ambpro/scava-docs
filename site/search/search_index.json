{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Scava Documentation This web site is the main documentation place for the Eclipse Scava project. SCAVA Installation Guide The SCAVA installation guide provides instructions on how to install and configure the CORSSMINER Platform on a server and how to deploy the Eclipses Plugin in development environment. SCAVA User Guide The SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin. SCAVA Developers Guide The developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API. SCAVA Contributors Guide The SCAVA Contributors guide containes all the material related to the architecture of the platform which could be useful for projects members and external open sources contributors. Useful links: Eclipse Scava home project: Eclipse Scava @ Eclipse Eclipse Scava code repository: github.com/crossminer/scava Eclipse Scava deployment repository: github.com/crossminer/scava-deployment Eclipse Scava documentation repository: github.com/crossminer/scava-docs Old Stuf : Documentation which must be migrated on one of the platfomrs guides","title":"Home"},{"location":"#scava-documentation","text":"This web site is the main documentation place for the Eclipse Scava project.","title":"Scava Documentation"},{"location":"#scava-installation-guide","text":"The SCAVA installation guide provides instructions on how to install and configure the CORSSMINER Platform on a server and how to deploy the Eclipses Plugin in development environment.","title":"SCAVA Installation Guide"},{"location":"#scava-user-guide","text":"The SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin.","title":"SCAVA User Guide"},{"location":"#scava-developers-guide","text":"The developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API.","title":"SCAVA Developers Guide"},{"location":"#scava-contributors-guide","text":"The SCAVA Contributors guide containes all the material related to the architecture of the platform which could be useful for projects members and external open sources contributors.","title":"SCAVA Contributors Guide"},{"location":"#useful-links","text":"Eclipse Scava home project: Eclipse Scava @ Eclipse Eclipse Scava code repository: github.com/crossminer/scava Eclipse Scava deployment repository: github.com/crossminer/scava-deployment Eclipse Scava documentation repository: github.com/crossminer/scava-docs Old Stuf : Documentation which must be migrated on one of the platfomrs guides","title":"Useful links:"},{"location":"contributors-guide/","text":"SCAVA Contributors Guide The Contributors Guide summarize the material related to the architecture of the platform which could be useful for projects members and external open sources contributors. Contributors Guidelines Guidelines related to the organisation ofthe SCAVA developement process. SCAVA Repository Organisation SCAVA Development Process SCAVA Testing recommendations SCAVA Naming recommendations SCAVA Licensing recommendations Technical Guidelines Tutorial and Guidelignes related to the various technologies used by the SCAVA Platform How to configure the API Gateway in order to integrate a new REST service How to consume a SCAVA REST services How to implement Restlet services How to extend the SCAVA data model How to generate REST API Documentation How to access to MongoDB using PONGO Architecture Guidelines Guidelines related to the architecture of some components of the SCAVA Platform API Gateway Component Authentication Component","title":"SCAVA Contributors Guide"},{"location":"contributors-guide/#scava-contributors-guide","text":"The Contributors Guide summarize the material related to the architecture of the platform which could be useful for projects members and external open sources contributors.","title":"SCAVA Contributors Guide"},{"location":"contributors-guide/#contributors-guidelines","text":"Guidelines related to the organisation ofthe SCAVA developement process. SCAVA Repository Organisation SCAVA Development Process SCAVA Testing recommendations SCAVA Naming recommendations SCAVA Licensing recommendations","title":"Contributors Guidelines"},{"location":"contributors-guide/#technical-guidelines","text":"Tutorial and Guidelignes related to the various technologies used by the SCAVA Platform How to configure the API Gateway in order to integrate a new REST service How to consume a SCAVA REST services How to implement Restlet services How to extend the SCAVA data model How to generate REST API Documentation How to access to MongoDB using PONGO","title":"Technical Guidelines"},{"location":"contributors-guide/#architecture-guidelines","text":"Guidelines related to the architecture of some components of the SCAVA Platform API Gateway Component Authentication Component","title":"Architecture Guidelines"},{"location":"contributors-guide/architecture-guidelignes/","text":"Architecture Guidelines Guidelines related to the architecture of some components of the SCAVA Platform API Gateway Component The API Gateway Component provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application) and a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services. Authentication Component The Scava Authentication service provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform and user management services, including user registration process, user profile editing and roles based authorization management.","title":"Architecture Guidelines"},{"location":"contributors-guide/architecture-guidelignes/#architecture-guidelines","text":"Guidelines related to the architecture of some components of the SCAVA Platform API Gateway Component The API Gateway Component provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application) and a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services. Authentication Component The Scava Authentication service provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform and user management services, including user registration process, user profile editing and roles based authorization management.","title":"Architecture Guidelines"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/","text":"API Gateway component The Scava API Gateway : Provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application). Provide a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services. API Gateway Architecture The API Gateway is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change. The API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the API Gateway will manage the authentication for all services of the platform. Authentication Mechanism JSON Web Tokens The Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io) Authentication Architecture In Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway. Authentication Flow To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Implementation The implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures. https://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul API Gateway Configuration The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters. Server Configuration id : server.port default : 8086 Port of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port. JWT Security Configuration id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. id : apigateway.security.jwt.url default : /login URL Path of the authentication service. id : apigateway.security.jwt.expiration default : 86400 (24H) Port of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port. Routing : Authentication Service Configuration id : zuul.routes.auth-center.path default : /api/authentication/** Relative path of the authentication service. id : zuul.routes.auth-center.url default : NA URL of the authentification server. Example: http://127.0.0.1:8081/ id : zuul.routes.auth-center.sensitiveHeaders default : Cookie,Set-Cookie Specify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers. id : zuul.routes.auth-center.stripPrefix default : false Switch off the stripping of the service-specific prefix from individual routes Routing : Service Configuration id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1 Configuration file example #API Gateway Port server.port=8086 # JWT Configuration apigateway.security.jwt.secret=otherpeopledontknowit apigateway.security.jwt.url=/api/authentication apigateway.security.jwt.expiration=86400 # Rooting Configuration : Authentication Service zuul.routes.auth-center.path=/api/authentication/** zuul.routes.auth-center.url=http://127.0.0.1:8081/ zuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie zuul.routes.auth-center.stripPrefix=false # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2 Control access API The Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including: * \u201cROLE_ADMIN\u201d * \u201cROLE_PROJECT_MANAGER\u201d * \u201cROLE_USER\u201d By the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d. # Filtering private restApi scava.routes.config.adminAccessApi[0]=/api/users scava.routes.config.adminAccessApi[1]=/api/user/** scava.routes.config.projectManagerAccessApi[0]=/administration/projects/create scava.routes.config.projectManagerAccessApi[1]=/administration/projects/import scava.routes.config.projectManagerAccessApi[2]=/administration/analysis/** scava.routes.config.userAccessApi[0]=/administration/projects scava.routes.config.userAccessApi[1]=/administration/projects/p/** scava.routes.config.userAccessApi[2]=/api/users/** scava.routes.config.userAccessApi[3]=/api/account Packaging Form Sources Maven Packaging mvn -Pprod install API Gateway Execution complete an put the \"application.properties\" configuration file in the execute directory. Execute the crossmeter-api-gateway-1.0.0.jar Jar. java -jar scava-api-gateway-1.0.0.jar Client Implementation How to consume a Scava REST services ? \\ This guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.","title":"API Gateway component"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-component","text":"The Scava API Gateway : Provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application). Provide a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services.","title":"API Gateway component"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-architecture","text":"The API Gateway is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change. The API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the API Gateway will manage the authentication for all services of the platform.","title":"API Gateway Architecture"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#authentication-mechanism","text":"","title":"Authentication Mechanism"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#json-web-tokens","text":"The Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io)","title":"JSON Web Tokens"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#authentication-architecture","text":"In Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway.","title":"Authentication Architecture"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#authentication-flow","text":"To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service.","title":"Authentication Flow"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#implementation","text":"The implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures. https://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul","title":"Implementation"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-configuration","text":"The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.","title":"API Gateway Configuration"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#server-configuration","text":"id : server.port default : 8086 Port of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port.","title":"Server Configuration"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#jwt-security-configuration","text":"id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. id : apigateway.security.jwt.url default : /login URL Path of the authentication service. id : apigateway.security.jwt.expiration default : 86400 (24H) Port of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port.","title":"JWT Security Configuration"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#routing-authentication-service-configuration","text":"id : zuul.routes.auth-center.path default : /api/authentication/** Relative path of the authentication service. id : zuul.routes.auth-center.url default : NA URL of the authentification server. Example: http://127.0.0.1:8081/ id : zuul.routes.auth-center.sensitiveHeaders default : Cookie,Set-Cookie Specify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers. id : zuul.routes.auth-center.stripPrefix default : false Switch off the stripping of the service-specific prefix from individual routes","title":"Routing : Authentication Service Configuration"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#routing-service-configuration","text":"id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1","title":"Routing : Service Configuration"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#configuration-file-example","text":"#API Gateway Port server.port=8086 # JWT Configuration apigateway.security.jwt.secret=otherpeopledontknowit apigateway.security.jwt.url=/api/authentication apigateway.security.jwt.expiration=86400 # Rooting Configuration : Authentication Service zuul.routes.auth-center.path=/api/authentication/** zuul.routes.auth-center.url=http://127.0.0.1:8081/ zuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie zuul.routes.auth-center.stripPrefix=false # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2","title":"Configuration file example"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#control-access-api","text":"The Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including: * \u201cROLE_ADMIN\u201d * \u201cROLE_PROJECT_MANAGER\u201d * \u201cROLE_USER\u201d By the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d. # Filtering private restApi scava.routes.config.adminAccessApi[0]=/api/users scava.routes.config.adminAccessApi[1]=/api/user/** scava.routes.config.projectManagerAccessApi[0]=/administration/projects/create scava.routes.config.projectManagerAccessApi[1]=/administration/projects/import scava.routes.config.projectManagerAccessApi[2]=/administration/analysis/** scava.routes.config.userAccessApi[0]=/administration/projects scava.routes.config.userAccessApi[1]=/administration/projects/p/** scava.routes.config.userAccessApi[2]=/api/users/** scava.routes.config.userAccessApi[3]=/api/account","title":"Control access API"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#packaging-form-sources","text":"Maven Packaging mvn -Pprod install","title":"Packaging Form Sources"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-execution","text":"complete an put the \"application.properties\" configuration file in the execute directory. Execute the crossmeter-api-gateway-1.0.0.jar Jar. java -jar scava-api-gateway-1.0.0.jar","title":"API Gateway Execution"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#client-implementation","text":"How to consume a Scava REST services ? \\ This guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.","title":"Client Implementation"},{"location":"contributors-guide/architecture-guidelignes/authentication/","text":"Authentication Component The Scava Authentication service: Provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform. Provides user management services, including user registration process, user profile editing and roles based authorization management. Authentication API The Authentication server is a component of The Scava platform which manages the authentication for all services accessible behind the API Gateway. Authenticate User POST /api/authentication Login as a registered user. ### JSON Web Tokens (JWT) JSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are: * Header * Payload * Signature This solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/). ### JWT Authentication Implementation * Users have to login to the authentication service API using their credentials username and password. curl -i -X POST -H \"Content-Type:application/json\" http://localhost:8086/api/authentication -d '{\"username\":\"admin\", \"password\": \"admin\"}' * Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header. * The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway. curl -i -X GET -H \"Content-Type:application/json\" -H \"Authorization:Bearer eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1dGhvcml0aWVzIjpbIlJPTEVfQURNSU4iLCJST0xFX1BST0pFQ1RfTUFOQUdFUiIsIlJPTEVfVVNFUiJdLCJpYXQiOjE1MzE4OTk3NDMsImV4cCI6MTUzMTk4NjE0M30.l-iCJcnae-1mlhMb3_y09HM4HZYFaHxe_JctWi2FRUY\" http://localhost:8086/api/users ## User Management API The Authentication component provides web services for CRUD user account. Register User POST /api/register Register new user. Activate User GET /api/activate Activate the registered user. Update User PUT /api/users Update an existing user. Retrieve Users GET /api/users Get all registered users. Retrieve Login User GET /api/users/{login} Get the \"login\" user. Delete User DELETE /api/users/{login} Delete the \"login\" user. Authentication Server Configuration The Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component. Server Configuration id : server.port default : 8085 Port of the Authentication API server. Each REST request sent to the gateway must be adressed to this port. JWT Security Configuration id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. Default ADMIN configuration Property Description Default Value scava.administration.username The administrator username admin scava.administration.password The administrator password admin scava.administration.admin-role The admin role ADMIN scava.administration.project-manager-role The project manager role PROJECT_MANAGER scava.administration.project-user-role The user role USER Mongodb Database Configuration Property Description Default Value spring.data.mongodb.uri Url of the MongoDB database server mongodb://localhost:27017 spring.data.mongodb.database Name of the MongoDB database scava Mail Server configuration In order to register new users, you have to configure a mail server. Property Description Default Value spring.mail.host Url of the mail service smtp.gmail.com spring.mail.port Port of the mail service 587 spring.mail.username Login of the mail account spring.mail.password Password of the mail account spring.mail.protocol mail protocole smtp spring.mail.tls - true spring.mail.properties.mail.smtp.auth - true spring.mail.properties.mail.smtp.starttls.enable - true spring.mail.properties.mail.smtp.ssl.trust= - smtp.gmail.com Administration Dashboard Setting id : scava.administration.base-url default : http://localhost:4200 The SCAVA administration base URL to generate the activation account URL. Packaging From Sources Maven Packaging mvn -Pprod install Authentication Server Execution complete an put the \"application.properties\" configuration file in the execution directory. Execute the scava-auth-service-1.0.0.jar Jar. java -jar scava-auth-service-1.0.0.jar","title":"Authentication Component"},{"location":"contributors-guide/architecture-guidelignes/authentication/#authentication-component","text":"The Scava Authentication service: Provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform. Provides user management services, including user registration process, user profile editing and roles based authorization management.","title":"Authentication Component"},{"location":"contributors-guide/architecture-guidelignes/authentication/#authentication-api","text":"The Authentication server is a component of The Scava platform which manages the authentication for all services accessible behind the API Gateway. Authenticate User POST /api/authentication Login as a registered user. ### JSON Web Tokens (JWT) JSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are: * Header * Payload * Signature This solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/). ### JWT Authentication Implementation * Users have to login to the authentication service API using their credentials username and password. curl -i -X POST -H \"Content-Type:application/json\" http://localhost:8086/api/authentication -d '{\"username\":\"admin\", \"password\": \"admin\"}' * Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header. * The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway. curl -i -X GET -H \"Content-Type:application/json\" -H \"Authorization:Bearer eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1dGhvcml0aWVzIjpbIlJPTEVfQURNSU4iLCJST0xFX1BST0pFQ1RfTUFOQUdFUiIsIlJPTEVfVVNFUiJdLCJpYXQiOjE1MzE4OTk3NDMsImV4cCI6MTUzMTk4NjE0M30.l-iCJcnae-1mlhMb3_y09HM4HZYFaHxe_JctWi2FRUY\" http://localhost:8086/api/users ## User Management API The Authentication component provides web services for CRUD user account. Register User POST /api/register Register new user. Activate User GET /api/activate Activate the registered user. Update User PUT /api/users Update an existing user. Retrieve Users GET /api/users Get all registered users. Retrieve Login User GET /api/users/{login} Get the \"login\" user. Delete User DELETE /api/users/{login} Delete the \"login\" user.","title":"Authentication API"},{"location":"contributors-guide/architecture-guidelignes/authentication/#authentication-server-configuration","text":"The Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component.","title":"Authentication Server Configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#server-configuration","text":"id : server.port default : 8085 Port of the Authentication API server. Each REST request sent to the gateway must be adressed to this port.","title":"Server Configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#jwt-security-configuration","text":"id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA.","title":"JWT Security Configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#default-admin-configuration","text":"Property Description Default Value scava.administration.username The administrator username admin scava.administration.password The administrator password admin scava.administration.admin-role The admin role ADMIN scava.administration.project-manager-role The project manager role PROJECT_MANAGER scava.administration.project-user-role The user role USER","title":"Default ADMIN configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#mongodb-database-configuration","text":"Property Description Default Value spring.data.mongodb.uri Url of the MongoDB database server mongodb://localhost:27017 spring.data.mongodb.database Name of the MongoDB database scava","title":"Mongodb Database Configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#mail-server-configuration","text":"In order to register new users, you have to configure a mail server. Property Description Default Value spring.mail.host Url of the mail service smtp.gmail.com spring.mail.port Port of the mail service 587 spring.mail.username Login of the mail account spring.mail.password Password of the mail account spring.mail.protocol mail protocole smtp spring.mail.tls - true spring.mail.properties.mail.smtp.auth - true spring.mail.properties.mail.smtp.starttls.enable - true spring.mail.properties.mail.smtp.ssl.trust= - smtp.gmail.com","title":"Mail Server configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#administration-dashboard-setting","text":"id : scava.administration.base-url default : http://localhost:4200 The SCAVA administration base URL to generate the activation account URL.","title":"Administration Dashboard Setting"},{"location":"contributors-guide/architecture-guidelignes/authentication/#packaging-from-sources","text":"Maven Packaging mvn -Pprod install","title":"Packaging From Sources"},{"location":"contributors-guide/architecture-guidelignes/authentication/#authentication-server-execution","text":"complete an put the \"application.properties\" configuration file in the execution directory. Execute the scava-auth-service-1.0.0.jar Jar. java -jar scava-auth-service-1.0.0.jar","title":"Authentication Server Execution"},{"location":"contributors-guide/contributors-guidelignes/","text":"Contributors Guidelines Guidelines related to the organisation ofthe SCAVA developement process. SCAVA Repository Organisation SCAVA Development Process SCAVA Testing recommendations SCAVA Naming recommendations SCAVA Licensing recommendations","title":"Contributors Guidelines"},{"location":"contributors-guide/contributors-guidelignes/#contributors-guidelines","text":"Guidelines related to the organisation ofthe SCAVA developement process. SCAVA Repository Organisation SCAVA Development Process SCAVA Testing recommendations SCAVA Naming recommendations SCAVA Licensing recommendations","title":"Contributors Guidelines"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/","text":"SCAVA Development Process Introduction This document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development. Process overview The development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system. It is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests. The different elements shown below are individually described in the remaining of the section. Figure 1. Overview of the CROSSMINER development process and tools. Source Code Repository Different branches will be created in the repository. In particular, the master branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The dev branch contains the most recent code integrated by partners before release. Other branches will be created for the different features/components, which will be eventually merged in the dev branch. The name of each branch contains the id of the issue it is supposed to implement. All source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository. Code: https://github.com/crossminer/scava Product documentation: https://github.com/crossminer/scava-docs Product deployment: https://github.com/crossminer/scava-deployment Since all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details). By taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3. Figure 2: Branching model Tests Each branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below. Figure 3: Explanatory master and branches BRANCH: Set of branches TEST: Set of tests INTGRATION_TEST < TEST UNIT_TEST < TEST function test: BRANCH -> TEST test(master) -> test(cool-feature) \u22c3 {itu, \u2026 itz} Continuous Integration Server Continuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus reduce cost and time . A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all unit tests . Integration tests are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities: Allow developers to write unit tests that can be executed automatically Perform automated tests against newly written code Show a list of tests that have passed and failed Quality analysis on source code Perform all the necessary actions to create a fully functioning build of the software when all tests have passed The currently used CI server is available at http://ci5.castalia.camp:8080/ Development and Production environments According to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform. Naming and change conventions The repository will contain the code of the high-level components shown in Table 1 Components Corresponding folder in repository Leader DevOps Dashboard BIT Workflow Diagram Editor /crossflow YORK Administration Web Application /administration SFT IDE /eclipse-based-ide FEA API Gateway /api-gateway SFT DevOps Backend BIT Knowledge Base /knowledge-base UDA Project Analyser /metric-platform SFT Data Collector /metric-platform SFT Project Scheduler /metric-platform SFT Metric Providers /metric-platform YORK, CWI, AUEB, EHU Data Storage /metric-platform SFT Web-based dashboards /web-dashboards BIT Table 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes Figure 4: Scava components architecture For each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus: when developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons; when a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR. The partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder: readme.md : this \ufb01le will contain the entry-point to all documentation for the component. Jenkinsfile : in order to compile the component, run tests, etc on the CI server. test : this is a folder containing the unit and/or integration tests. As a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base The following items are typical practices when applying CI: * Maintain a code repository * Automate the build * Make the build self-testing * Everyone commits to the baseline every day * Every commit (to baseline) should be built * Keep the build fast * Test in a clone of the production environment * Make it easy to get the latest deliverables * Everyone can see the results of the latest build * Automate deployment Communication and collaboration means The development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of: Slack channel: http://crossminer.slack.com Eclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev GitHub repository for documentation: https://github.com/crossminer/scava-docs/","title":"SCAVA Development Process"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#scava-development-process","text":"","title":"SCAVA Development Process"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#introduction","text":"This document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development.","title":"Introduction"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#process-overview","text":"The development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system. It is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests. The different elements shown below are individually described in the remaining of the section. Figure 1. Overview of the CROSSMINER development process and tools.","title":"Process overview"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#source-code-repository","text":"Different branches will be created in the repository. In particular, the master branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The dev branch contains the most recent code integrated by partners before release. Other branches will be created for the different features/components, which will be eventually merged in the dev branch. The name of each branch contains the id of the issue it is supposed to implement. All source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository. Code: https://github.com/crossminer/scava Product documentation: https://github.com/crossminer/scava-docs Product deployment: https://github.com/crossminer/scava-deployment Since all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details). By taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3. Figure 2: Branching model","title":"Source Code Repository"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#tests","text":"Each branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below. Figure 3: Explanatory master and branches BRANCH: Set of branches TEST: Set of tests INTGRATION_TEST < TEST UNIT_TEST < TEST function test: BRANCH -> TEST test(master) -> test(cool-feature) \u22c3 {itu, \u2026 itz}","title":"Tests"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#continuous-integration-server","text":"Continuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus reduce cost and time . A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all unit tests . Integration tests are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities: Allow developers to write unit tests that can be executed automatically Perform automated tests against newly written code Show a list of tests that have passed and failed Quality analysis on source code Perform all the necessary actions to create a fully functioning build of the software when all tests have passed The currently used CI server is available at http://ci5.castalia.camp:8080/","title":"Continuous Integration Server"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#development-and-production-environments","text":"According to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform.","title":"Development and Production environments"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#naming-and-change-conventions","text":"The repository will contain the code of the high-level components shown in Table 1 Components Corresponding folder in repository Leader DevOps Dashboard BIT Workflow Diagram Editor /crossflow YORK Administration Web Application /administration SFT IDE /eclipse-based-ide FEA API Gateway /api-gateway SFT DevOps Backend BIT Knowledge Base /knowledge-base UDA Project Analyser /metric-platform SFT Data Collector /metric-platform SFT Project Scheduler /metric-platform SFT Metric Providers /metric-platform YORK, CWI, AUEB, EHU Data Storage /metric-platform SFT Web-based dashboards /web-dashboards BIT Table 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes Figure 4: Scava components architecture For each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus: when developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons; when a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR. The partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder: readme.md : this \ufb01le will contain the entry-point to all documentation for the component. Jenkinsfile : in order to compile the component, run tests, etc on the CI server. test : this is a folder containing the unit and/or integration tests. As a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base The following items are typical practices when applying CI: * Maintain a code repository * Automate the build * Make the build self-testing * Everyone commits to the baseline every day * Every commit (to baseline) should be built * Keep the build fast * Test in a clone of the production environment * Make it easy to get the latest deliverables * Everyone can see the results of the latest build * Automate deployment","title":"Naming and change conventions"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#communication-and-collaboration-means","text":"The development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of: Slack channel: http://crossminer.slack.com Eclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev GitHub repository for documentation: https://github.com/crossminer/scava-docs/","title":"Communication and collaboration means"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/","text":"SCAVA Licensing recommendations Content The Scava project is licensed under Eclipse Public License - v 2.0 license. As consequence of our status of project hosted by the eclipse foundation, all Scava components must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header. \"Eclipse Public License\" licensing file The text below must be integrated to the root folder of your project on a text file name \"LICENSE\". Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership. Source File header All sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...). Optionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization. Example of Java license header file /******************************************************************************* * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"} * This program and the accompanying materials are made * available under the terms of the Eclipse Public License 2.0 * which is available at https://www.eclipse.org/legal/epl-2.0/ * * SPDX-License-Identifier: EPL-2.0 ******************************************************************************/ Comment n Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files. This plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.","title":"SCAVA Licensing recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#scava-licensing-recommendations","text":"","title":"SCAVA Licensing recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#content","text":"The Scava project is licensed under Eclipse Public License - v 2.0 license. As consequence of our status of project hosted by the eclipse foundation, all Scava components must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header.","title":"Content"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#eclipse-public-license-licensing-file","text":"The text below must be integrated to the root folder of your project on a text file name \"LICENSE\". Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership.","title":"\"Eclipse Public License\" licensing file"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#source-file-header","text":"All sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...). Optionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization. Example of Java license header file /******************************************************************************* * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"} * This program and the accompanying materials are made * available under the terms of the Eclipse Public License 2.0 * which is available at https://www.eclipse.org/legal/epl-2.0/ * * SPDX-License-Identifier: EPL-2.0 ******************************************************************************/","title":"Source File header"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#comment","text":"n Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files. This plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.","title":"Comment"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/","text":"SCAVA Naming recommendations Component Naming As consequence of our status of project hosted by the eclipse foundation, we have to follow a specific naming schema for all components implemented in context of SCAVA project. In this section, \"component\" means big funcional componen of the SCAVA project. Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration Project Naming For Eclipse Plugin org.eclipse.scava.{componentname} For Maven Project : the name of the project if the ArtifactId If your component is composed of one project : {component-name} If your component is composed of several sub projects : {sub-component-name} ``` ## Source Code Namsespace All sources must be nemspaces by : org.eclipse.scava.{componentname} ## Maven Ids For the Maven projects: * If your component is composed of one project : Group Id : org.eclipse.scava ArtifactId : {component-name} * If your component is composed of several sub projects : Group Id : org.eclipse.scava.{component-name} ArtifactId : {sub-component-name} ``` REST services Naming The REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of platform services , we need to used a common naming schema for all REST services provided by the platform. How to name a REST service ? /{ componentid }/{ categoryname }/{ servicename } / componentid / : Name of the Architectural component which provide the service / categoryname / (Optional) : optional category of the service / servicename / : Name of the rest service Component Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration","title":"SCAVA Naming recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#scava-naming-recommendations","text":"","title":"SCAVA Naming recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#component-naming","text":"As consequence of our status of project hosted by the eclipse foundation, we have to follow a specific naming schema for all components implemented in context of SCAVA project. In this section, \"component\" means big funcional componen of the SCAVA project. Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration","title":"Component Naming"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#project-naming","text":"For Eclipse Plugin org.eclipse.scava.{componentname} For Maven Project : the name of the project if the ArtifactId If your component is composed of one project : {component-name} If your component is composed of several sub projects : {sub-component-name} ``` ## Source Code Namsespace All sources must be nemspaces by : org.eclipse.scava.{componentname} ## Maven Ids For the Maven projects: * If your component is composed of one project : Group Id : org.eclipse.scava ArtifactId : {component-name} * If your component is composed of several sub projects : Group Id : org.eclipse.scava.{component-name} ArtifactId : {sub-component-name} ```","title":"Project Naming"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#rest-services-naming","text":"The REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of platform services , we need to used a common naming schema for all REST services provided by the platform.","title":"REST services Naming"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#how-to-name-a-rest-service","text":"/{ componentid }/{ categoryname }/{ servicename } / componentid / : Name of the Architectural component which provide the service / categoryname / (Optional) : optional category of the service / servicename / : Name of the rest service","title":"How to name a REST service ?"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#component","text":"Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration","title":"Component"},{"location":"contributors-guide/contributors-guidelignes/scava-repository-organisation/","text":"SCAVA Repository Organisation The SCAVA code repository is organized by functional components with one package for each of this components. General organisation metric-platform platform : Core projects of the metric platform platform-extensions : Extensions of the metric platform metric-providers : Metric Providers implementations projects factoids : Factoids implementations projects tests : Test projects related to the metric-platform web-dashboards : DevOps Dashboard and DevOpsDashboard components knowledge-base : Knowledge Base implementation Workflow Execution Engine : Workflow Execution Engine implementation eclipse-based-ide : SCAVA Eclipse plugin api-gateway : API Gateway implementation. administration : Administration component implementation mingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project.","title":"SCAVA Repository Organisation"},{"location":"contributors-guide/contributors-guidelignes/scava-repository-organisation/#scava-repository-organisation","text":"The SCAVA code repository is organized by functional components with one package for each of this components.","title":"SCAVA Repository Organisation"},{"location":"contributors-guide/contributors-guidelignes/scava-repository-organisation/#general-organisation","text":"metric-platform platform : Core projects of the metric platform platform-extensions : Extensions of the metric platform metric-providers : Metric Providers implementations projects factoids : Factoids implementations projects tests : Test projects related to the metric-platform web-dashboards : DevOps Dashboard and DevOpsDashboard components knowledge-base : Knowledge Base implementation Workflow Execution Engine : Workflow Execution Engine implementation eclipse-based-ide : SCAVA Eclipse plugin api-gateway : API Gateway implementation. administration : Administration component implementation mingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project.","title":"General organisation"},{"location":"contributors-guide/contributors-guidelignes/scava-testing-organisation/","text":"SCAVA Testing recommendations Knowledge Base This builds needs some configuration to run successfully. In the application.properties files: * /knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties * /knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties Edit the following parameters: lucene.index.folder=/tmp/scava_lucene/ egit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57 To generate the GitHub access token you need to go to your own GitHub account and create a new one. Once it's done simply restart the tests, they should pass.","title":"SCAVA Testing recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-testing-organisation/#scava-testing-recommendations","text":"","title":"SCAVA Testing recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-testing-organisation/#knowledge-base","text":"This builds needs some configuration to run successfully. In the application.properties files: * /knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties * /knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties Edit the following parameters: lucene.index.folder=/tmp/scava_lucene/ egit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57 To generate the GitHub access token you need to go to your own GitHub account and create a new one. Once it's done simply restart the tests, they should pass.","title":"Knowledge Base"},{"location":"contributors-guide/technical-guidelignes/","text":"Technical Guidelignes Tutorial and Guidelignes related to the various technologies used by the SCAVA Platform How to configure the API Gateway in order to integrate a new REST service How to consume a SCAVA REST services How to implement Restlet services How to extend the SCAVA data model How to generate REST API Documentation How to access to MongoDB using PONGO","title":"Technical Guidelignes"},{"location":"contributors-guide/technical-guidelignes/#technical-guidelignes","text":"Tutorial and Guidelignes related to the various technologies used by the SCAVA Platform How to configure the API Gateway in order to integrate a new REST service How to consume a SCAVA REST services How to implement Restlet services How to extend the SCAVA data model How to generate REST API Documentation How to access to MongoDB using PONGO","title":"Technical Guidelignes"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/","text":"How to access to MongoDB using PONGO When to use ? This guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas. Context The Scava platform use a MongoDb data base to store his data. We go through PONGO , a template based Java POJO generator to access MongoDB database. With Pongo we can define the data model which generates strongly-typed Java classes. In this guideligne, we will present : * The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform. * The access to MongoDb Document on from an external Java application. * How to preform basic CRUD operation with a PONGO Java data model. We consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model : Extend MongoDB Data Model . You want to access to MongoDB Document from an Eclipse Plugin ? 1. Add a dependency to the Java Data Model Edit the plugin.xml file of your plugin. In Dependency section, add a dependency to the plugin which contained the data model you went to access. To org.ossmeter.repository.model to access data related to project administration , metric execution process and authentification system. To org.ossmeter.repository.model.'project delta manager' to access configuration informations related to source codes managemeny tools. To specific metric provider plugins to access data related to a specific metric provider implementation contains his once data model. ... others plugin which contained the data model In Dependency section, add a dependency to com.googlecode.pongo.runtime plugin 2. Initiate a Connection to the MongoDb In context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base. In Dependency section of plugin.xml file , add a dependency to the org.ossmeter.platform plugin. You can now create a new connection to the database using the Configuration service. Mongo mongo = Configuration.getInstance().getMongoConnection(); You want to access to MongoDB Document on from an External Java Application ? 1. Add a dependency to the Java Data Model Add to your java project a dependency to the jar which contained the data model you went to access. You will have to deliver this jar with your application. Add o your java project a dependency to the pongo.jar jar file which can be download at this url : https://github.com/kolovos/pongo/releases 2. Initiate a Connection to MongoDb // Define ServerAddress of the MongoDb database List<ServerAddress> mongoHostAddresses = new ArrayList<>(); mongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1]))); // Create Connection Mongo mongo = new Mongo(mongoHostAddresses); Once the connection to MongoDb has been created, you have to make the link between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document. DB db = mongo.getDB(\"databasename\"); // Initiate the Project Java model ProjectRepository = new ProjectRepository(db); Basic CRUD with a PONGO Java data model 1. CREATE // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true); 2. READ // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to access object properties metricprovider.getname(); ...... 3. UPDATE // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true); 4. DELETE Mongo mongo = new Mongo(); mongo.dropDatabase(\"databasename\");","title":"How to access to MongoDB using PONGO"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#how-to-access-to-mongodb-using-pongo","text":"","title":"How to access to MongoDB using PONGO"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#when-to-use","text":"This guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas.","title":"When to use ?"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#context","text":"The Scava platform use a MongoDb data base to store his data. We go through PONGO , a template based Java POJO generator to access MongoDB database. With Pongo we can define the data model which generates strongly-typed Java classes. In this guideligne, we will present : * The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform. * The access to MongoDb Document on from an external Java application. * How to preform basic CRUD operation with a PONGO Java data model. We consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model : Extend MongoDB Data Model .","title":"Context"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#you-want-to-access-to-mongodb-document-from-an-eclipse-plugin","text":"","title":"You want to access to MongoDB Document from an Eclipse Plugin ?"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#1-add-a-dependency-to-the-java-data-model","text":"Edit the plugin.xml file of your plugin. In Dependency section, add a dependency to the plugin which contained the data model you went to access. To org.ossmeter.repository.model to access data related to project administration , metric execution process and authentification system. To org.ossmeter.repository.model.'project delta manager' to access configuration informations related to source codes managemeny tools. To specific metric provider plugins to access data related to a specific metric provider implementation contains his once data model. ... others plugin which contained the data model In Dependency section, add a dependency to com.googlecode.pongo.runtime plugin","title":"1. Add a dependency to the Java Data Model"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#2-initiate-a-connection-to-the-mongodb","text":"In context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base. In Dependency section of plugin.xml file , add a dependency to the org.ossmeter.platform plugin. You can now create a new connection to the database using the Configuration service. Mongo mongo = Configuration.getInstance().getMongoConnection();","title":"2. Initiate a Connection to the MongoDb"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#you-want-to-access-to-mongodb-document-on-from-an-external-java-application","text":"","title":"You want to access to MongoDB Document on from an External Java Application ?"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#1-add-a-dependency-to-the-java-data-model_1","text":"Add to your java project a dependency to the jar which contained the data model you went to access. You will have to deliver this jar with your application. Add o your java project a dependency to the pongo.jar jar file which can be download at this url : https://github.com/kolovos/pongo/releases","title":"1. Add a dependency to the Java Data Model"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#2-initiate-a-connection-to-mongodb","text":"// Define ServerAddress of the MongoDb database List<ServerAddress> mongoHostAddresses = new ArrayList<>(); mongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1]))); // Create Connection Mongo mongo = new Mongo(mongoHostAddresses); Once the connection to MongoDb has been created, you have to make the link between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document. DB db = mongo.getDB(\"databasename\"); // Initiate the Project Java model ProjectRepository = new ProjectRepository(db);","title":"2. Initiate a Connection to MongoDb"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#basic-crud-with-a-pongo-java-data-model","text":"","title":"Basic CRUD with a PONGO Java data model"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#1-create","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true);","title":"1. CREATE"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#2-read","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to access object properties metricprovider.getname(); ......","title":"2. READ"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#3-update","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true);","title":"3. UPDATE"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#4-delete","text":"Mongo mongo = new Mongo(); mongo.dropDatabase(\"databasename\");","title":"4. DELETE"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/","text":"How to configure the API Gateway in order to integrate a new REST service When to use this guideline ? This guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API. Context The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters. Routing : Service Configuration To reference a new remote REST API in the gateway, you have to add 2 new properties in the application.properties configuration file : the relative path of services which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process. Examples : * api gateway url = http://85.36.10.13:8080 * path = /administration/** * url = http://85.36.10.12:8082/administration The request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1 Configuration file example # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2","title":"How to configure the API Gateway in order to integrate a new REST service"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/#how-to-configure-the-api-gateway-in-order-to-integrate-a-new-rest-service","text":"","title":"How to configure the API Gateway in order to integrate a new REST service"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/#when-to-use-this-guideline","text":"This guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API.","title":"When to use this guideline ?"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/#context","text":"The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.","title":"Context"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/#routing-service-configuration","text":"To reference a new remote REST API in the gateway, you have to add 2 new properties in the application.properties configuration file : the relative path of services which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process. Examples : * api gateway url = http://85.36.10.13:8080 * path = /administration/** * url = http://85.36.10.12:8082/administration The request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1","title":"Routing : Service Configuration"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/#configuration-file-example","text":"# Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2","title":"Configuration file example"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/","text":"How to consume a SCAVA REST services When to use ? This guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications. REST API Reference The reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]]. API Gateway The Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway. All web service request form clients have to go through the gateway. The api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform. Platform Authentication The CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io). 1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. 1. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Authentication in Java Retrieve a Web Tokens from authentication service private String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException { // Authentication Service URI URL url = new URL(\"http://localhost:8086/api/authentication\"); // AUthentication Request HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setDoOutput(true); connection.setRequestMethod(\"POST\"); connection.setRequestProperty(\"Content-Type\", \"application/json\"); String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\"; OutputStream os = connection.getOutputStream(); os.write(input.getBytes()); os.flush(); if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) { throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode()); } connection.disconnect(); // A JWT Token is return in the Header of the response return connection.getHeaderField(\"Authorization\"); } REST Service Call curl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication Service Consumption To consume a REST service provided by the integrated platform, the client must include the Token in the header of his request. ```java // Service URL URL url = new URL(\"http://localhost:8086/api/users\"); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setRequestMethod(\"GET\"); // Add Token to the request header connection.setRequestProperty(\"Authorization\",token);","title":"How to consume a SCAVA REST services"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#how-to-consume-a-scava-rest-services","text":"","title":"How to consume a SCAVA REST services"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#when-to-use","text":"This guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications.","title":"When to use ?"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#rest-api-reference","text":"The reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]].","title":"REST API Reference"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#api-gateway","text":"The Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway. All web service request form clients have to go through the gateway. The api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform.","title":"API Gateway"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#platform-authentication","text":"The CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io). 1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. 1. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Authentication in Java Retrieve a Web Tokens from authentication service private String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException { // Authentication Service URI URL url = new URL(\"http://localhost:8086/api/authentication\"); // AUthentication Request HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setDoOutput(true); connection.setRequestMethod(\"POST\"); connection.setRequestProperty(\"Content-Type\", \"application/json\"); String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\"; OutputStream os = connection.getOutputStream(); os.write(input.getBytes()); os.flush(); if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) { throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode()); } connection.disconnect(); // A JWT Token is return in the Header of the response return connection.getHeaderField(\"Authorization\"); } REST Service Call curl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication","title":"Platform Authentication"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#service-consumption","text":"To consume a REST service provided by the integrated platform, the client must include the Token in the header of his request. ```java // Service URL URL url = new URL(\"http://localhost:8086/api/users\"); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setRequestMethod(\"GET\"); // Add Token to the request header connection.setRequestProperty(\"Authorization\",token);","title":"Service Consumption"},{"location":"contributors-guide/technical-guidelignes/rest-api-doc-generation/","text":"How to generate REST API Documentation REST API Tutorial files Install First of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install. Execute the following steps. Clone repository: https://github.com/patrickneubauer/crossminer-workflow Import projects from the cloned repo to empty Eclipse workspace: Import root via Maven > Existing Maven Projects , then the rest via General > Existing projects into Workspace without checking Search for nested projects . Install new software packages: Install Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling [web page] Install Epsilon (Stable): http://download.eclipse.org/epsilon/updates/ [update site] Update Epsilon (Interim): http://download.eclipse.org/epsilon/interim/ [update site] Install Emfatic: http://download.eclipse.org/emfatic/update/ [update site] Note: I didn't see the the feature until Group items by category was unchecked. Install GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ [update site] Import the given json projects ( org.eclipse.epsilon.emc.json and org.eclipse.epsilon.emc.json.dt ) into the workspace via General > Existing projects into Workspace . Now right-click on org.eclipse.crossmeter.workflow project and select Maven > Update Project.... Copy the give M2M_Environment_oxygen.launch (for Windows 10) to org.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator and overwrite the old one. Refresh project org.eclipse.crossmeter.workflow.restmule.generator in Eclipse. Before running the .launch file, right-click on it, Run as > Run configurations.... Here go to Plug-ins tab and click on Add Required Plug-ins . Now you can run the .launch file TBA:How? In the Runtime Eclipse import the org.eclipse.crossmeter.workflow.restmule.generator project as Maven Project. Run generateFromOAS.launch as you can see in the videos https://youtu.be/BJXuozHJPeg. Now, you should see it generating the project. For further steps, watch Patrick's videos . github client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg github client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI github client generation example \u2014 https://youtu.be/ltSNnSZRETA kafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc kafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4 kafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA MDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk Example I've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following: /square/{number} . You can replace the {number} with any number. The repsonse for this request is a JSON object, which looks like this: { \"squared\": {squaredValue} } It contains only one field, named squared , which has the value of the square of the given number. From the specification the generator will provide us a Java Project which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines: ITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings). int number = 7; //The number to be squared. IData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server. NumberValue numberValue; //NumberValue is the class which represents the object received from the server numberValue = squared.observe().blockingSingle(); //Get the actually received object from the response. System.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer. Everything, including the IEntityApi interface and the NumberValue class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through Java to the objects/field inside of the received JSON object. And once again, they are all generated from the OpenAPI specification. So, to generate the mentioned Java Project , you have to put your OpenAPI specification file into the schemas folder inside the org.eclipse.crossmeter.workflow.restmule.generator project. I'll call mine as TestAPI.json , and it's made up of the following: { \"swagger\": \"2.0\", \"schemes\": [ \"http\" ], \"host\": \"localhost:8080\", \"basePath\": \"/\", \"info\": { \"description\": \"This is a test API, only for demonstration of the generator.\", \"termsOfService\": \"\", \"title\": \"TestAPI\", \"version\": \"v1\" }, \"consumes\": [ \"application/json\" ], \"produces\": [ \"application/json\" ], \"securityDefinitions\": { }, \"paths\": { \"/square/{number}\": { \"get\": { \"description\": \"Square a number.\", \"parameters\": [ { \"description\": \"The number to be squared\", \"in\": \"path\", \"name\": \"number\", \"required\": true, \"type\": \"integer\" } ], \"responses\": { \"200\": { \"description\": \"OK\", \"schema\": { \"$ref\": \"#/definitions/NumberValue\" } } } } } }, \"definitions\": { \"NumberValue\": { \"properties\": { \"squared\": { \"type\": \"integer\" } }, \"type\": \"object\" } } } You can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own. Then we have to set the generator to use our new schema. To do this, open the build.xml in the generator project and modify the api and json.model.file property to testapi and schemas/TestAPI.json , respectively. <!--API Variables --> <property name=\"api\" value=\"testapi\" /> <property name=\"json.model.file\" value=\"schemas/TestAPI.json\" /> We are almost done, but we have to take one more small step. We have to provide an .eol file in the epsilon/util/fix/ folder in the generator project. Its name must be the same as the value of the api property in the previous step, so for this example we use the testapi.eol . The content of this file: import \"../restmule.eol\"; var api = RestMule!API.all.first(); // RATE LIMITS var search = new RestMule!RatePolicyScope; search.scope = \"Search\"; var entity = new RestMule!RatePolicyScope; entity.scope = \"Entity\"; // RATE POLICY var policy = new RestMule!RatePolicy; policy.scopes.add(entity); policy.scopes.add(search); var reset = new RestMule!ResponseHeader; var resetInt = new RestMule!TInteger; resetInt.label = \"X-RateLimit-Reset\"; reset.type = resetInt; policy.reset = reset; var limit = new RestMule!ResponseHeader; var limitInt = new RestMule!TInteger; limitInt.label = \"X-RateLimit-Limit\"; limit.type = limitInt; policy.limit = limit; var remaining = new RestMule!ResponseHeader; var remainingInt = new RestMule!TInteger; remainingInt.label = \"X-RateLimit-Remaining\"; remaining.type = remainingInt; policy.remaining = remaining; api.ratePolicy = policy; // PAGINATION var pagination= new RestMule!PaginationPolicy; pagination.start = 1; pagination.max = 10; pagination.increment = 1; pagination.maxPerIteration = 100; var perIteration = new RestMule!Query; perIteration.description = \"Items per page\"; perIteration.required = false; var type = new RestMule!TInteger; type.label = \"per_page\"; type.name = type.label; perIteration.type = type; pagination.perIteration = perIteration; var page = new RestMule!Query; page.description = \"Page identifier\"; page.required = false; var type1 = new RestMule!TInteger; type1.label = \"page\"; type1.name = type1.label; page.type = type1; pagination.page = page; var link = new RestMule!ResponseHeader; link.description = \"Page links\"; var format = new RestMule!TFormattedString; format.label = \"Link\"; format.name = format.label; link.type = format; pagination.links = link; api.pagination = pagination; // WRAPPER var wrapper = new RestMule!Wrapper; wrapper.name = \"Wrapper\"; var items = new RestMule!ListType; items.label = \"items\"; wrapper.items = items; wrapper.totalLabel= \"total_count\"; wrapper.incompleteLabel = \"incomplete_results\"; api.pageWrapper = wrapper; // ADD RATE & WRAPPER TO REQUESTS (FIXME) for (r in RestMule!Request.all){ if (r.parent.path.startsWith(\"/search\")){ r.scope = search; } else { r.scope = entity; } r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader))); for (resp in r.responses.select(s|s.responseType <> null)){ resp.unwrap(); } } /* ////////// * OPERATIONS *////////// operation RestMule!ObjectType hasWrapper() : Boolean { var wrapper = RestMule!Wrapper.all.first; var lists = self.listFields.collect(a|a.label); return (not lists.isEmpty()) and lists .includes(wrapper.items.label); } operation RestMule!Response unwrap() : RestMule!ObjectType{ if (self.responseType.instanceOf(ObjectType)){ if (self.responseType.hasWrapper()){ (\"Unwrapping : \"+ self.responseType.name).println; var wrapper = RestMule!Wrapper.all.first; var name = self.responseType.name.println; self.responseType = self.responseType.println.listFields .select(b| b.label == wrapper.items.label).first.elements.first; self.responseType.name = name; self.pageWrapped = true; self.responseType.description = \"UNWRAPPED: \" + self.responseType.description; } } } After this, we can run the generator and it will generate our Java Project from the specification. In the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.","title":"How to generate REST API Documentation"},{"location":"contributors-guide/technical-guidelignes/rest-api-doc-generation/#how-to-generate-rest-api-documentation","text":"REST API Tutorial files","title":"How to generate REST API Documentation"},{"location":"contributors-guide/technical-guidelignes/rest-api-doc-generation/#install","text":"First of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install. Execute the following steps. Clone repository: https://github.com/patrickneubauer/crossminer-workflow Import projects from the cloned repo to empty Eclipse workspace: Import root via Maven > Existing Maven Projects , then the rest via General > Existing projects into Workspace without checking Search for nested projects . Install new software packages: Install Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling [web page] Install Epsilon (Stable): http://download.eclipse.org/epsilon/updates/ [update site] Update Epsilon (Interim): http://download.eclipse.org/epsilon/interim/ [update site] Install Emfatic: http://download.eclipse.org/emfatic/update/ [update site] Note: I didn't see the the feature until Group items by category was unchecked. Install GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ [update site] Import the given json projects ( org.eclipse.epsilon.emc.json and org.eclipse.epsilon.emc.json.dt ) into the workspace via General > Existing projects into Workspace . Now right-click on org.eclipse.crossmeter.workflow project and select Maven > Update Project.... Copy the give M2M_Environment_oxygen.launch (for Windows 10) to org.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator and overwrite the old one. Refresh project org.eclipse.crossmeter.workflow.restmule.generator in Eclipse. Before running the .launch file, right-click on it, Run as > Run configurations.... Here go to Plug-ins tab and click on Add Required Plug-ins . Now you can run the .launch file TBA:How? In the Runtime Eclipse import the org.eclipse.crossmeter.workflow.restmule.generator project as Maven Project. Run generateFromOAS.launch as you can see in the videos https://youtu.be/BJXuozHJPeg. Now, you should see it generating the project. For further steps, watch Patrick's videos . github client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg github client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI github client generation example \u2014 https://youtu.be/ltSNnSZRETA kafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc kafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4 kafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA MDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk","title":"Install"},{"location":"contributors-guide/technical-guidelignes/rest-api-doc-generation/#example","text":"I've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following: /square/{number} . You can replace the {number} with any number. The repsonse for this request is a JSON object, which looks like this: { \"squared\": {squaredValue} } It contains only one field, named squared , which has the value of the square of the given number. From the specification the generator will provide us a Java Project which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines: ITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings). int number = 7; //The number to be squared. IData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server. NumberValue numberValue; //NumberValue is the class which represents the object received from the server numberValue = squared.observe().blockingSingle(); //Get the actually received object from the response. System.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer. Everything, including the IEntityApi interface and the NumberValue class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through Java to the objects/field inside of the received JSON object. And once again, they are all generated from the OpenAPI specification. So, to generate the mentioned Java Project , you have to put your OpenAPI specification file into the schemas folder inside the org.eclipse.crossmeter.workflow.restmule.generator project. I'll call mine as TestAPI.json , and it's made up of the following: { \"swagger\": \"2.0\", \"schemes\": [ \"http\" ], \"host\": \"localhost:8080\", \"basePath\": \"/\", \"info\": { \"description\": \"This is a test API, only for demonstration of the generator.\", \"termsOfService\": \"\", \"title\": \"TestAPI\", \"version\": \"v1\" }, \"consumes\": [ \"application/json\" ], \"produces\": [ \"application/json\" ], \"securityDefinitions\": { }, \"paths\": { \"/square/{number}\": { \"get\": { \"description\": \"Square a number.\", \"parameters\": [ { \"description\": \"The number to be squared\", \"in\": \"path\", \"name\": \"number\", \"required\": true, \"type\": \"integer\" } ], \"responses\": { \"200\": { \"description\": \"OK\", \"schema\": { \"$ref\": \"#/definitions/NumberValue\" } } } } } }, \"definitions\": { \"NumberValue\": { \"properties\": { \"squared\": { \"type\": \"integer\" } }, \"type\": \"object\" } } } You can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own. Then we have to set the generator to use our new schema. To do this, open the build.xml in the generator project and modify the api and json.model.file property to testapi and schemas/TestAPI.json , respectively. <!--API Variables --> <property name=\"api\" value=\"testapi\" /> <property name=\"json.model.file\" value=\"schemas/TestAPI.json\" /> We are almost done, but we have to take one more small step. We have to provide an .eol file in the epsilon/util/fix/ folder in the generator project. Its name must be the same as the value of the api property in the previous step, so for this example we use the testapi.eol . The content of this file: import \"../restmule.eol\"; var api = RestMule!API.all.first(); // RATE LIMITS var search = new RestMule!RatePolicyScope; search.scope = \"Search\"; var entity = new RestMule!RatePolicyScope; entity.scope = \"Entity\"; // RATE POLICY var policy = new RestMule!RatePolicy; policy.scopes.add(entity); policy.scopes.add(search); var reset = new RestMule!ResponseHeader; var resetInt = new RestMule!TInteger; resetInt.label = \"X-RateLimit-Reset\"; reset.type = resetInt; policy.reset = reset; var limit = new RestMule!ResponseHeader; var limitInt = new RestMule!TInteger; limitInt.label = \"X-RateLimit-Limit\"; limit.type = limitInt; policy.limit = limit; var remaining = new RestMule!ResponseHeader; var remainingInt = new RestMule!TInteger; remainingInt.label = \"X-RateLimit-Remaining\"; remaining.type = remainingInt; policy.remaining = remaining; api.ratePolicy = policy; // PAGINATION var pagination= new RestMule!PaginationPolicy; pagination.start = 1; pagination.max = 10; pagination.increment = 1; pagination.maxPerIteration = 100; var perIteration = new RestMule!Query; perIteration.description = \"Items per page\"; perIteration.required = false; var type = new RestMule!TInteger; type.label = \"per_page\"; type.name = type.label; perIteration.type = type; pagination.perIteration = perIteration; var page = new RestMule!Query; page.description = \"Page identifier\"; page.required = false; var type1 = new RestMule!TInteger; type1.label = \"page\"; type1.name = type1.label; page.type = type1; pagination.page = page; var link = new RestMule!ResponseHeader; link.description = \"Page links\"; var format = new RestMule!TFormattedString; format.label = \"Link\"; format.name = format.label; link.type = format; pagination.links = link; api.pagination = pagination; // WRAPPER var wrapper = new RestMule!Wrapper; wrapper.name = \"Wrapper\"; var items = new RestMule!ListType; items.label = \"items\"; wrapper.items = items; wrapper.totalLabel= \"total_count\"; wrapper.incompleteLabel = \"incomplete_results\"; api.pageWrapper = wrapper; // ADD RATE & WRAPPER TO REQUESTS (FIXME) for (r in RestMule!Request.all){ if (r.parent.path.startsWith(\"/search\")){ r.scope = search; } else { r.scope = entity; } r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader))); for (resp in r.responses.select(s|s.responseType <> null)){ resp.unwrap(); } } /* ////////// * OPERATIONS *////////// operation RestMule!ObjectType hasWrapper() : Boolean { var wrapper = RestMule!Wrapper.all.first; var lists = self.listFields.collect(a|a.label); return (not lists.isEmpty()) and lists .includes(wrapper.items.label); } operation RestMule!Response unwrap() : RestMule!ObjectType{ if (self.responseType.instanceOf(ObjectType)){ if (self.responseType.hasWrapper()){ (\"Unwrapping : \"+ self.responseType.name).println; var wrapper = RestMule!Wrapper.all.first; var name = self.responseType.name.println; self.responseType = self.responseType.println.listFields .select(b| b.label == wrapper.items.label).first.elements.first; self.responseType.name = name; self.pageWrapped = true; self.responseType.description = \"UNWRAPPED: \" + self.responseType.description; } } } After this, we can run the generator and it will generate our Java Project from the specification. In the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.","title":"Example"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/","text":"How to implement Restlet services When to use this guideline ? This guideline present how to create a new REST service using the RESTLET framework in the Scava platform. Context Scava project manages REST services with the RESTLET framework. The usage of Restlet framework has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin : org.eclipse.crossmeter.platform.client.api. The RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in. You want to access to create a new REST Service ? 1. Create a new Route To register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service. Naming the Route The routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service : Naming-Scava-REST-Services.html Register the Route The org.scava.platform.services plug-in contained the class PlatformRoute.java responsible for declaring routes. package org.scava.platform.services; import org.restlet.Application; import org.restlet.Restlet; import org.restlet.routing.Router; public class PlatformRoute extends Application { @Override public Restlet createInboundRoot() { Router router = new Router(getContext()); router.attach(\"/\", PingResource.class); router.attach(\"/search\", SearchProjectResource.class); ... router.attach(\"/projects/p/{projectid}\", ProjectResource.class); router.attach(\"/raw/metrics\", RawMetricListResource.class); ... return router; } } Route Example : router.attach(\"/raw/metrics\", RawMetricListResource.class); \"/raw/metrics\" : Represent the route URL. \"RawMetricListResource.class\" : Represent the class where the service to be implemented for this path. A route can contained some parameters. In this case, parameters are identified by a name with curly brackets {} . 2. Implement the Service A service implementation is a Java class which extend the ServerResource class provided by the RESTLET framework. To create a new service create a new Class : * Named \" ServiceName \" + Resource. Ex : ProjectCreationResource.java * On a namespace based on the route. Ex : org.scava.platform.services.administration for platform administration services. * Who extend the org.restlet.resource.ServerResource class. GET Service To implement a service of type GET, create a new method : * Based on the following signature : public final Representation represent() * Add the @Get(\"json\") annotation @Get(\"json\") public final Representation represent() { // Initialise Response Header Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\"); if (responseHeaders == null) { responseHeaders = new Series(Header.class); getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders); } responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\")); responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\")); // Get Route parameter if required {projectid} String projectId = (String) getRequest().getAttributes().get(\"projectid\"); try { .... // Provide Result getResponse().setStatus(Status.SUCCESS_OK); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } You can also extend the AbstractApiResource , a service class provided by the platform and dedicate to services who request a connection to MongoDb database instead of the ServerResource . In this case you will have to implement the doRepresent() method. public class RawMetricListResource extends AbstractApiResource { public Representation doRepresent() { ObjectNode res = mapper.createObjectNode(); ArrayNode metrics = mapper.createArrayNode(); res.put(\"metrics\", metrics); ... return Util.createJsonRepresentation(res); } } POST Service To implement a service of type POST, crate a new method : * Based on the following signature : public Representation myServiceName (Representation entity) * Add the @Post annotation @Post public Representation myServiceName(Representation entity) { try { // Read Json Datas JsonNode json = mapper.readTree(entity.getText()); ... // Provide Result getResponse().setStatus(Status.SUCCESS_CREATED); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } DELETE Service To do .... 3. Document the Service The REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to allow an easy integration, this contract must be documented :","title":"How to implement Restlet services"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#how-to-implement-restlet-services","text":"","title":"How to implement Restlet services"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#when-to-use-this-guideline","text":"This guideline present how to create a new REST service using the RESTLET framework in the Scava platform.","title":"When to use this guideline ?"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#context","text":"Scava project manages REST services with the RESTLET framework. The usage of Restlet framework has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin : org.eclipse.crossmeter.platform.client.api. The RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in.","title":"Context"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#you-want-to-access-to-create-a-new-rest-service","text":"","title":"You want to access to create a new REST Service ?"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#1-create-a-new-route","text":"To register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service.","title":"1. Create a new Route"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#naming-the-route","text":"The routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service : Naming-Scava-REST-Services.html","title":"Naming the Route"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#register-the-route","text":"The org.scava.platform.services plug-in contained the class PlatformRoute.java responsible for declaring routes. package org.scava.platform.services; import org.restlet.Application; import org.restlet.Restlet; import org.restlet.routing.Router; public class PlatformRoute extends Application { @Override public Restlet createInboundRoot() { Router router = new Router(getContext()); router.attach(\"/\", PingResource.class); router.attach(\"/search\", SearchProjectResource.class); ... router.attach(\"/projects/p/{projectid}\", ProjectResource.class); router.attach(\"/raw/metrics\", RawMetricListResource.class); ... return router; } } Route Example : router.attach(\"/raw/metrics\", RawMetricListResource.class); \"/raw/metrics\" : Represent the route URL. \"RawMetricListResource.class\" : Represent the class where the service to be implemented for this path. A route can contained some parameters. In this case, parameters are identified by a name with curly brackets {} .","title":"Register the Route"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#2-implement-the-service","text":"A service implementation is a Java class which extend the ServerResource class provided by the RESTLET framework. To create a new service create a new Class : * Named \" ServiceName \" + Resource. Ex : ProjectCreationResource.java * On a namespace based on the route. Ex : org.scava.platform.services.administration for platform administration services. * Who extend the org.restlet.resource.ServerResource class.","title":"2. Implement the Service"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#get-service","text":"To implement a service of type GET, create a new method : * Based on the following signature : public final Representation represent() * Add the @Get(\"json\") annotation @Get(\"json\") public final Representation represent() { // Initialise Response Header Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\"); if (responseHeaders == null) { responseHeaders = new Series(Header.class); getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders); } responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\")); responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\")); // Get Route parameter if required {projectid} String projectId = (String) getRequest().getAttributes().get(\"projectid\"); try { .... // Provide Result getResponse().setStatus(Status.SUCCESS_OK); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } You can also extend the AbstractApiResource , a service class provided by the platform and dedicate to services who request a connection to MongoDb database instead of the ServerResource . In this case you will have to implement the doRepresent() method. public class RawMetricListResource extends AbstractApiResource { public Representation doRepresent() { ObjectNode res = mapper.createObjectNode(); ArrayNode metrics = mapper.createArrayNode(); res.put(\"metrics\", metrics); ... return Util.createJsonRepresentation(res); } }","title":"GET Service"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#post-service","text":"To implement a service of type POST, crate a new method : * Based on the following signature : public Representation myServiceName (Representation entity) * Add the @Post annotation @Post public Representation myServiceName(Representation entity) { try { // Read Json Datas JsonNode json = mapper.readTree(entity.getText()); ... // Provide Result getResponse().setStatus(Status.SUCCESS_CREATED); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } }","title":"POST Service"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#delete-service","text":"To do ....","title":"DELETE Service"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#3-document-the-service","text":"The REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to allow an easy integration, this contract must be documented :","title":"3. Document the Service"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/","text":"How to extend the SCAVA data model When to use ? In this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order the data layer of Scava platform during the evolution. Context The Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class. Each MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model. The current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins : The Platform data model (org.ossmeter.repository.model) : Contains data related to project administration , metric execution process and authentification system. Source Code Repositroy managers (org.ossmeter.repository.model.'project delta manager') : Contains configuration informations related to source codes managemeny tools. Metric Providers data models (in each metric provider plugins) : Each metric provider implementation contains his once data model. You need to Extend an Existing Data Model ? The first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same. 1. Locate the *.emf file of this data model A presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model. Ex : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf) 2. Update the Data Model description A data model description file contains a statical description of a MongoDb document. @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . attr String stackTrace; } ... If we would like to add one more attribute to the element ProjectError , we could add it this way : @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . . . attr String stackTrace; + attr String TestAttribute; } ... You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines 3. Generate the Java Class using the Pongo Tool. Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Replace the existing Java class by the new generated java class. More information about Pongo : https://github.com/kolovos/pongo/wiki You need to Create a new Data Model ? The second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model. In this case, we invite you to create a new plugin which will contain your data model. 1. Create a new Eclipse Plug-In Create a new Eclipse Plug-In Project ( In Eclipse Toolbar : File > New > Plug-In Project Name of the project : org.scava. mycomponent .repository.model Disable the generation of an Activator Class / contribution to the ui Edit the MANIFEST.MF file In Dependency : add a dependency to the org.eclipse.core.runtime plugin In Dependency : add a dependency to the com.googlecode.pongo.runtime plugin In Dependency : add a dependency to the org.apache.commons.lang3 plugin In Extentions : reference an extension point named com.googlecode.pongo.runtime.osgi In source directory Create a package named org.scava. mycomponent .repository.model In this package create an emf file named mycomponent.emf A presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file : ```package org.scava.mycomponent.repository.model; @db class MyComponent { .... } ``` You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines 2. Generate the Java Class using the Pongo Tool. Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Add this class in your org.scava. mycomponent .repository.model package More information about Pongo : https://github.com/kolovos/pongo/wiki","title":"How to extend the SCAVA data model"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#how-to-extend-the-scava-data-model","text":"","title":"How to extend the SCAVA data model"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#when-to-use","text":"In this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order the data layer of Scava platform during the evolution.","title":"When to use ?"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#context","text":"The Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class. Each MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model. The current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins : The Platform data model (org.ossmeter.repository.model) : Contains data related to project administration , metric execution process and authentification system. Source Code Repositroy managers (org.ossmeter.repository.model.'project delta manager') : Contains configuration informations related to source codes managemeny tools. Metric Providers data models (in each metric provider plugins) : Each metric provider implementation contains his once data model.","title":"Context"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#you-need-to-extend-an-existing-data-model","text":"The first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same.","title":"You need to Extend an Existing Data Model ?"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#1-locate-the-emf-file-of-this-data-model","text":"A presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model. Ex : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf)","title":"1. Locate the *.emf file of this data model"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#2-update-the-data-model-description","text":"A data model description file contains a statical description of a MongoDb document. @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . attr String stackTrace; } ... If we would like to add one more attribute to the element ProjectError , we could add it this way : @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . . . attr String stackTrace; + attr String TestAttribute; } ... You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines","title":"2. Update the Data  Model description"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#3-generate-the-java-class-using-the-pongo-tool","text":"Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Replace the existing Java class by the new generated java class. More information about Pongo : https://github.com/kolovos/pongo/wiki","title":"3. Generate the Java Class using the Pongo Tool."},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#you-need-to-create-a-new-data-model","text":"The second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model. In this case, we invite you to create a new plugin which will contain your data model.","title":"You need to Create a new Data Model ?"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#1-create-a-new-eclipse-plug-in","text":"Create a new Eclipse Plug-In Project ( In Eclipse Toolbar : File > New > Plug-In Project Name of the project : org.scava. mycomponent .repository.model Disable the generation of an Activator Class / contribution to the ui Edit the MANIFEST.MF file In Dependency : add a dependency to the org.eclipse.core.runtime plugin In Dependency : add a dependency to the com.googlecode.pongo.runtime plugin In Dependency : add a dependency to the org.apache.commons.lang3 plugin In Extentions : reference an extension point named com.googlecode.pongo.runtime.osgi In source directory Create a package named org.scava. mycomponent .repository.model In this package create an emf file named mycomponent.emf A presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file : ```package org.scava.mycomponent.repository.model; @db class MyComponent { .... } ``` You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines","title":"1. Create a new Eclipse Plug-In"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#2-generate-the-java-class-using-the-pongo-tool","text":"Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Add this class in your org.scava. mycomponent .repository.model package More information about Pongo : https://github.com/kolovos/pongo/wiki","title":"2. Generate the Java Class using the Pongo Tool."},{"location":"developers-guide/","text":"SCAVA Developers Guide The developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API. Running SCAVA Platform form Sources The following section provide informations related to how the main platform components can be run from sources in developers mode Analysis Platform Administration Application Visualisation Dashboard Eclipse Plugin Metric Provider Development Guide The following section provide the key informations required to develop a new metric provider and to integrate it on the platforme REST API Reference Guide Reference Guide of REST API exposed by the main platform components Access to the REST API via the API Gateway Analysis Platform Knowledge Base Visualisation Dashboard Workflow Engine","title":"SCAVA Developers Guide"},{"location":"developers-guide/#scava-developers-guide","text":"The developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API.","title":"SCAVA Developers Guide"},{"location":"developers-guide/#running-scava-platform-form-sources","text":"The following section provide informations related to how the main platform components can be run from sources in developers mode Analysis Platform Administration Application Visualisation Dashboard Eclipse Plugin","title":"Running SCAVA Platform form Sources"},{"location":"developers-guide/#metric-provider-development-guide","text":"The following section provide the key informations required to develop a new metric provider and to integrate it on the platforme","title":"Metric Provider Development Guide"},{"location":"developers-guide/#rest-api-reference-guide","text":"Reference Guide of REST API exposed by the main platform components Access to the REST API via the API Gateway Analysis Platform Knowledge Base Visualisation Dashboard Workflow Engine","title":"REST API Reference Guide"},{"location":"developers-guide/api-reference-guide/","text":"REST API Reference Guide Reference Guide of REST API exposed by the main platform components Access to the REST API via the API Gateway (Softeam) Analysis Platform (Softeam) Knowledge Base (UDA) Visualisation Dashboard (Bitergia) Workflow Engine (York)","title":"REST API Reference Guide"},{"location":"developers-guide/api-reference-guide/#rest-api-reference-guide","text":"Reference Guide of REST API exposed by the main platform components Access to the REST API via the API Gateway (Softeam) Analysis Platform (Softeam) Knowledge Base (UDA) Visualisation Dashboard (Bitergia) Workflow Engine (York)","title":"REST API Reference Guide"},{"location":"developers-guide/api-reference-guide/analysis-platform/","text":"Analysis Platform REST API reference Guide The MetricPlatform APIs documentation generated with swagger UI is available : SCAVA MetricPlatform APIs","title":"Analysis Platform REST API reference Guide"},{"location":"developers-guide/api-reference-guide/analysis-platform/#analysis-platform-rest-api-reference-guide","text":"The MetricPlatform APIs documentation generated with swagger UI is available : SCAVA MetricPlatform APIs","title":"Analysis Platform REST API reference Guide"},{"location":"developers-guide/api-reference-guide/api-gateway/","text":"Access to the REST API via the API Gateway","title":"Access to the REST API via the API Gateway"},{"location":"developers-guide/api-reference-guide/api-gateway/#access-to-the-rest-api-via-the-api-gateway","text":"","title":"Access to the REST API via the API Gateway"},{"location":"developers-guide/api-reference-guide/knowledge-base/","text":"Knowledge Base REST API reference Guide The MetricPlatform APIs documentation generated with swagger UI is available : SCAVA KB APIs","title":"Knowledge Base REST API reference Guide"},{"location":"developers-guide/api-reference-guide/knowledge-base/#knowledge-base-rest-api-reference-guide","text":"The MetricPlatform APIs documentation generated with swagger UI is available : SCAVA KB APIs","title":"Knowledge Base REST API reference Guide"},{"location":"developers-guide/api-reference-guide/visualisation-dashboard/","text":"Visualisation Platform REST API reference Guide","title":"Visualisation Platform REST API reference Guide"},{"location":"developers-guide/api-reference-guide/visualisation-dashboard/#visualisation-platform-rest-api-reference-guide","text":"","title":"Visualisation Platform REST API reference Guide"},{"location":"developers-guide/api-reference-guide/workflow-engine/","text":"Workflow Engine REST API reference Guide","title":"Workflow Engine REST API reference Guide"},{"location":"developers-guide/api-reference-guide/workflow-engine/#workflow-engine-rest-api-reference-guide","text":"","title":"Workflow Engine REST API reference Guide"},{"location":"developers-guide/metric-provider-developement-guide/","text":"Metric Provider Developement Guide In this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a transient metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a historical metric provider, will count the total number of commits over time. We'll go through the steps above for each metric provider. Pre-requisites Eclipse The Emfatic plug-in should be installed in your Eclipse The Pongo plug-in should be installed in your Eclipse The OSSMETER source code should be in your workspace The Transient Metric Provider This metric provider will store a complete history of the commits in the version control system(s) used by a project. 0. Setup Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list. 1. The data model We define the data model using the Emfatic language. In your newly created plug-in, create a package called org.ossmeter.metricprovider.trans.commits.model . In that package create an empty file called commits.emf . In this file, we will define our data model. First of all, we need to state the name of the package. package org.ossmeter.metricprovider.trans.commits.model; This is used by the Pongo code generator - the generated classes will be put in this package. We then define the database for our model: @db(qualifiedCollectionNames=\"true\") class Commits { val Repository[*] repositories; val Commit[*] commits; } The @db annotation tells Pongo that this will be the container database for the data. Adding the qualifiedCollectionNames=true property will prepend the database name to all Mongo collections. The Commits class above says that we want a database with two collections, name repositories and commits . If qualifiedCollectionNames is set to true , the collections will be named Commits.repositories and Commits.commits . We now define the schema of the Commits.repositories collection: ~~~java class Repository { @searchable attr String url; attr String repoType; attr String revision; attr int totalNumberOfCommits; ref CommitData[*] commits; } This class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection. The `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time. Each commit is represented in the `Commits.commits` collection by the following model: ~~~~java class Commit { @searchable attr Date date; attr String identifier; attr String message; attr String author; } For each commit, we store its date , identifier (revision ID), the commit message , and the author . We also create an index on the date to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated findByDate(String date) or findOneByDate(String date) methods). Now we need to use Pongo to generate code from this model. Right-click on the commits.emf file and select Pongo > Generate Pongos and plugin.xml . You should see some Java classes appear in your package. Now that we have our data model, we can implement the metric provider. 2. The metric provider Create a Java class called org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider This class should extend AbstractTransientMetricProvider and specify Commits for the generic argument: public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits> The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. adapt(DB db) This method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows: @Override public Commits adapt(DB db) { return new Commits(db); } The next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider: @Override public String getShortIdentifier() { // This may be deprecated very soon return \"transient-commits\"; } @Override public String getFriendlyName() { return \"Commit History\"; } @Override public String getSummaryInformation() { return \"The commit history of the project.\"; } The next method allows you to declare whether the metric provider is applicable to a given project: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } Our metric applies to any project that has at least one VCS repository. Finally, we have the measure(...) method that performs the actual metric calculation: @Override public void measure(Project project, ProjectDelta delta, Commits db) { for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) { Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl()); if (repo == null) { repo = new Repository(); repo.setUrl(repoDelta.getRepository().getUrl()); db.getRepositories().add(repo); } for (VcsCommit commit : repoDelta.getCommits()) { Commit c = new Commit(); c.setDate(commit.getJavaDate()); c.setMessage(commit.getMessage()); c.setAuthor(commit.getAuthor()); c.setIdentifier(commit.getRevision()); repo.getCommits().add(c); db.getCommits().add(c); } } db.getCommits().sync(); db.getRepositories().sync(); } The above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new Commit object is created for each commit in the delta. 3. Make the metric provider discoverable Metric providers are registered with the OSSMETER platform using extension points : Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsTransientMetricProvider' class Now everything is ready for the metric to be executed :) The Historic Metric Provider This metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value. 0. Setup Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list. 1. The data model In your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model: package org.ossmeter.metricprovider.historic.commits.model; class HistoricCommits { attr int numberOfCommits; } The data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed. 2. The metric provider Create a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider This class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)): public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. First of all, complete the typical information-related methods: @Override public String getShortIdentifier() { return \"historicalcommits\"; } @Override public String getFriendlyName() { return \"Historical commits\"; } @Override public String getSummaryInformation() { return \"...\"; } Now complete the standard appliesTo method: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } We now need to specify a dependency on the transient metric provider that we just implemented. @Override public List<String> getIdentifiersOfUses() { return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName()); } This tells the platform that we need access to the CommitsTransientMetricProvider database. The platform will assign this to the uses field that is available to the historical metric provider, as you'll see in the measure method: @Override public Pongo measure(Project project) { Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0)); int commits = (int) transDb.getCommits().size(); HistoricCommits hist = new HistoricCommits(); hist.setNumberOfCommits(commits); return hist; } First of all, we get hold of the database of the transient commits metric provider, and simply count the size of the commits collection. We save this in an instance of the HistoricCommits Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project. 3. Make the metric provider discoverable This process is the same as the transient metric provider: Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class Now everything is ready for both metrics to be executed :) But first. Let's specify how we want this historical metric to be visualised. 4. Define a MetVis visualisation specification MetVis is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis web page has numerous examples of this. Create a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider: { \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\", \"vis\" : [ { \"id\" : \"historicalcommits\", \"name\" : \"Commits over time\", \"description\" : \"This metric shows when the projects commits occurred\", \"type\" : \"LineChart\", \"datatable\" : { \"cols\" : [ { \"name\" : \"Date\", \"field\" : \"$__date\" }, { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" } ] }, \"x\" : \"Date\", \"y\" : \"Commits\" } ] } The metricId field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use $__date . The final two fields ( x and y ) are references to column names in the datatable specification. The type states that the data should be plotted as a line chart. You can test your MetVis specifications on the MetVis playpen . 5. Make the visualisation specification discoverable As with metric providers, visualisation specifications are registered using extension points. Add the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.visualisation.metric On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'historicalcommits.json' file Good job. Running the metric providers See Running from Source Homework Adapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.","title":"Metric Provider Developement Guide"},{"location":"developers-guide/metric-provider-developement-guide/#metric-provider-developement-guide","text":"In this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a transient metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a historical metric provider, will count the total number of commits over time. We'll go through the steps above for each metric provider.","title":"Metric Provider Developement Guide"},{"location":"developers-guide/metric-provider-developement-guide/#pre-requisites","text":"Eclipse The Emfatic plug-in should be installed in your Eclipse The Pongo plug-in should be installed in your Eclipse The OSSMETER source code should be in your workspace","title":"Pre-requisites"},{"location":"developers-guide/metric-provider-developement-guide/#the-transient-metric-provider","text":"This metric provider will store a complete history of the commits in the version control system(s) used by a project.","title":"The Transient Metric Provider"},{"location":"developers-guide/metric-provider-developement-guide/#0-setup","text":"Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.","title":"0. Setup"},{"location":"developers-guide/metric-provider-developement-guide/#1-the-data-model","text":"We define the data model using the Emfatic language. In your newly created plug-in, create a package called org.ossmeter.metricprovider.trans.commits.model . In that package create an empty file called commits.emf . In this file, we will define our data model. First of all, we need to state the name of the package. package org.ossmeter.metricprovider.trans.commits.model; This is used by the Pongo code generator - the generated classes will be put in this package. We then define the database for our model: @db(qualifiedCollectionNames=\"true\") class Commits { val Repository[*] repositories; val Commit[*] commits; } The @db annotation tells Pongo that this will be the container database for the data. Adding the qualifiedCollectionNames=true property will prepend the database name to all Mongo collections. The Commits class above says that we want a database with two collections, name repositories and commits . If qualifiedCollectionNames is set to true , the collections will be named Commits.repositories and Commits.commits . We now define the schema of the Commits.repositories collection: ~~~java class Repository { @searchable attr String url; attr String repoType; attr String revision; attr int totalNumberOfCommits; ref CommitData[*] commits; } This class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection. The `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time. Each commit is represented in the `Commits.commits` collection by the following model: ~~~~java class Commit { @searchable attr Date date; attr String identifier; attr String message; attr String author; } For each commit, we store its date , identifier (revision ID), the commit message , and the author . We also create an index on the date to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated findByDate(String date) or findOneByDate(String date) methods). Now we need to use Pongo to generate code from this model. Right-click on the commits.emf file and select Pongo > Generate Pongos and plugin.xml . You should see some Java classes appear in your package. Now that we have our data model, we can implement the metric provider.","title":"1. The data model"},{"location":"developers-guide/metric-provider-developement-guide/#2-the-metric-provider","text":"Create a Java class called org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider This class should extend AbstractTransientMetricProvider and specify Commits for the generic argument: public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits> The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn.","title":"2. The metric provider"},{"location":"developers-guide/metric-provider-developement-guide/#adaptdb-db","text":"This method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows: @Override public Commits adapt(DB db) { return new Commits(db); } The next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider: @Override public String getShortIdentifier() { // This may be deprecated very soon return \"transient-commits\"; } @Override public String getFriendlyName() { return \"Commit History\"; } @Override public String getSummaryInformation() { return \"The commit history of the project.\"; } The next method allows you to declare whether the metric provider is applicable to a given project: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } Our metric applies to any project that has at least one VCS repository. Finally, we have the measure(...) method that performs the actual metric calculation: @Override public void measure(Project project, ProjectDelta delta, Commits db) { for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) { Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl()); if (repo == null) { repo = new Repository(); repo.setUrl(repoDelta.getRepository().getUrl()); db.getRepositories().add(repo); } for (VcsCommit commit : repoDelta.getCommits()) { Commit c = new Commit(); c.setDate(commit.getJavaDate()); c.setMessage(commit.getMessage()); c.setAuthor(commit.getAuthor()); c.setIdentifier(commit.getRevision()); repo.getCommits().add(c); db.getCommits().add(c); } } db.getCommits().sync(); db.getRepositories().sync(); } The above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new Commit object is created for each commit in the delta.","title":"adapt(DB db)"},{"location":"developers-guide/metric-provider-developement-guide/#3-make-the-metric-provider-discoverable","text":"Metric providers are registered with the OSSMETER platform using extension points : Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsTransientMetricProvider' class Now everything is ready for the metric to be executed :)","title":"3. Make the metric provider discoverable"},{"location":"developers-guide/metric-provider-developement-guide/#the-historic-metric-provider","text":"This metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value.","title":"The Historic Metric Provider"},{"location":"developers-guide/metric-provider-developement-guide/#0-setup_1","text":"Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.","title":"0. Setup"},{"location":"developers-guide/metric-provider-developement-guide/#1-the-data-model_1","text":"In your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model: package org.ossmeter.metricprovider.historic.commits.model; class HistoricCommits { attr int numberOfCommits; } The data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed.","title":"1. The data model"},{"location":"developers-guide/metric-provider-developement-guide/#2-the-metric-provider_1","text":"Create a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider This class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)): public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. First of all, complete the typical information-related methods: @Override public String getShortIdentifier() { return \"historicalcommits\"; } @Override public String getFriendlyName() { return \"Historical commits\"; } @Override public String getSummaryInformation() { return \"...\"; } Now complete the standard appliesTo method: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } We now need to specify a dependency on the transient metric provider that we just implemented. @Override public List<String> getIdentifiersOfUses() { return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName()); } This tells the platform that we need access to the CommitsTransientMetricProvider database. The platform will assign this to the uses field that is available to the historical metric provider, as you'll see in the measure method: @Override public Pongo measure(Project project) { Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0)); int commits = (int) transDb.getCommits().size(); HistoricCommits hist = new HistoricCommits(); hist.setNumberOfCommits(commits); return hist; } First of all, we get hold of the database of the transient commits metric provider, and simply count the size of the commits collection. We save this in an instance of the HistoricCommits Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project.","title":"2. The metric provider"},{"location":"developers-guide/metric-provider-developement-guide/#3-make-the-metric-provider-discoverable_1","text":"This process is the same as the transient metric provider: Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class Now everything is ready for both metrics to be executed :) But first. Let's specify how we want this historical metric to be visualised.","title":"3. Make the metric provider discoverable"},{"location":"developers-guide/metric-provider-developement-guide/#4-define-a-metvis-visualisation-specification","text":"MetVis is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis web page has numerous examples of this. Create a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider: { \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\", \"vis\" : [ { \"id\" : \"historicalcommits\", \"name\" : \"Commits over time\", \"description\" : \"This metric shows when the projects commits occurred\", \"type\" : \"LineChart\", \"datatable\" : { \"cols\" : [ { \"name\" : \"Date\", \"field\" : \"$__date\" }, { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" } ] }, \"x\" : \"Date\", \"y\" : \"Commits\" } ] } The metricId field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use $__date . The final two fields ( x and y ) are references to column names in the datatable specification. The type states that the data should be plotted as a line chart. You can test your MetVis specifications on the MetVis playpen .","title":"4. Define a MetVis visualisation specification"},{"location":"developers-guide/metric-provider-developement-guide/#5-make-the-visualisation-specification-discoverable","text":"As with metric providers, visualisation specifications are registered using extension points. Add the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.visualisation.metric On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'historicalcommits.json' file Good job.","title":"5. Make the visualisation specification discoverable"},{"location":"developers-guide/metric-provider-developement-guide/#running-the-metric-providers","text":"See Running from Source","title":"Running the metric providers"},{"location":"developers-guide/metric-provider-developement-guide/#homework","text":"Adapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.","title":"Homework"},{"location":"developers-guide/runing-from-sources/","text":"Running SCAVA Platform form Sources The following section provide informations related to how the main platform components can be run from sources in developers mode Analysis Platform (Softeam) Administration Application (Softeam) Visualisation Dashboard (Bitergia) Eclipse Plugin (FrontendArt)","title":"Running SCAVA Platform form Sources"},{"location":"developers-guide/runing-from-sources/#running-scava-platform-form-sources","text":"The following section provide informations related to how the main platform components can be run from sources in developers mode Analysis Platform (Softeam) Administration Application (Softeam) Visualisation Dashboard (Bitergia) Eclipse Plugin (FrontendArt)","title":"Running SCAVA Platform form Sources"},{"location":"developers-guide/runing-from-sources/administration-application/","text":"Running the Administration Application form Sources Prerequisites: Start running the Metric Platform . Development Toolkits Scava Administration is a based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI 6.1.4 tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart ). sudo npm install @angular/cli@6.1.4 Get the Code Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev Run the Administration webapp The following instructions show how to run the dashboard web app from source: * Enter the administration/scava-administration/ directory within the scava repository. * Install Angular dependency using npm install * Run the web app on port 4200 using angular-cli: ng serve * Navigate to http://localhost:4200/","title":"Running the Administration Application form Sources"},{"location":"developers-guide/runing-from-sources/administration-application/#running-the-administration-application-form-sources","text":"","title":"Running the Administration Application form Sources"},{"location":"developers-guide/runing-from-sources/administration-application/#prerequisites","text":"Start running the Metric Platform .","title":"Prerequisites:"},{"location":"developers-guide/runing-from-sources/administration-application/#development-toolkits","text":"Scava Administration is a based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI 6.1.4 tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart ). sudo npm install @angular/cli@6.1.4","title":"Development Toolkits"},{"location":"developers-guide/runing-from-sources/administration-application/#get-the-code","text":"Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev","title":"Get the Code"},{"location":"developers-guide/runing-from-sources/administration-application/#run-the-administration-webapp","text":"The following instructions show how to run the dashboard web app from source: * Enter the administration/scava-administration/ directory within the scava repository. * Install Angular dependency using npm install * Run the web app on port 4200 using angular-cli: ng serve * Navigate to http://localhost:4200/","title":"Run the Administration webapp"},{"location":"developers-guide/runing-from-sources/analysis-platform/","text":"Running the Analysis Platform form Sources This is a quick start guide to get the SCAVA platform running from source with Eclipse. Prerequisite Install MongoDB You can download MongoDB from the MongoDb website . Install EclipseIDE Although these instructions may apply to other versions of Eclipse IDE, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package). Get the Code Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev Configuration Configure The Eclipse IDE Setup the Target Platform Definition We first need to setup the Target Platform definition of Scava. In Eclipse, import ( File -> Import -> Existing projects into workspace ) the project metric-platform/releng/org.eclipse.scava.target . Then, go to Window -> Preferences -> Plug-in Development -> Target Platform and check the scava target definition in the list. Confirm your selection and wait for the target platform to be resolved by Eclipse; this may take a while. Figure-01: Setup the Eclipse Target Platform. Import Projects into Eclipse Workspace Import all projects from the top level directory of the Scava code ( File -> Import -> Maven -> Existing Maven Projects ), and wait for all the projects to compile without errors. In the case that the Eclipse IDE may trigger errors related to \"Plugin execution not covered by lifecycle configuration: org.eclipse.tycho:tycho-compiler-plugin:1.1.0:compile\". Right-click on one of them, select Quick Fix, let Eclipse install the appropriate M2E connectors and restart: Figure-02: Install m2e-connectors plugins. Configure the Analysis Platform identifier=<your name> The identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID. If you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers. log.type=console|file|rolling You can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify file or rolling , you must complete the log.file.path or log.rolling.path property as well. If the property is not specified, it will default to the console logger. log.file.path=<path> The path to the file to store the log. E.g. log.file.path=/tmp/lovelylog.log log.rolling.path=<path> The path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify log.rolling.path=/tmp/mylovelylog.log , it will store files like so: /tmp/mylovelylog.log.2014-12-17-00 and /tmp/mylovelylog.log.2014-12-17-12 . maven_executable=<path> The path to where Maven is installed. E.g. maven_executable=/usr/bin/mvn storage_path=<path> The path to where files should be stored. E.g. storage_path=/mnt/ossmeter/ mongo_hosts A comma-separated list of the hosts and ports in a replica set. E.g. ua002:27017,ua009:27017,ua019:27017,ua020:27017 Run the Analysis Platform Start MongoDB Instructions for starting mongo can be found in the MongoDB manual . For example: sudo systemctl start mongod or sudo service mongod start Run the api-gateway Right click on scava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java Then click on Run As -> Java Application Run the authentication service Right click on scava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java Then click on Run As -> Java Application Validate and Run the Platform Open releng/org.eclipse.scava.product/scava.product * Click the Validate... icon in the top right of the product configuration editor (the icon is a piece of paper with a tick) * If things do not validate, there's something wrong -- get in touch :) Problems related to org.eclipse.e4.core.di aren't critical. * Then, click the Export an Eclipse product on the left of the Validate... button. Uncheck the Generate p2 repository checkbox, select a destination directory and validate. After a while, the SCAVA platform will be generated in the selected directory. * The platform can then be run using the generated eclipse binary; it accepts the following arguments: * -apiServer : Starts up the client API on localhost:8182 * -worker ${id-worker} : Spawns a thread that analyses registered projects * To get a full platform running, first launch a worker thread, then the API server. When starting the platform, you can pass a configuration file to control the behaviour of the platform: ./eclipse -worker ${id-worker} -config myconfiguration.properties","title":"Running the Analysis Platform form Sources"},{"location":"developers-guide/runing-from-sources/analysis-platform/#running-the-analysis-platform-form-sources","text":"This is a quick start guide to get the SCAVA platform running from source with Eclipse.","title":"Running the Analysis Platform form Sources"},{"location":"developers-guide/runing-from-sources/analysis-platform/#prerequisite","text":"","title":"Prerequisite"},{"location":"developers-guide/runing-from-sources/analysis-platform/#install-mongodb","text":"You can download MongoDB from the MongoDb website .","title":"Install MongoDB"},{"location":"developers-guide/runing-from-sources/analysis-platform/#install-eclipseide","text":"Although these instructions may apply to other versions of Eclipse IDE, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package).","title":"Install EclipseIDE"},{"location":"developers-guide/runing-from-sources/analysis-platform/#get-the-code","text":"Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev","title":"Get the Code"},{"location":"developers-guide/runing-from-sources/analysis-platform/#configuration","text":"","title":"Configuration"},{"location":"developers-guide/runing-from-sources/analysis-platform/#configure-the-eclipse-ide","text":"","title":"Configure The Eclipse IDE"},{"location":"developers-guide/runing-from-sources/analysis-platform/#setup-the-target-platform-definition","text":"We first need to setup the Target Platform definition of Scava. In Eclipse, import ( File -> Import -> Existing projects into workspace ) the project metric-platform/releng/org.eclipse.scava.target . Then, go to Window -> Preferences -> Plug-in Development -> Target Platform and check the scava target definition in the list. Confirm your selection and wait for the target platform to be resolved by Eclipse; this may take a while. Figure-01: Setup the Eclipse Target Platform.","title":"Setup the Target Platform Definition"},{"location":"developers-guide/runing-from-sources/analysis-platform/#import-projects-into-eclipse-workspace","text":"Import all projects from the top level directory of the Scava code ( File -> Import -> Maven -> Existing Maven Projects ), and wait for all the projects to compile without errors. In the case that the Eclipse IDE may trigger errors related to \"Plugin execution not covered by lifecycle configuration: org.eclipse.tycho:tycho-compiler-plugin:1.1.0:compile\". Right-click on one of them, select Quick Fix, let Eclipse install the appropriate M2E connectors and restart: Figure-02: Install m2e-connectors plugins.","title":"Import Projects into Eclipse Workspace"},{"location":"developers-guide/runing-from-sources/analysis-platform/#configure-the-analysis-platform","text":"","title":"Configure the Analysis Platform"},{"location":"developers-guide/runing-from-sources/analysis-platform/#identifierltyour-namegt","text":"The identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID. If you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers.","title":"identifier=&lt;your name&gt;"},{"location":"developers-guide/runing-from-sources/analysis-platform/#logtypeconsolefilerolling","text":"You can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify file or rolling , you must complete the log.file.path or log.rolling.path property as well. If the property is not specified, it will default to the console logger.","title":"log.type=console|file|rolling"},{"location":"developers-guide/runing-from-sources/analysis-platform/#logfilepathltpathgt","text":"The path to the file to store the log. E.g. log.file.path=/tmp/lovelylog.log","title":"log.file.path=&lt;path&gt;"},{"location":"developers-guide/runing-from-sources/analysis-platform/#logrollingpathltpathgt","text":"The path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify log.rolling.path=/tmp/mylovelylog.log , it will store files like so: /tmp/mylovelylog.log.2014-12-17-00 and /tmp/mylovelylog.log.2014-12-17-12 .","title":"log.rolling.path=&lt;path&gt;"},{"location":"developers-guide/runing-from-sources/analysis-platform/#maven_executableltpathgt","text":"The path to where Maven is installed. E.g. maven_executable=/usr/bin/mvn","title":"maven_executable=&lt;path&gt;"},{"location":"developers-guide/runing-from-sources/analysis-platform/#storage_pathltpathgt","text":"The path to where files should be stored. E.g. storage_path=/mnt/ossmeter/","title":"storage_path=&lt;path&gt;"},{"location":"developers-guide/runing-from-sources/analysis-platform/#mongo_hosts","text":"A comma-separated list of the hosts and ports in a replica set. E.g. ua002:27017,ua009:27017,ua019:27017,ua020:27017","title":"mongo_hosts"},{"location":"developers-guide/runing-from-sources/analysis-platform/#run-the-analysis-platform","text":"","title":"Run the Analysis Platform"},{"location":"developers-guide/runing-from-sources/analysis-platform/#start-mongodb","text":"Instructions for starting mongo can be found in the MongoDB manual . For example: sudo systemctl start mongod or sudo service mongod start","title":"Start MongoDB"},{"location":"developers-guide/runing-from-sources/analysis-platform/#run-the-api-gateway","text":"Right click on scava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java Then click on Run As -> Java Application","title":"Run the api-gateway"},{"location":"developers-guide/runing-from-sources/analysis-platform/#run-the-authentication-service","text":"Right click on scava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java Then click on Run As -> Java Application","title":"Run the authentication service"},{"location":"developers-guide/runing-from-sources/analysis-platform/#validate-and-run-the-platform","text":"Open releng/org.eclipse.scava.product/scava.product * Click the Validate... icon in the top right of the product configuration editor (the icon is a piece of paper with a tick) * If things do not validate, there's something wrong -- get in touch :) Problems related to org.eclipse.e4.core.di aren't critical. * Then, click the Export an Eclipse product on the left of the Validate... button. Uncheck the Generate p2 repository checkbox, select a destination directory and validate. After a while, the SCAVA platform will be generated in the selected directory. * The platform can then be run using the generated eclipse binary; it accepts the following arguments: * -apiServer : Starts up the client API on localhost:8182 * -worker ${id-worker} : Spawns a thread that analyses registered projects * To get a full platform running, first launch a worker thread, then the API server. When starting the platform, you can pass a configuration file to control the behaviour of the platform: ./eclipse -worker ${id-worker} -config myconfiguration.properties","title":"Validate and Run the Platform"},{"location":"developers-guide/runing-from-sources/eclipse-plugin/","text":"Running the Eclipse Plugin form Sources (To be completed by FrontendArt)","title":"Running the Eclipse Plugin form Sources (To be completed by FrontendArt)"},{"location":"developers-guide/runing-from-sources/eclipse-plugin/#running-the-eclipse-plugin-form-sources-to-be-completed-by-frontendart","text":"","title":"Running the Eclipse Plugin form Sources (To be completed by FrontendArt)"},{"location":"developers-guide/runing-from-sources/visualisation-dashboard/","text":"Running the Visualisation Dashboard form Sources (To be reviewed and completed by Bitergia) All the documentation below describes how to setup and run the different components related to the Web Dashboards. Install steps for the different components Install GrimoireLab Python Env The data processing is done with GrimoireLab python platform. A virtual env in Python is used to install the tools needed. In Debian/Ubuntu you need to execute: sudo apt-get install python3-venv To create the python virtualenv and activate it: mkdir ~/venvs python3 -m venv ~/venvs/crossminer source ~/venvs/crossminer/bin/activate pip3 install grimoire-elk Install Elasticsearch and Kibana An Elasticsearch and Kibana services are needed. docker-compose can be used to start them. Elasticsearch needs this config: sudo sh -c \"echo 262144 > /proc/sys/vm/max_map_count\"","title":"Running the Visualisation Dashboard form Sources (To be reviewed and completed by Bitergia)"},{"location":"developers-guide/runing-from-sources/visualisation-dashboard/#running-the-visualisation-dashboard-form-sources-to-be-reviewed-and-completed-by-bitergia","text":"All the documentation below describes how to setup and run the different components related to the Web Dashboards.","title":"Running the Visualisation Dashboard form Sources (To be reviewed and completed by Bitergia)"},{"location":"developers-guide/runing-from-sources/visualisation-dashboard/#install-steps-for-the-different-components","text":"","title":"Install steps for the different components"},{"location":"developers-guide/runing-from-sources/visualisation-dashboard/#install-grimoirelab-python-env","text":"The data processing is done with GrimoireLab python platform. A virtual env in Python is used to install the tools needed. In Debian/Ubuntu you need to execute: sudo apt-get install python3-venv To create the python virtualenv and activate it: mkdir ~/venvs python3 -m venv ~/venvs/crossminer source ~/venvs/crossminer/bin/activate pip3 install grimoire-elk","title":"Install GrimoireLab Python Env"},{"location":"developers-guide/runing-from-sources/visualisation-dashboard/#install-elasticsearch-and-kibana","text":"An Elasticsearch and Kibana services are needed. docker-compose can be used to start them. Elasticsearch needs this config: sudo sh -c \"echo 262144 > /proc/sys/vm/max_map_count\"","title":"Install Elasticsearch and Kibana"},{"location":"installation-guide/","text":"SCAVA Installation Guide The SCAVA installation guide provides instructions on how to install and configure the SCAVA Platform on a server and how to deploy the Eclipses Plugin in development environment. Platform Installation using DOCKER images Platform Installation using individual components Platform Data Base Backup And Restoration Eclipse Plugin Installation Platform Configuration Licencing Information","title":"SCAVA Installation Guide"},{"location":"installation-guide/#scava-installation-guide","text":"The SCAVA installation guide provides instructions on how to install and configure the SCAVA Platform on a server and how to deploy the Eclipses Plugin in development environment. Platform Installation using DOCKER images Platform Installation using individual components Platform Data Base Backup And Restoration Eclipse Plugin Installation Platform Configuration Licencing Information","title":"SCAVA Installation Guide"},{"location":"installation-guide/database-backup/","text":"Platform Data Base Backup And Restoration The SCAVA Platform use a Mongo database to strore data related to analysed projets (project data, analyse process data and measurements collected durnig the analysis process). A backup of this data can be perfroemd using the default Mongo backup service. SCAVA Paltform Data Model The SCAVA Data mode is composed of several data bases. Each of this databases must be backuped individualy Database Descripton scava The scava databases containes informations related to Analysed Projects including the list of all repository related to this project scava-analysis The scava-analysis databases containes informations related to analysis process. For each analysed project, this database containes information related to the defined analysed tasks and information related the execution of this anlaysis tasks {AnalysedProjectName} For each analysed project, the SCAVA Platform store the result of the analyse (collected measurements) a is a separate database. The Name of this database is the shortName of the project. The shortName of a project cab be retrive using the REST API exposed by the platforme ( http:/{platforme url}:{platforme port}/project) Backup Process The Mongo installation package(windows and linux) provide the mongodump service whic allow to create a database dump mongodump --gzip --archive= --db --port Example : mongodump --gzip --archive=scava.20180906.gz --db scava --port 27018 // project database mongodump --gzip --archive=scava-analysis.20180906.gz --db scava-analysis --port 27018 // analysis process database mongodump --gzip --archive=QualityGuard.20180906.gz --db QualityGuard --port 27018 // QualityGuard project database Restoration Process The Mongo installation package( windows and linux) provide the mongorestore service which allow to restore a database dump mongorestore --drop --gzip --archive=Activemq.20181203.gz Example : mongorestore --drop --gzip --archive=scava.20181203.gz // project database mongorestore --drop --gzip --archive=scava-analysis.20181203.gz // analysis process database mongorestore --drop --gzip --archive=QualityGuard.20181203.gz // QualityGuard project database","title":"Platform Data Base Backup And Restoration"},{"location":"installation-guide/database-backup/#platform-data-base-backup-and-restoration","text":"The SCAVA Platform use a Mongo database to strore data related to analysed projets (project data, analyse process data and measurements collected durnig the analysis process). A backup of this data can be perfroemd using the default Mongo backup service.","title":"Platform Data Base Backup And Restoration"},{"location":"installation-guide/database-backup/#scava-paltform-data-model","text":"The SCAVA Data mode is composed of several data bases. Each of this databases must be backuped individualy Database Descripton scava The scava databases containes informations related to Analysed Projects including the list of all repository related to this project scava-analysis The scava-analysis databases containes informations related to analysis process. For each analysed project, this database containes information related to the defined analysed tasks and information related the execution of this anlaysis tasks {AnalysedProjectName} For each analysed project, the SCAVA Platform store the result of the analyse (collected measurements) a is a separate database. The Name of this database is the shortName of the project. The shortName of a project cab be retrive using the REST API exposed by the platforme ( http:/{platforme url}:{platforme port}/project)","title":"SCAVA Paltform Data Model"},{"location":"installation-guide/database-backup/#backup-process","text":"The Mongo installation package(windows and linux) provide the mongodump service whic allow to create a database dump mongodump --gzip --archive= --db --port Example : mongodump --gzip --archive=scava.20180906.gz --db scava --port 27018 // project database mongodump --gzip --archive=scava-analysis.20180906.gz --db scava-analysis --port 27018 // analysis process database mongodump --gzip --archive=QualityGuard.20180906.gz --db QualityGuard --port 27018 // QualityGuard project database","title":"Backup Process"},{"location":"installation-guide/database-backup/#restoration-process","text":"The Mongo installation package( windows and linux) provide the mongorestore service which allow to restore a database dump mongorestore --drop --gzip --archive=Activemq.20181203.gz Example : mongorestore --drop --gzip --archive=scava.20181203.gz // project database mongorestore --drop --gzip --archive=scava-analysis.20181203.gz // analysis process database mongorestore --drop --gzip --archive=QualityGuard.20181203.gz // QualityGuard project database","title":"Restoration Process"},{"location":"installation-guide/licencing/","text":"Licencing for Scava The Scava project is licensed under Eclipse Public License - v 2.0 license. Eclipse Public License licensing Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership.","title":"Licencing for Scava"},{"location":"installation-guide/licencing/#licencing-for-scava","text":"The Scava project is licensed under Eclipse Public License - v 2.0 license.","title":"Licencing for Scava"},{"location":"installation-guide/licencing/#eclipse-public-license-licensing","text":"Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership.","title":"Eclipse Public License licensing"},{"location":"installation-guide/docker-installation/","text":"Platform Installation using DOCKER images This page is about how to deploy a SCAVA instance on the behalf of Docker. At the actual stage of the project, there two ways to get started with the docker images: 1. Ready-to-use images are stored on the Crossminer Docker-hub account . 1. Build them from the scava-deployment repository. They have to be built from various Dockerfile's and with help of a docker-compose file. Summary of containers The whole Docker stack consists of 11 services: Docker service name Full Name Default port Comments admin-webapp Administration UI 5601 Built from /web-admin oss-app Metric Plateform 8182 Built from /metric-platform oss-db MongoDB (metrics storage) 27017 Built from /metric-platform to start with a pre-loaded database (few projects analysed) or use a clean MongoDB image for a clean installation ( image: mongo:3.4 ). Can be used to connect a MongoDB visualisation tool kb Knowledge base 8080 Built from /KB kb-db Knowledge base DB (based on MongoDB) 27018 Built from /KB-db. Can be used to connect a MongoDB visualisation tool api-gw API Gateway 8086 Built from /api-gw auth Authentication 8085 Built from /auth elasticsearch ElasticSearch 9200 Pulled from docker hub acsdocker/elasticsearch:6.3.1-secured. Can be used to connect an ElasticSearch visualisation tool kibiter Kibiter (Bitergia\u2019s customized Kibana) 80 Pulled from docker hub acsdocker/grimoirelab-kibiter:crossminer-6.3.1 dashb-importer Dashboard importer (to kibiter) No port exposed on the host prosoul Prosoul Quality Model Viewer 8000 Pulled from docker hub bitergia/prosoul Prerequisites In order to run Scava, you need to: Edit the hosts of your machine, creating a new record for the IP address: 127.0.0.1 with hostname: admin-webapp Edit the docker-compose-build.yml file and change: The environment variable API-GATEWAY_VAR , replacing localhost by the IP address of your host on service admin-webapp Make sure that the ports defined in the file are not already used on the host, and adjust the various ports as required for your setup. Note that the Kibiter dashboard is on port 80 by default. Update the ALLOWED_HOST directive to include the host name on service prosoul . This is used by Django on the prosoul image to publish the quality model used by Crossminer. Building the Docker images The deployment setup is hosted in the scava-deployment repository. One needs to clone the repository locally in order to build and run the docker images. To build all the required Docker images, simply go to the root of the cloned repository and issue the following command. This will rebuild all images, dismissing any cached layers. $ docker-compose -f docker-compose-build.yml build --no-cache This will build required images using the latest version of the binaries on the CI server and pull images hosted on docker hub. Setup Configurations Some optional configurations can be made to customize the dockerized SCAVA instance. These configuration are applied to the docker-compose description files docker-compose-build.yml or docker-compose-dockerhub.yml and are applied every time that the instance runs. Data persistence Volumes can be created to persist the data of the databases between execution of the Scava platform. To enable the creation of volumes uncomemnt the corresponding lines on the oss-db and kb-db services: On oss-db : volumes: #creates volume on container - ~/oss-data:/data/db On kb-db : volumes: #creates volume on container - ~/kb-data:/data/kb-db Multiple Workers Configuration One instance of the metric-platform is only able to run one worker. Therefore, a setup where it is required to have more than one worker requires the deployment of multiple instances of metric platform. The main instance of the metric platform runs under the service oss-app . This is the service started by default with the master and apiServer flags, and launchs the worker w1 . To enable a second worker, the docker-compose-build.yml (or docker-compose-dockerhub.yml ) file must be edited and uncomented the service oss-slave . This service will launch a new instance of the metric platform that will only run one worker (with default name w2 ). This slave instance do not have the flags master or apiServer . New slaves can be added (e.g. oss-slave01 , oss-slave02 , oss-slave03 , etc.) by copy-paste of the oss-slave service and changing the name of service and name of the worker. Running the locally built docker images To run the locally built images, run the following command. Note that if the images are not available they will be rebuilt. $ docker-compose -f docker-compose-build.yml up Access the administration web app by using the following address in the web browser: http://admin-webapp/ For login use user: admin pass: admin Running the pre-built docker images Please note that the docker hub images are not yet ready! We're working on it! :-) The easiest way to run the full Scava setup is to use the docker images stored on Docker Hub . Use the docker-compose-dockerhub.yml file to download all required images and start the stack: $ docker-compose -f docker-compose-dockerhub.yml up Access the administration web app by using the following address in the web browser: http://admin-webapp/ For login use user: admin pass: admin Post-install tasks Configuring the GitHub token In order to use the GitHub connectors, one needs to setup an authentication mechanism. Simply create a authentication token in yourGitHub account, and create a new property in the webadmin UI: key: githubToken value: the github token created on the github account. Kibana dashboard The first time Kibana is started, it will ask for a default index pattern. To select one, log into the dashboard (admin/admin), go to the dashboard menu item and select metrics-scava . Then click on the star on the top right. Continuous integration We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the #ci channel in Slack.","title":"Platform Installation using DOCKER images"},{"location":"installation-guide/docker-installation/#platform-installation-using-docker-images","text":"This page is about how to deploy a SCAVA instance on the behalf of Docker. At the actual stage of the project, there two ways to get started with the docker images: 1. Ready-to-use images are stored on the Crossminer Docker-hub account . 1. Build them from the scava-deployment repository. They have to be built from various Dockerfile's and with help of a docker-compose file.","title":"Platform Installation using DOCKER images"},{"location":"installation-guide/docker-installation/#summary-of-containers","text":"The whole Docker stack consists of 11 services: Docker service name Full Name Default port Comments admin-webapp Administration UI 5601 Built from /web-admin oss-app Metric Plateform 8182 Built from /metric-platform oss-db MongoDB (metrics storage) 27017 Built from /metric-platform to start with a pre-loaded database (few projects analysed) or use a clean MongoDB image for a clean installation ( image: mongo:3.4 ). Can be used to connect a MongoDB visualisation tool kb Knowledge base 8080 Built from /KB kb-db Knowledge base DB (based on MongoDB) 27018 Built from /KB-db. Can be used to connect a MongoDB visualisation tool api-gw API Gateway 8086 Built from /api-gw auth Authentication 8085 Built from /auth elasticsearch ElasticSearch 9200 Pulled from docker hub acsdocker/elasticsearch:6.3.1-secured. Can be used to connect an ElasticSearch visualisation tool kibiter Kibiter (Bitergia\u2019s customized Kibana) 80 Pulled from docker hub acsdocker/grimoirelab-kibiter:crossminer-6.3.1 dashb-importer Dashboard importer (to kibiter) No port exposed on the host prosoul Prosoul Quality Model Viewer 8000 Pulled from docker hub bitergia/prosoul","title":"Summary of containers"},{"location":"installation-guide/docker-installation/#prerequisites","text":"In order to run Scava, you need to: Edit the hosts of your machine, creating a new record for the IP address: 127.0.0.1 with hostname: admin-webapp Edit the docker-compose-build.yml file and change: The environment variable API-GATEWAY_VAR , replacing localhost by the IP address of your host on service admin-webapp Make sure that the ports defined in the file are not already used on the host, and adjust the various ports as required for your setup. Note that the Kibiter dashboard is on port 80 by default. Update the ALLOWED_HOST directive to include the host name on service prosoul . This is used by Django on the prosoul image to publish the quality model used by Crossminer.","title":"Prerequisites"},{"location":"installation-guide/docker-installation/#building-the-docker-images","text":"The deployment setup is hosted in the scava-deployment repository. One needs to clone the repository locally in order to build and run the docker images. To build all the required Docker images, simply go to the root of the cloned repository and issue the following command. This will rebuild all images, dismissing any cached layers. $ docker-compose -f docker-compose-build.yml build --no-cache This will build required images using the latest version of the binaries on the CI server and pull images hosted on docker hub.","title":"Building the Docker images"},{"location":"installation-guide/docker-installation/#setup-configurations","text":"Some optional configurations can be made to customize the dockerized SCAVA instance. These configuration are applied to the docker-compose description files docker-compose-build.yml or docker-compose-dockerhub.yml and are applied every time that the instance runs.","title":"Setup Configurations"},{"location":"installation-guide/docker-installation/#data-persistence","text":"Volumes can be created to persist the data of the databases between execution of the Scava platform. To enable the creation of volumes uncomemnt the corresponding lines on the oss-db and kb-db services: On oss-db : volumes: #creates volume on container - ~/oss-data:/data/db On kb-db : volumes: #creates volume on container - ~/kb-data:/data/kb-db","title":"Data persistence"},{"location":"installation-guide/docker-installation/#multiple-workers-configuration","text":"One instance of the metric-platform is only able to run one worker. Therefore, a setup where it is required to have more than one worker requires the deployment of multiple instances of metric platform. The main instance of the metric platform runs under the service oss-app . This is the service started by default with the master and apiServer flags, and launchs the worker w1 . To enable a second worker, the docker-compose-build.yml (or docker-compose-dockerhub.yml ) file must be edited and uncomented the service oss-slave . This service will launch a new instance of the metric platform that will only run one worker (with default name w2 ). This slave instance do not have the flags master or apiServer . New slaves can be added (e.g. oss-slave01 , oss-slave02 , oss-slave03 , etc.) by copy-paste of the oss-slave service and changing the name of service and name of the worker.","title":"Multiple Workers Configuration"},{"location":"installation-guide/docker-installation/#running-the-locally-built-docker-images","text":"To run the locally built images, run the following command. Note that if the images are not available they will be rebuilt. $ docker-compose -f docker-compose-build.yml up Access the administration web app by using the following address in the web browser: http://admin-webapp/ For login use user: admin pass: admin","title":"Running the locally built docker images"},{"location":"installation-guide/docker-installation/#running-the-pre-built-docker-images","text":"Please note that the docker hub images are not yet ready! We're working on it! :-) The easiest way to run the full Scava setup is to use the docker images stored on Docker Hub . Use the docker-compose-dockerhub.yml file to download all required images and start the stack: $ docker-compose -f docker-compose-dockerhub.yml up Access the administration web app by using the following address in the web browser: http://admin-webapp/ For login use user: admin pass: admin","title":"Running the pre-built docker images"},{"location":"installation-guide/docker-installation/#post-install-tasks","text":"","title":"Post-install tasks"},{"location":"installation-guide/docker-installation/#configuring-the-github-token","text":"In order to use the GitHub connectors, one needs to setup an authentication mechanism. Simply create a authentication token in yourGitHub account, and create a new property in the webadmin UI: key: githubToken value: the github token created on the github account.","title":"Configuring the GitHub token"},{"location":"installation-guide/docker-installation/#kibana-dashboard","text":"The first time Kibana is started, it will ask for a default index pattern. To select one, log into the dashboard (admin/admin), go to the dashboard menu item and select metrics-scava . Then click on the star on the top right.","title":"Kibana dashboard"},{"location":"installation-guide/docker-installation/#continuous-integration","text":"We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the #ci channel in Slack.","title":"Continuous integration"},{"location":"installation-guide/platform-configuration/","text":"Platform Installation using individual components (UNPARALLEL)","title":"Platform Installation using individual components (UNPARALLEL)"},{"location":"installation-guide/platform-configuration/#platform-installation-using-individual-components-unparallel","text":"","title":"Platform Installation using individual components (UNPARALLEL)"},{"location":"installation-guide/platform-installation/","text":"Platform Installation using individual components (UNPARALLEL) KB installation un it by maven: mvn spring-boot:run test it by maven: mvn test When starting the platform, you can set configuration application parameters in properly configuration file: deploy configuration file at src/test/resources/application.properties. test cnfiguration file at src/test/resources/application.properties. The configuration file is a typical Java properties file. The properties that can be configured are: spring.data.mongodb.host=<DB_HOST> spring.data.mongodb.port=<DB_PORT> spring.data.mongodb.database=<DB_NAME> lucene.index.folder=<path_to_lucene_index> egit.github.token=<GITHUB_TOKEN> ossmeter.url=<OSSMETER_URL> ... An instance of configuration file: spring.data.mongodb.host=localhost spring.data.mongodb.port=27017 spring.data.mongodb.database=CROSSMINER_TEST lucene.index.folder=/home/user/CROSSMiner_lucene/ egit.github.token=<github_token> ossmeter.url=http://localhost:8182/ ...","title":"Platform Installation using individual components (UNPARALLEL)"},{"location":"installation-guide/platform-installation/#platform-installation-using-individual-components-unparallel","text":"","title":"Platform Installation using individual components (UNPARALLEL)"},{"location":"installation-guide/platform-installation/#kb-installation","text":"un it by maven: mvn spring-boot:run test it by maven: mvn test When starting the platform, you can set configuration application parameters in properly configuration file: deploy configuration file at src/test/resources/application.properties. test cnfiguration file at src/test/resources/application.properties. The configuration file is a typical Java properties file. The properties that can be configured are: spring.data.mongodb.host=<DB_HOST> spring.data.mongodb.port=<DB_PORT> spring.data.mongodb.database=<DB_NAME> lucene.index.folder=<path_to_lucene_index> egit.github.token=<GITHUB_TOKEN> ossmeter.url=<OSSMETER_URL> ... An instance of configuration file: spring.data.mongodb.host=localhost spring.data.mongodb.port=27017 spring.data.mongodb.database=CROSSMINER_TEST lucene.index.folder=/home/user/CROSSMiner_lucene/ egit.github.token=<github_token> ossmeter.url=http://localhost:8182/ ...","title":"KB installation"},{"location":"installation-guide/plugin-installation/","text":"Eclipse Plugin Installation","title":"Eclipse Plugin Installation"},{"location":"installation-guide/plugin-installation/#eclipse-plugin-installation","text":"","title":"Eclipse Plugin Installation"},{"location":"others/","text":"Welcome to the Scava documentation This web site is the main documentation place for the Eclipse Scava project. Useful links: Eclipse Scava home project: Eclipse Scava @ Eclipse Eclipse Scava code repository: github.com/crossminer/scava Eclipse Scava deployment repository: github.com/crossminer/scava-deployment Eclipse Scava documentation repository: github.com/crossminer/scava-docs Platform installation Docker-SCAVA How to build and run the Scava docker image. Running the platform Quick start guide to get the Scava platform running from source on an Eclipse development environment. Configuring the platform Quick start guide to present how to configure the platform using a configuration file. Docker-OSSMETER How to build and run the Ossmeter docker image. Administration Scava Administration The administration dashboard take care of managing Scava's services. API Gateway Configuration Extending MongoDB Data Model Users Scava metrics lists metrics computed by the various Scava Components. Consuming the REST services This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues Running Scava in Eclipse How to setup and run the Scava Eclipse IDE plugin. REST API Documentation Reference documentation of REST services provided by the Scava platform. REST API Generation Tutorial about automatic generation of REST API Scava library using OpenAPI. Accessing Scava resources A summary of where to find the various outputs of the Scava platform. Development Contributing Collection of Architectural and Technical guidelines dedicated to Scava developers. Development guidelines Rules and guidelines used for the development of the Scava project. Testing Guidelines Collection of testing guidelines dedicated to Scava developers. Repository-Organisation How to develop a metric provider Want to add a new metric provider? Here are some hints. Licensing Information about licensing used within Scava. Architecture API Gateway Component The API Gateway allows to access to all Scava REST services using a centralised and securised common gateway. Authentication Component The administration dashboard takes care of managing Scava's services.","title":"Welcome to the Scava documentation"},{"location":"others/#welcome-to-the-scava-documentation","text":"This web site is the main documentation place for the Eclipse Scava project. Useful links: Eclipse Scava home project: Eclipse Scava @ Eclipse Eclipse Scava code repository: github.com/crossminer/scava Eclipse Scava deployment repository: github.com/crossminer/scava-deployment Eclipse Scava documentation repository: github.com/crossminer/scava-docs","title":"Welcome to the Scava documentation"},{"location":"others/#platform-installation","text":"Docker-SCAVA How to build and run the Scava docker image. Running the platform Quick start guide to get the Scava platform running from source on an Eclipse development environment. Configuring the platform Quick start guide to present how to configure the platform using a configuration file. Docker-OSSMETER How to build and run the Ossmeter docker image.","title":"Platform installation"},{"location":"others/#administration","text":"Scava Administration The administration dashboard take care of managing Scava's services. API Gateway Configuration Extending MongoDB Data Model","title":"Administration"},{"location":"others/#users","text":"Scava metrics lists metrics computed by the various Scava Components. Consuming the REST services This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues Running Scava in Eclipse How to setup and run the Scava Eclipse IDE plugin. REST API Documentation Reference documentation of REST services provided by the Scava platform. REST API Generation Tutorial about automatic generation of REST API Scava library using OpenAPI. Accessing Scava resources A summary of where to find the various outputs of the Scava platform.","title":"Users"},{"location":"others/#development","text":"Contributing Collection of Architectural and Technical guidelines dedicated to Scava developers. Development guidelines Rules and guidelines used for the development of the Scava project. Testing Guidelines Collection of testing guidelines dedicated to Scava developers. Repository-Organisation How to develop a metric provider Want to add a new metric provider? Here are some hints. Licensing Information about licensing used within Scava.","title":"Development"},{"location":"others/#architecture","text":"API Gateway Component The API Gateway allows to access to all Scava REST services using a centralised and securised common gateway. Authentication Component The administration dashboard takes care of managing Scava's services.","title":"Architecture"},{"location":"others/admin/API-Gateway-Configuration/","text":"API Gateway configuration When to use this guideline ? This guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API. Context The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters. Routing : Service Configuration To reference a new remote REST API in the gateway, you have to add 2 new properties in the application.properties configuration file : the relative path of services which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process. Examples : * api gateway url = http://85.36.10.13:8080 * path = /administration/** * url = http://85.36.10.12:8082/administration The request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1 Configuration file example # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2 Comments More information about API Gateway configuration: API Gateway Component","title":"API Gateway configuration"},{"location":"others/admin/API-Gateway-Configuration/#api-gateway-configuration","text":"","title":"API Gateway configuration"},{"location":"others/admin/API-Gateway-Configuration/#when-to-use-this-guideline","text":"This guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API.","title":"When to use this guideline ?"},{"location":"others/admin/API-Gateway-Configuration/#context","text":"The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.","title":"Context"},{"location":"others/admin/API-Gateway-Configuration/#routing-service-configuration","text":"To reference a new remote REST API in the gateway, you have to add 2 new properties in the application.properties configuration file : the relative path of services which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process. Examples : * api gateway url = http://85.36.10.13:8080 * path = /administration/** * url = http://85.36.10.12:8082/administration The request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1","title":"Routing : Service Configuration"},{"location":"others/admin/API-Gateway-Configuration/#configuration-file-example","text":"# Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2","title":"Configuration file example"},{"location":"others/admin/API-Gateway-Configuration/#comments","text":"More information about API Gateway configuration: API Gateway Component","title":"Comments"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/","text":"When to use ? This guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas. Context The Scava platform use a MongoDb data base to store his data. We go through PONGO , a template based Java POJO generator to access MongoDB database. With Pongo we can define the data model which generates strongly-typed Java classes. In this guideligne, we will present : * The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform. * The access to MongoDb Document on from an external Java application. * How to preform basic CRUD operation with a PONGO Java data model. We consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model : Extend MongoDB Data Model . You want to access to MongoDB Document from an Eclipse Plugin ? 1. Add a dependency to the Java Data Model Edit the plugin.xml file of your plugin. In Dependency section, add a dependency to the plugin which contained the data model you went to access. To org.ossmeter.repository.model to access data related to project administration , metric execution process and authentification system. To org.ossmeter.repository.model.'project delta manager' to access configuration informations related to source codes managemeny tools. To specific metric provider plugins to access data related to a specific metric provider implementation contains his once data model. ... others plugin which contained the data model In Dependency section, add a dependency to com.googlecode.pongo.runtime plugin 2. Initiate a Connection to the MongoDb In context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base. In Dependency section of plugin.xml file , add a dependency to the org.ossmeter.platform plugin. You can now create a new connection to the database using the Configuration service. Mongo mongo = Configuration.getInstance().getMongoConnection(); You want to access to MongoDB Document on from an External Java Application ? 1. Add a dependency to the Java Data Model Add to your java project a dependency to the jar which contained the data model you went to access. You will have to deliver this jar with your application. Add o your java project a dependency to the pongo.jar jar file which can be download at this url : https://github.com/kolovos/pongo/releases 2. Initiate a Connection to MongoDb // Define ServerAddress of the MongoDb database List<ServerAddress> mongoHostAddresses = new ArrayList<>(); mongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1]))); // Create Connection Mongo mongo = new Mongo(mongoHostAddresses); Once the connection to MongoDb has been created, you have to make the link between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document. DB db = mongo.getDB(\"databasename\"); // Initiate the Project Java model ProjectRepository = new ProjectRepository(db); Basic CRUD with a PONGO Java data model 1. CREATE // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true); 2. READ // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to access object properties metricprovider.getname(); ...... 3. UPDATE // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true); 4. DELETE Mongo mongo = new Mongo(); mongo.dropDatabase(\"databasename\"); Comment This wiki has dealt with the access of MongoDB database using PONGO. To continue learning how to modify and make a new model with Pongo, we have another page here Extend MongoDB Data Model .","title":"Access to MongoDB database using PONGO"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#when-to-use","text":"This guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas.","title":"When to use ?"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#context","text":"The Scava platform use a MongoDb data base to store his data. We go through PONGO , a template based Java POJO generator to access MongoDB database. With Pongo we can define the data model which generates strongly-typed Java classes. In this guideligne, we will present : * The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform. * The access to MongoDb Document on from an external Java application. * How to preform basic CRUD operation with a PONGO Java data model. We consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model : Extend MongoDB Data Model .","title":"Context"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#you-want-to-access-to-mongodb-document-from-an-eclipse-plugin","text":"","title":"You want to access to MongoDB Document from an Eclipse Plugin ?"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#1-add-a-dependency-to-the-java-data-model","text":"Edit the plugin.xml file of your plugin. In Dependency section, add a dependency to the plugin which contained the data model you went to access. To org.ossmeter.repository.model to access data related to project administration , metric execution process and authentification system. To org.ossmeter.repository.model.'project delta manager' to access configuration informations related to source codes managemeny tools. To specific metric provider plugins to access data related to a specific metric provider implementation contains his once data model. ... others plugin which contained the data model In Dependency section, add a dependency to com.googlecode.pongo.runtime plugin","title":"1. Add a dependency to the Java Data Model"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#2-initiate-a-connection-to-the-mongodb","text":"In context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base. In Dependency section of plugin.xml file , add a dependency to the org.ossmeter.platform plugin. You can now create a new connection to the database using the Configuration service. Mongo mongo = Configuration.getInstance().getMongoConnection();","title":"2. Initiate a Connection to the MongoDb"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#you-want-to-access-to-mongodb-document-on-from-an-external-java-application","text":"","title":"You want to access to MongoDB Document on from an External Java Application ?"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#1-add-a-dependency-to-the-java-data-model_1","text":"Add to your java project a dependency to the jar which contained the data model you went to access. You will have to deliver this jar with your application. Add o your java project a dependency to the pongo.jar jar file which can be download at this url : https://github.com/kolovos/pongo/releases","title":"1. Add a dependency to the Java Data Model"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#2-initiate-a-connection-to-mongodb","text":"// Define ServerAddress of the MongoDb database List<ServerAddress> mongoHostAddresses = new ArrayList<>(); mongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1]))); // Create Connection Mongo mongo = new Mongo(mongoHostAddresses); Once the connection to MongoDb has been created, you have to make the link between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document. DB db = mongo.getDB(\"databasename\"); // Initiate the Project Java model ProjectRepository = new ProjectRepository(db);","title":"2. Initiate a Connection to MongoDb"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#basic-crud-with-a-pongo-java-data-model","text":"","title":"Basic CRUD with a PONGO Java data model"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#1-create","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true);","title":"1. CREATE"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#2-read","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to access object properties metricprovider.getname(); ......","title":"2. READ"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#3-update","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true);","title":"3. UPDATE"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#4-delete","text":"Mongo mongo = new Mongo(); mongo.dropDatabase(\"databasename\");","title":"4. DELETE"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#comment","text":"This wiki has dealt with the access of MongoDB database using PONGO. To continue learning how to modify and make a new model with Pongo, we have another page here Extend MongoDB Data Model .","title":"Comment"},{"location":"others/admin/SCAVA-Administration/","text":"Scava Administration The SCAVA administration dashboard take care of: Provide user administration feature, including user profile activation service and roles based authorization management. Provide services to analyse automatically open source software projects. Administration Dashboard Installation Prerequired The SCAVA administration dashboard is front end web application based on Angular Framework. It can be executed in both Linux or Windows systems where it's required the installation of the following tools: Node.js Download Node.js ver. 8.11.2 (includes npm 5.6.0) or above : https://nodejs.org/en/download/ Yarn Package Manager Download Yarn ver. 1.7.0 or above : https://yarnpkg.com Get Started Scava Administration Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart). Scava Dashboard Deployment In order to deploy the Scava Administration, you must to build and copy the output directory to a web server (For instance Apache HTTP Server). Using the development profile: Execute the development build using the Angular CLI command line : ng build . Copy everything within the output folder (dist/ by default) to a folder on the server. If you copy the files into a server sub-folder, append the build flag, --base-href and set the appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to like this. or simply you can run: ng build --base-href=/administration/ Using the production profile: You can generate an optimized build with additional CLI command line flags: ng build -- prod . Copy everything within the output folder (dist/ by default) to a folder on the server. If you copy the files into a server sub-folder, append the build flag, --base-href and set the appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to like this. or simply you can run: ng build --base-href=/administration/ .","title":"Scava Administration"},{"location":"others/admin/SCAVA-Administration/#scava-administration","text":"The SCAVA administration dashboard take care of: Provide user administration feature, including user profile activation service and roles based authorization management. Provide services to analyse automatically open source software projects.","title":"Scava Administration"},{"location":"others/admin/SCAVA-Administration/#administration-dashboard-installation","text":"","title":"Administration Dashboard Installation"},{"location":"others/admin/SCAVA-Administration/#prerequired","text":"The SCAVA administration dashboard is front end web application based on Angular Framework. It can be executed in both Linux or Windows systems where it's required the installation of the following tools:","title":"Prerequired"},{"location":"others/admin/SCAVA-Administration/#nodejs","text":"Download Node.js ver. 8.11.2 (includes npm 5.6.0) or above : https://nodejs.org/en/download/","title":"Node.js"},{"location":"others/admin/SCAVA-Administration/#yarn-package-manager","text":"Download Yarn ver. 1.7.0 or above : https://yarnpkg.com","title":"Yarn Package Manager"},{"location":"others/admin/SCAVA-Administration/#get-started-scava-administration","text":"Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart).","title":"Get Started Scava Administration"},{"location":"others/admin/SCAVA-Administration/#scava-dashboard-deployment","text":"In order to deploy the Scava Administration, you must to build and copy the output directory to a web server (For instance Apache HTTP Server).","title":"Scava Dashboard Deployment"},{"location":"others/admin/SCAVA-Administration/#using-the-development-profile","text":"Execute the development build using the Angular CLI command line : ng build . Copy everything within the output folder (dist/ by default) to a folder on the server. If you copy the files into a server sub-folder, append the build flag, --base-href and set the appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to like this. or simply you can run: ng build --base-href=/administration/","title":"Using the development profile:"},{"location":"others/admin/SCAVA-Administration/#using-the-production-profile","text":"You can generate an optimized build with additional CLI command line flags: ng build -- prod . Copy everything within the output folder (dist/ by default) to a folder on the server. If you copy the files into a server sub-folder, append the build flag, --base-href and set the appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to like this. or simply you can run: ng build --base-href=/administration/ .","title":"Using the production profile:"},{"location":"others/architecture/API-Gateway-Component/","text":"The API Gateway component The Scava API Gateway : Provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application). Provide a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services. API Gateway Architecture The API Gateway is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change. The API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the API Gateway will manage the authentication for all services of the platform. Authentication Mechanism JSON Web Tokens The Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io) Authentication Architecture In Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway. Authentication Flow To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Implementation The implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures. https://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul API Gateway Configuration The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters. Server Configuration id : server.port default : 8086 Port of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port. JWT Security Configuration id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. id : apigateway.security.jwt.url default : /login URL Path of the authentication service. id : apigateway.security.jwt.expiration default : 86400 (24H) Port of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port. Routing : Authentication Service Configuration id : zuul.routes.auth-center.path default : /api/authentication/** Relative path of the authentication service. id : zuul.routes.auth-center.url default : NA URL of the authentification server. Example: http://127.0.0.1:8081/ id : zuul.routes.auth-center.sensitiveHeaders default : Cookie,Set-Cookie Specify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers. id : zuul.routes.auth-center.stripPrefix default : false Switch off the stripping of the service-specific prefix from individual routes Routing : Service Configuration id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1 Configuration file example #API Gateway Port server.port=8086 # JWT Configuration apigateway.security.jwt.secret=otherpeopledontknowit apigateway.security.jwt.url=/api/authentication apigateway.security.jwt.expiration=86400 # Rooting Configuration : Authentication Service zuul.routes.auth-center.path=/api/authentication/** zuul.routes.auth-center.url=http://127.0.0.1:8081/ zuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie zuul.routes.auth-center.stripPrefix=false # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2 Control access API The Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including: * \u201cROLE_ADMIN\u201d * \u201cROLE_PROJECT_MANAGER\u201d * \u201cROLE_USER\u201d By the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d. # Filtering private restApi scava.routes.config.adminAccessApi[0]=/api/users scava.routes.config.adminAccessApi[1]=/api/user/** scava.routes.config.projectManagerAccessApi[0]=/administration/projects/create scava.routes.config.projectManagerAccessApi[1]=/administration/projects/import scava.routes.config.projectManagerAccessApi[2]=/administration/analysis/** scava.routes.config.userAccessApi[0]=/administration/projects scava.routes.config.userAccessApi[1]=/administration/projects/p/** scava.routes.config.userAccessApi[2]=/api/users/** scava.routes.config.userAccessApi[3]=/api/account Packaging Form Sources Maven Packaging mvn -Pprod install API Gateway Execution complete an put the \"application.properties\" configuration file in the execute directory. Execute the crossmeter-api-gateway-1.0.0.jar Jar. java -jar scava-api-gateway-1.0.0.jar Client Implementation How to consume a Scava REST services ? \\ This guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.","title":"The API Gateway component"},{"location":"others/architecture/API-Gateway-Component/#the-api-gateway-component","text":"The Scava API Gateway : Provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application). Provide a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services.","title":"The API Gateway component"},{"location":"others/architecture/API-Gateway-Component/#api-gateway-architecture","text":"The API Gateway is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change. The API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the API Gateway will manage the authentication for all services of the platform.","title":"API Gateway Architecture"},{"location":"others/architecture/API-Gateway-Component/#authentication-mechanism","text":"","title":"Authentication Mechanism"},{"location":"others/architecture/API-Gateway-Component/#json-web-tokens","text":"The Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io)","title":"JSON Web Tokens"},{"location":"others/architecture/API-Gateway-Component/#authentication-architecture","text":"In Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway.","title":"Authentication Architecture"},{"location":"others/architecture/API-Gateway-Component/#authentication-flow","text":"To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service.","title":"Authentication Flow"},{"location":"others/architecture/API-Gateway-Component/#implementation","text":"The implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures. https://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul","title":"Implementation"},{"location":"others/architecture/API-Gateway-Component/#api-gateway-configuration","text":"The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.","title":"API Gateway Configuration"},{"location":"others/architecture/API-Gateway-Component/#server-configuration","text":"id : server.port default : 8086 Port of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port.","title":"Server Configuration"},{"location":"others/architecture/API-Gateway-Component/#jwt-security-configuration","text":"id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. id : apigateway.security.jwt.url default : /login URL Path of the authentication service. id : apigateway.security.jwt.expiration default : 86400 (24H) Port of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port.","title":"JWT Security Configuration"},{"location":"others/architecture/API-Gateway-Component/#routing-authentication-service-configuration","text":"id : zuul.routes.auth-center.path default : /api/authentication/** Relative path of the authentication service. id : zuul.routes.auth-center.url default : NA URL of the authentification server. Example: http://127.0.0.1:8081/ id : zuul.routes.auth-center.sensitiveHeaders default : Cookie,Set-Cookie Specify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers. id : zuul.routes.auth-center.stripPrefix default : false Switch off the stripping of the service-specific prefix from individual routes","title":"Routing : Authentication Service Configuration"},{"location":"others/architecture/API-Gateway-Component/#routing-service-configuration","text":"id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1","title":"Routing : Service Configuration"},{"location":"others/architecture/API-Gateway-Component/#configuration-file-example","text":"#API Gateway Port server.port=8086 # JWT Configuration apigateway.security.jwt.secret=otherpeopledontknowit apigateway.security.jwt.url=/api/authentication apigateway.security.jwt.expiration=86400 # Rooting Configuration : Authentication Service zuul.routes.auth-center.path=/api/authentication/** zuul.routes.auth-center.url=http://127.0.0.1:8081/ zuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie zuul.routes.auth-center.stripPrefix=false # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2","title":"Configuration file example"},{"location":"others/architecture/API-Gateway-Component/#control-access-api","text":"The Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including: * \u201cROLE_ADMIN\u201d * \u201cROLE_PROJECT_MANAGER\u201d * \u201cROLE_USER\u201d By the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d. # Filtering private restApi scava.routes.config.adminAccessApi[0]=/api/users scava.routes.config.adminAccessApi[1]=/api/user/** scava.routes.config.projectManagerAccessApi[0]=/administration/projects/create scava.routes.config.projectManagerAccessApi[1]=/administration/projects/import scava.routes.config.projectManagerAccessApi[2]=/administration/analysis/** scava.routes.config.userAccessApi[0]=/administration/projects scava.routes.config.userAccessApi[1]=/administration/projects/p/** scava.routes.config.userAccessApi[2]=/api/users/** scava.routes.config.userAccessApi[3]=/api/account","title":"Control access API"},{"location":"others/architecture/API-Gateway-Component/#packaging-form-sources","text":"Maven Packaging mvn -Pprod install","title":"Packaging Form Sources"},{"location":"others/architecture/API-Gateway-Component/#api-gateway-execution","text":"complete an put the \"application.properties\" configuration file in the execute directory. Execute the crossmeter-api-gateway-1.0.0.jar Jar. java -jar scava-api-gateway-1.0.0.jar","title":"API Gateway Execution"},{"location":"others/architecture/API-Gateway-Component/#client-implementation","text":"How to consume a Scava REST services ? \\ This guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.","title":"Client Implementation"},{"location":"others/architecture/Authentication-Component/","text":"Authentication Component The Scava Authentication service: Provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform. Provides user management services, including user registration process, user profile editing and roles based authorization management. Authentication API The Authentication server is a component of The Scava platform which manages the authentication for all services accessible behind the API Gateway. Authenticate User POST /api/authentication Login as a registered user. ### JSON Web Tokens (JWT) JSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are: * Header * Payload * Signature This solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/). ### JWT Authentication Implementation * Users have to login to the authentication service API using their credentials username and password. curl -i -X POST -H \"Content-Type:application/json\" http://localhost:8086/api/authentication -d '{\"username\":\"admin\", \"password\": \"admin\"}' * Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header. * The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway. curl -i -X GET -H \"Content-Type:application/json\" -H \"Authorization:Bearer eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1dGhvcml0aWVzIjpbIlJPTEVfQURNSU4iLCJST0xFX1BST0pFQ1RfTUFOQUdFUiIsIlJPTEVfVVNFUiJdLCJpYXQiOjE1MzE4OTk3NDMsImV4cCI6MTUzMTk4NjE0M30.l-iCJcnae-1mlhMb3_y09HM4HZYFaHxe_JctWi2FRUY\" http://localhost:8086/api/users ## User Management API The Authentication component provides web services for CRUD user account. Register User POST /api/register Register new user. Activate User GET /api/activate Activate the registered user. Update User PUT /api/users Update an existing user. Retrieve Users GET /api/users Get all registered users. Retrieve Login User GET /api/users/{login} Get the \"login\" user. Delete User DELETE /api/users/{login} Delete the \"login\" user. Authentication Server Configuration The Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component. Server Configuration id : server.port default : 8085 Port of the Authentication API server. Each REST request sent to the gateway must be adressed to this port. JWT Security Configuration id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. Default ADMIN configuration Property Description Default Value scava.administration.username The administrator username admin scava.administration.password The administrator password admin scava.administration.admin-role The admin role ADMIN scava.administration.project-manager-role The project manager role PROJECT_MANAGER scava.administration.project-user-role The user role USER Mongodb Database Configuration Property Description Default Value spring.data.mongodb.uri Url of the MongoDB database server mongodb://localhost:27017 spring.data.mongodb.database Name of the MongoDB database scava Mail Server configuration In order to register new users, you have to configure a mail server. Property Description Default Value spring.mail.host Url of the mail service smtp.gmail.com spring.mail.port Port of the mail service 587 spring.mail.username Login of the mail account spring.mail.password Password of the mail account spring.mail.protocol mail protocole smtp spring.mail.tls - true spring.mail.properties.mail.smtp.auth - true spring.mail.properties.mail.smtp.starttls.enable - true spring.mail.properties.mail.smtp.ssl.trust= - smtp.gmail.com Administration Dashboard Setting id : scava.administration.base-url default : http://localhost:4200 The SCAVA administration base URL to generate the activation account URL. Packaging From Sources Maven Packaging mvn -Pprod install Authentication Server Execution complete an put the \"application.properties\" configuration file in the execution directory. Execute the scava-auth-service-1.0.0.jar Jar. java -jar scava-auth-service-1.0.0.jar","title":"Authentication Component"},{"location":"others/architecture/Authentication-Component/#authentication-component","text":"The Scava Authentication service: Provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform. Provides user management services, including user registration process, user profile editing and roles based authorization management.","title":"Authentication Component"},{"location":"others/architecture/Authentication-Component/#authentication-api","text":"The Authentication server is a component of The Scava platform which manages the authentication for all services accessible behind the API Gateway. Authenticate User POST /api/authentication Login as a registered user. ### JSON Web Tokens (JWT) JSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are: * Header * Payload * Signature This solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/). ### JWT Authentication Implementation * Users have to login to the authentication service API using their credentials username and password. curl -i -X POST -H \"Content-Type:application/json\" http://localhost:8086/api/authentication -d '{\"username\":\"admin\", \"password\": \"admin\"}' * Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header. * The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway. curl -i -X GET -H \"Content-Type:application/json\" -H \"Authorization:Bearer eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1dGhvcml0aWVzIjpbIlJPTEVfQURNSU4iLCJST0xFX1BST0pFQ1RfTUFOQUdFUiIsIlJPTEVfVVNFUiJdLCJpYXQiOjE1MzE4OTk3NDMsImV4cCI6MTUzMTk4NjE0M30.l-iCJcnae-1mlhMb3_y09HM4HZYFaHxe_JctWi2FRUY\" http://localhost:8086/api/users ## User Management API The Authentication component provides web services for CRUD user account. Register User POST /api/register Register new user. Activate User GET /api/activate Activate the registered user. Update User PUT /api/users Update an existing user. Retrieve Users GET /api/users Get all registered users. Retrieve Login User GET /api/users/{login} Get the \"login\" user. Delete User DELETE /api/users/{login} Delete the \"login\" user.","title":"Authentication API"},{"location":"others/architecture/Authentication-Component/#authentication-server-configuration","text":"The Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component.","title":"Authentication Server Configuration"},{"location":"others/architecture/Authentication-Component/#server-configuration","text":"id : server.port default : 8085 Port of the Authentication API server. Each REST request sent to the gateway must be adressed to this port.","title":"Server Configuration"},{"location":"others/architecture/Authentication-Component/#jwt-security-configuration","text":"id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA.","title":"JWT Security Configuration"},{"location":"others/architecture/Authentication-Component/#default-admin-configuration","text":"Property Description Default Value scava.administration.username The administrator username admin scava.administration.password The administrator password admin scava.administration.admin-role The admin role ADMIN scava.administration.project-manager-role The project manager role PROJECT_MANAGER scava.administration.project-user-role The user role USER","title":"Default ADMIN configuration"},{"location":"others/architecture/Authentication-Component/#mongodb-database-configuration","text":"Property Description Default Value spring.data.mongodb.uri Url of the MongoDB database server mongodb://localhost:27017 spring.data.mongodb.database Name of the MongoDB database scava","title":"Mongodb Database Configuration"},{"location":"others/architecture/Authentication-Component/#mail-server-configuration","text":"In order to register new users, you have to configure a mail server. Property Description Default Value spring.mail.host Url of the mail service smtp.gmail.com spring.mail.port Port of the mail service 587 spring.mail.username Login of the mail account spring.mail.password Password of the mail account spring.mail.protocol mail protocole smtp spring.mail.tls - true spring.mail.properties.mail.smtp.auth - true spring.mail.properties.mail.smtp.starttls.enable - true spring.mail.properties.mail.smtp.ssl.trust= - smtp.gmail.com","title":"Mail Server configuration"},{"location":"others/architecture/Authentication-Component/#administration-dashboard-setting","text":"id : scava.administration.base-url default : http://localhost:4200 The SCAVA administration base URL to generate the activation account URL.","title":"Administration Dashboard Setting"},{"location":"others/architecture/Authentication-Component/#packaging-from-sources","text":"Maven Packaging mvn -Pprod install","title":"Packaging From Sources"},{"location":"others/architecture/Authentication-Component/#authentication-server-execution","text":"complete an put the \"application.properties\" configuration file in the execution directory. Execute the scava-auth-service-1.0.0.jar Jar. java -jar scava-auth-service-1.0.0.jar","title":"Authentication Server Execution"},{"location":"others/deploy/Docker-Ossmeter/","text":"Docker Ossmeter This page lists information about the Ossmeter docker image. It should be noted that the generated image uses the old Ossmeter binaries and will not be updated -- all new development will go into the Crossminer repository. All images are stored on the Crossminer Docker-hub account . The Docker image is composed of 4 services: oss-web: corresponds to the service of OSSMETER platform website. oss-app: service running api server and the orchestrator of OSSMETER slave instances. oss-slave: service corresponding to the OSSMETER slaves responsible for the analysis of software projects. There can be several slaves serving the same master for load balancing. oss-db: service responsible for the the storage of OSSMETER data. Uses a MongoDB image. The database comes pre-populated with a project and a user. The loaded dump comes from md2manoppello's repo . Login information: user: demo@crossminer.org password: demo18 Custom quality is in the user object (demo@crossminer.org) stored in the users collection of users db. It resembles the demo quality model . Running the Ossmeter docker image The easiest way to build the full stack is to run the docker-compose file: $ docker-compose up This command will download the images and run them. The application is then available on localhost:9000 . Building the Ossmeter docker image Two containers actually need to be built. They can be built individually. oss-platform Build the image from the oss-platform directory: $ docker build -t bbaldassari/ossmeter-platform . Sending build context to Docker daemon 3.072kB Step 1/5 : FROM openjdk:8-jdk oss-web Build the image from the oss-web directory: $ docker build -t bbaldassari/ossmeter-web . Sending build context to Docker daemon 3.072kB Step 1/7 : FROM openjdk:8-jre-alpine Continuous integration We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the #ci channel in Slack.","title":"Docker Ossmeter"},{"location":"others/deploy/Docker-Ossmeter/#docker-ossmeter","text":"This page lists information about the Ossmeter docker image. It should be noted that the generated image uses the old Ossmeter binaries and will not be updated -- all new development will go into the Crossminer repository. All images are stored on the Crossminer Docker-hub account . The Docker image is composed of 4 services: oss-web: corresponds to the service of OSSMETER platform website. oss-app: service running api server and the orchestrator of OSSMETER slave instances. oss-slave: service corresponding to the OSSMETER slaves responsible for the analysis of software projects. There can be several slaves serving the same master for load balancing. oss-db: service responsible for the the storage of OSSMETER data. Uses a MongoDB image. The database comes pre-populated with a project and a user. The loaded dump comes from md2manoppello's repo . Login information: user: demo@crossminer.org password: demo18 Custom quality is in the user object (demo@crossminer.org) stored in the users collection of users db. It resembles the demo quality model .","title":"Docker Ossmeter"},{"location":"others/deploy/Docker-Ossmeter/#running-the-ossmeter-docker-image","text":"The easiest way to build the full stack is to run the docker-compose file: $ docker-compose up This command will download the images and run them. The application is then available on localhost:9000 .","title":"Running the Ossmeter docker image"},{"location":"others/deploy/Docker-Ossmeter/#building-the-ossmeter-docker-image","text":"Two containers actually need to be built. They can be built individually.","title":"Building the Ossmeter docker image"},{"location":"others/deploy/Docker-Ossmeter/#oss-platform","text":"Build the image from the oss-platform directory: $ docker build -t bbaldassari/ossmeter-platform . Sending build context to Docker daemon 3.072kB Step 1/5 : FROM openjdk:8-jdk","title":"oss-platform"},{"location":"others/deploy/Docker-Ossmeter/#oss-web","text":"Build the image from the oss-web directory: $ docker build -t bbaldassari/ossmeter-web . Sending build context to Docker daemon 3.072kB Step 1/7 : FROM openjdk:8-jre-alpine","title":"oss-web"},{"location":"others/deploy/Docker-Ossmeter/#continuous-integration","text":"We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the #ci channel in Slack.","title":"Continuous integration"},{"location":"others/deploy/Platform-configuration/","text":"When starting the platform, you can pass a configuration file to control the behaviour of the platform: ./eclipse -slave -config myconfiguration.properties The configuration file is a typical Java properties file. The properties that can be configured are: identifier=<your name> The identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID. If you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers. log.type=console|file|rolling You can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify file or rolling , you must complete the log.file.path or log.rolling.path property as well. If the property is not specified, it will default to the console logger. log.file.path=<path> The path to the file to store the log. E.g. log.file.path=/tmp/lovelylog.log log.rolling.path=<path> The path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify log.rolling.path=/tmp/mylovelylog.log , it will store files like so: /tmp/mylovelylog.log.2014-12-17-00 and /tmp/mylovelylog.log.2014-12-17-12 . maven_executable=<path> The path to where Maven is installed. E.g. maven_executable=/usr/bin/mvn storage_path=<path> The path to where files should be stored. E.g. storage_path=/mnt/ossmeter/ mongo_hosts A comma-separated list of the hosts and ports in a replica set. E.g. ua002:27017,ua009:27017,ua019:27017,ua020:27017","title":"Platform configuration"},{"location":"others/deploy/Platform-configuration/#identifierltyour-namegt","text":"The identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID. If you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers.","title":"identifier=&lt;your name&gt;"},{"location":"others/deploy/Platform-configuration/#logtypeconsolefilerolling","text":"You can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify file or rolling , you must complete the log.file.path or log.rolling.path property as well. If the property is not specified, it will default to the console logger.","title":"log.type=console|file|rolling"},{"location":"others/deploy/Platform-configuration/#logfilepathltpathgt","text":"The path to the file to store the log. E.g. log.file.path=/tmp/lovelylog.log","title":"log.file.path=&lt;path&gt;"},{"location":"others/deploy/Platform-configuration/#logrollingpathltpathgt","text":"The path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify log.rolling.path=/tmp/mylovelylog.log , it will store files like so: /tmp/mylovelylog.log.2014-12-17-00 and /tmp/mylovelylog.log.2014-12-17-12 .","title":"log.rolling.path=&lt;path&gt;"},{"location":"others/deploy/Platform-configuration/#maven_executableltpathgt","text":"The path to where Maven is installed. E.g. maven_executable=/usr/bin/mvn","title":"maven_executable=&lt;path&gt;"},{"location":"others/deploy/Platform-configuration/#storage_pathltpathgt","text":"The path to where files should be stored. E.g. storage_path=/mnt/ossmeter/","title":"storage_path=&lt;path&gt;"},{"location":"others/deploy/Platform-configuration/#mongo_hosts","text":"A comma-separated list of the hosts and ports in a replica set. E.g. ua002:27017,ua009:27017,ua019:27017,ua020:27017","title":"mongo_hosts"},{"location":"others/deploy/Running-the-platform/","text":"Running the platform This is a quick start guide to get the OSSMETER platform running from source. Although these instructions may apply to other versions of Eclipse, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package). A step-by-step video guide is also available at https://youtu.be/3Ry4KKfNdYg Start MongoDB You can download MongoDB from the MongoDb website . Instructions for starting mongo can be found in the MongoDB manual . For example: mongod --dbpath /data/db --port 27017 Get the Code Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev Setup Eclipse Open Eclipse and import all projects from the top level directory of the Scava code ( File -> Import -> Existing projects into workspace ), and wait for all the projects to compile without errors. Validate and Run the Platform Open org.ossmeter.platform.osgi/ossmeterfromfeature.product * Click the Validate... icon in the top right of the product configuration editor (the icon is a piece of paper with a tick) * If things do not validate, there's something wrong -- get in touch :) Problems related to org.eclipse.e4.core.di aren't critical. * Then, click the Export an Eclipse product on the left of the Validate... button. Uncheck the Generate p2 repository checkbox, select a destination directory and validate. After a while, the OSSMETER platform will be generated in the selected directory. * The platform can then be run using the generated eclipse binary; it accepts the following arguments: * -apiServer : Starts up the client API on localhost:8182 * -worker ${id-worker} : Spawns a thread that analyses registered projects * To get a full platform running, first launch a master thread, then a slave, and finally the API server. If you are developing code for the Scava platform, be sure to check out the Contributing . Run the api-gateway Right click on scava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java Then click on Run As -> Java Application Run the authentication service Right click on scava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java Then click on Run As -> Java Application Run the administration dashboard Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart). The following instructions show how to run the dashboard web app: * Enter the administration/scava-administration/ directory within the scava repository. * Run the web app on port 4200 using angular-cli: ng serve * Navigate to http://localhost:4200/","title":"Running the platform"},{"location":"others/deploy/Running-the-platform/#running-the-platform","text":"This is a quick start guide to get the OSSMETER platform running from source. Although these instructions may apply to other versions of Eclipse, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package). A step-by-step video guide is also available at https://youtu.be/3Ry4KKfNdYg","title":"Running the platform"},{"location":"others/deploy/Running-the-platform/#start-mongodb","text":"You can download MongoDB from the MongoDb website . Instructions for starting mongo can be found in the MongoDB manual . For example: mongod --dbpath /data/db --port 27017","title":"Start MongoDB"},{"location":"others/deploy/Running-the-platform/#get-the-code","text":"Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev","title":"Get the Code"},{"location":"others/deploy/Running-the-platform/#setup-eclipse","text":"Open Eclipse and import all projects from the top level directory of the Scava code ( File -> Import -> Existing projects into workspace ), and wait for all the projects to compile without errors.","title":"Setup Eclipse"},{"location":"others/deploy/Running-the-platform/#validate-and-run-the-platform","text":"Open org.ossmeter.platform.osgi/ossmeterfromfeature.product * Click the Validate... icon in the top right of the product configuration editor (the icon is a piece of paper with a tick) * If things do not validate, there's something wrong -- get in touch :) Problems related to org.eclipse.e4.core.di aren't critical. * Then, click the Export an Eclipse product on the left of the Validate... button. Uncheck the Generate p2 repository checkbox, select a destination directory and validate. After a while, the OSSMETER platform will be generated in the selected directory. * The platform can then be run using the generated eclipse binary; it accepts the following arguments: * -apiServer : Starts up the client API on localhost:8182 * -worker ${id-worker} : Spawns a thread that analyses registered projects * To get a full platform running, first launch a master thread, then a slave, and finally the API server. If you are developing code for the Scava platform, be sure to check out the Contributing .","title":"Validate and Run the Platform"},{"location":"others/deploy/Running-the-platform/#run-the-api-gateway","text":"Right click on scava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java Then click on Run As -> Java Application","title":"Run the api-gateway"},{"location":"others/deploy/Running-the-platform/#run-the-authentication-service","text":"Right click on scava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java Then click on Run As -> Java Application","title":"Run the authentication service"},{"location":"others/deploy/Running-the-platform/#run-the-administration-dashboard","text":"Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart). The following instructions show how to run the dashboard web app: * Enter the administration/scava-administration/ directory within the scava repository. * Run the web app on port 4200 using angular-cli: ng serve * Navigate to http://localhost:4200/","title":"Run the administration dashboard"},{"location":"others/development/Component-Naming/","text":"As consequence of our status of project hosted by the eclipse foundation, we have to follow a specific naming schema for all components implemented in context of SCAVA project. In this section, \"component\" means big funcional componen of the SCAVA project. Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration Project Naming For Eclipse Plugin org.eclipse.scava.{componentname} For Maven Project : the name of the project if the ArtifactId If your component is composed of one project : {component-name} If your component is composed of several sub projects : {sub-component-name} ``` ## Source Code Namsespace All sources must be nemspaces by : org.eclipse.scava.{componentname} ## Maven Ids For the Maven projects: * If your component is composed of one project : Group Id : org.eclipse.scava ArtifactId : {component-name} * If your component is composed of several sub projects : Group Id : org.eclipse.scava.{component-name} ArtifactId : {sub-component-name} ```","title":"Component Naming"},{"location":"others/development/Component-Naming/#project-naming","text":"For Eclipse Plugin org.eclipse.scava.{componentname} For Maven Project : the name of the project if the ArtifactId If your component is composed of one project : {component-name} If your component is composed of several sub projects : {sub-component-name} ``` ## Source Code Namsespace All sources must be nemspaces by : org.eclipse.scava.{componentname} ## Maven Ids For the Maven projects: * If your component is composed of one project : Group Id : org.eclipse.scava ArtifactId : {component-name} * If your component is composed of several sub projects : Group Id : org.eclipse.scava.{component-name} ArtifactId : {sub-component-name} ```","title":"Project Naming"},{"location":"others/development/Contributing/","text":"Contributing Subcategories SCAVA Repository Organisation Guideline describing how the SCAVA code repository is organised and how to add a new component in this repository. How to name SCAVA components? Guideline describing naming constraints for a new scava component (component name, java namespace, maven artefact id and group id, etc. How to name SCAVA REST services? This guideline provide naming rules for each REST services routes implemented by the SCAVA platform. How to manage Licensing Guideline describing licensing requirements for SCAVA components. Technical Guidelines REST API Each implemented REST services must be documented (see /users directory): REST API DOCUMENTATION How to configure the SCAVA Gateway in order to integrate a new REST service Customers access SCAVA services through the SCAVA API Gateway. This guideline present how to configure the API Gateway to integrate new remote service provider. How to consume a SCAVA REST services This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues. How to implement Restlet services Guideline which describe how to integrate and implement a REST service in SCAVA platform using the RESTLET framework. DATA ACCESS How to access MongoDB database using PONGO Guideline which describe how to the access to MongoDB database using the PONGO framework. How to extend the SCAVA data model Guideline which describe ways to extend the SCAVA Data Model, stored in a MongoDb database and based on the PONGO framework. OSGI How to integrate OSGI service plugin in SCAVA Architecture Todo How to communicate between OSGI plugin using JMS Todo","title":"Contributing"},{"location":"others/development/Contributing/#contributing","text":"","title":"Contributing"},{"location":"others/development/Contributing/#subcategories","text":"SCAVA Repository Organisation Guideline describing how the SCAVA code repository is organised and how to add a new component in this repository. How to name SCAVA components? Guideline describing naming constraints for a new scava component (component name, java namespace, maven artefact id and group id, etc. How to name SCAVA REST services? This guideline provide naming rules for each REST services routes implemented by the SCAVA platform. How to manage Licensing Guideline describing licensing requirements for SCAVA components.","title":"Subcategories"},{"location":"others/development/Contributing/#technical-guidelines","text":"","title":"Technical Guidelines"},{"location":"others/development/Contributing/#rest-api","text":"Each implemented REST services must be documented (see /users directory): REST API DOCUMENTATION How to configure the SCAVA Gateway in order to integrate a new REST service Customers access SCAVA services through the SCAVA API Gateway. This guideline present how to configure the API Gateway to integrate new remote service provider. How to consume a SCAVA REST services This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues. How to implement Restlet services Guideline which describe how to integrate and implement a REST service in SCAVA platform using the RESTLET framework.","title":"REST API"},{"location":"others/development/Contributing/#data-access","text":"How to access MongoDB database using PONGO Guideline which describe how to the access to MongoDB database using the PONGO framework. How to extend the SCAVA data model Guideline which describe ways to extend the SCAVA Data Model, stored in a MongoDb database and based on the PONGO framework.","title":"DATA ACCESS"},{"location":"others/development/Contributing/#osgi","text":"How to integrate OSGI service plugin in SCAVA Architecture Todo How to communicate between OSGI plugin using JMS Todo","title":"OSGI"},{"location":"others/development/Development-Guidelines/","text":"Development Guidelines Introduction This document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development. Process overview The development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system. It is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests. The different elements shown below are individually described in the remaining of the section. Figure 1. Overview of the CROSSMINER development process and tools Source Code Repository Different branches will be created in the repository. In particular, the master branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The dev branch contains the most recent code integrated by partners before release. Other branches will be created for the different features/components, which will be eventually merged in the dev branch. The name of each branch contains the id of the issue it is supposed to implement. All source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository. Code: https://github.com/crossminer/scava Product documentation: https://github.com/crossminer/scava-docs Product deployment: https://github.com/crossminer/scava-deployment Since all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details). By taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3. Figure 2: Branching model Tests Each branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below. Figure 2: Explanatory master and branches BRANCH: Set of branches TEST: Set of tests INTGRATION_TEST < TEST UNIT_TEST < TEST function test: BRANCH -> TEST test(master) -> test(cool-feature) \u22c3 {itu, \u2026 itz} Continuous Integration Server Continuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus reduce cost and time . A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all unit tests . Integration tests are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities: Allow developers to write unit tests that can be executed automatically Perform automated tests against newly written code Show a list of tests that have passed and failed Quality analysis on source code Perform all the necessary actions to create a fully functioning build of the software when all tests have passed The currently used CI server is available at http://ci5.castalia.camp:8080/ Development and Production environments According to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform. Naming and change conventions The repository will contain the code of the high-level components shown in Table 1 Components Corresponding folder in repository Leader DevOps Dashboard BIT Workflow Diagram Editor /crossflow YORK Administration Web Application /administration SFT IDE /eclipse-based-ide FEA API Gateway /api-gateway SFT DevOps Backend BIT Knowledge Base /knowledge-base UDA Project Analyser /metric-platform SFT Data Collector /metric-platform SFT Project Scheduler /metric-platform SFT Metric Providers /metric-platform YORK, CWI, AUEB, EHU Data Storage /metric-platform SFT Web-based dashboards /web-dashboards BIT Table 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes Figure 2: Scava components architecture For each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus: when developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons; when a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR. The partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder: readme.md : this \ufb01le will contain the entry-point to all documentation for the component. Jenkinsfile : in order to compile the component, run tests, etc on the CI server. test : this is a folder containing the unit and/or integration tests. As a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base The following items are typical practices when applying CI: * Maintain a code repository * Automate the build * Make the build self-testing * Everyone commits to the baseline every day * Every commit (to baseline) should be built * Keep the build fast * Test in a clone of the production environment * Make it easy to get the latest deliverables * Everyone can see the results of the latest build * Automate deployment Communication and collaboration means The development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of: Slack channel: http://crossminer.slack.com Eclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev GitHub repository for documentation: https://github.com/crossminer/scava-docs/ Remark I recall that we are running a research and innovative project. Reviewers that will evaluate our work will not be impressed by the fact that 100% of our tests succeed, that we perfectly adhere to a detailed code convention, etc. Of course, the community will do!!! Thus, we have to find a reasonably compromise: to have all the deliverables accepted !!! to develop in a way, which is preparatory to achieve our \u201cdreams\u201d \u201cBetter have a low test coverage than a part of our tests fail\u201d [cit. Boris] :-)","title":"Development Guidelines"},{"location":"others/development/Development-Guidelines/#development-guidelines","text":"","title":"Development Guidelines"},{"location":"others/development/Development-Guidelines/#introduction","text":"This document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development.","title":"Introduction"},{"location":"others/development/Development-Guidelines/#process-overview","text":"The development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system. It is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests. The different elements shown below are individually described in the remaining of the section. Figure 1. Overview of the CROSSMINER development process and tools","title":"Process overview"},{"location":"others/development/Development-Guidelines/#source-code-repository","text":"Different branches will be created in the repository. In particular, the master branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The dev branch contains the most recent code integrated by partners before release. Other branches will be created for the different features/components, which will be eventually merged in the dev branch. The name of each branch contains the id of the issue it is supposed to implement. All source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository. Code: https://github.com/crossminer/scava Product documentation: https://github.com/crossminer/scava-docs Product deployment: https://github.com/crossminer/scava-deployment Since all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details). By taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3. Figure 2: Branching model","title":"Source Code Repository"},{"location":"others/development/Development-Guidelines/#tests","text":"Each branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below. Figure 2: Explanatory master and branches BRANCH: Set of branches TEST: Set of tests INTGRATION_TEST < TEST UNIT_TEST < TEST function test: BRANCH -> TEST test(master) -> test(cool-feature) \u22c3 {itu, \u2026 itz}","title":"Tests"},{"location":"others/development/Development-Guidelines/#continuous-integration-server","text":"Continuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus reduce cost and time . A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all unit tests . Integration tests are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities: Allow developers to write unit tests that can be executed automatically Perform automated tests against newly written code Show a list of tests that have passed and failed Quality analysis on source code Perform all the necessary actions to create a fully functioning build of the software when all tests have passed The currently used CI server is available at http://ci5.castalia.camp:8080/","title":"Continuous Integration Server"},{"location":"others/development/Development-Guidelines/#development-and-production-environments","text":"According to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform.","title":"Development and Production environments"},{"location":"others/development/Development-Guidelines/#naming-and-change-conventions","text":"The repository will contain the code of the high-level components shown in Table 1 Components Corresponding folder in repository Leader DevOps Dashboard BIT Workflow Diagram Editor /crossflow YORK Administration Web Application /administration SFT IDE /eclipse-based-ide FEA API Gateway /api-gateway SFT DevOps Backend BIT Knowledge Base /knowledge-base UDA Project Analyser /metric-platform SFT Data Collector /metric-platform SFT Project Scheduler /metric-platform SFT Metric Providers /metric-platform YORK, CWI, AUEB, EHU Data Storage /metric-platform SFT Web-based dashboards /web-dashboards BIT Table 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes Figure 2: Scava components architecture For each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus: when developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons; when a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR. The partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder: readme.md : this \ufb01le will contain the entry-point to all documentation for the component. Jenkinsfile : in order to compile the component, run tests, etc on the CI server. test : this is a folder containing the unit and/or integration tests. As a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base The following items are typical practices when applying CI: * Maintain a code repository * Automate the build * Make the build self-testing * Everyone commits to the baseline every day * Every commit (to baseline) should be built * Keep the build fast * Test in a clone of the production environment * Make it easy to get the latest deliverables * Everyone can see the results of the latest build * Automate deployment","title":"Naming and change conventions"},{"location":"others/development/Development-Guidelines/#communication-and-collaboration-means","text":"The development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of: Slack channel: http://crossminer.slack.com Eclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev GitHub repository for documentation: https://github.com/crossminer/scava-docs/","title":"Communication and collaboration means"},{"location":"others/development/Development-Guidelines/#remark","text":"I recall that we are running a research and innovative project. Reviewers that will evaluate our work will not be impressed by the fact that 100% of our tests succeed, that we perfectly adhere to a detailed code convention, etc. Of course, the community will do!!! Thus, we have to find a reasonably compromise: to have all the deliverables accepted !!! to develop in a way, which is preparatory to achieve our \u201cdreams\u201d \u201cBetter have a low test coverage than a part of our tests fail\u201d [cit. Boris] :-)","title":"Remark"},{"location":"others/development/Extend-MongoDB-Data-Model/","text":"Extending MongoDB data model When to use ? In this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order the data layer of Scava platform during the evolution. Context The Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class. Each MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model. The current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins : The Platform data model (org.ossmeter.repository.model) : Contains data related to project administration , metric execution process and authentification system. Source Code Repositroy managers (org.ossmeter.repository.model.'project delta manager') : Contains configuration informations related to source codes managemeny tools. Metric Providers data models (in each metric provider plugins) : Each metric provider implementation contains his once data model. You need to Extend an Existing Data Model ? The first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same. 1. Locate the *.emf file of this data model A presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model. Ex : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf) 2. Update the Data Model description A data model description file contains a statical description of a MongoDb document. @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . attr String stackTrace; } ... If we would like to add one more attribute to the element ProjectError , we could add it this way : @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . . . attr String stackTrace; + attr String TestAttribute; } ... You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines 3. Generate the Java Class using the Pongo Tool. Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Replace the existing Java class by the new generated java class. More information about Pongo : https://github.com/kolovos/pongo/wiki You need to Create a new Data Model ? The second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model. In this case, we invite you to create a new plugin which will contain your data model. 1. Create a new Eclipse Plug-In Create a new Eclipse Plug-In Project ( In Eclipse Toolbar : File > New > Plug-In Project Name of the project : org.scava. mycomponent .repository.model Disable the generation of an Activator Class / contribution to the ui Edit the MANIFEST.MF file In Dependency : add a dependency to the org.eclipse.core.runtime plugin In Dependency : add a dependency to the com.googlecode.pongo.runtime plugin In Dependency : add a dependency to the org.apache.commons.lang3 plugin In Extentions : reference an extension point named com.googlecode.pongo.runtime.osgi In source directory Create a package named org.scava. mycomponent .repository.model In this package create an emf file named mycomponent.emf A presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file : ```package org.scava.mycomponent.repository.model; @db class MyComponent { .... } ``` You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines 2. Generate the Java Class using the Pongo Tool. Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Add this class in your org.scava. mycomponent .repository.model package More information about Pongo : https://github.com/kolovos/pongo/wiki Comment Here we learnt ways to modify model in the Scava platform. To know more about the access of data with the Pongo APIs link here .","title":"Extending MongoDB data model"},{"location":"others/development/Extend-MongoDB-Data-Model/#extending-mongodb-data-model","text":"","title":"Extending MongoDB data model"},{"location":"others/development/Extend-MongoDB-Data-Model/#when-to-use","text":"In this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order the data layer of Scava platform during the evolution.","title":"When to use ?"},{"location":"others/development/Extend-MongoDB-Data-Model/#context","text":"The Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class. Each MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model. The current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins : The Platform data model (org.ossmeter.repository.model) : Contains data related to project administration , metric execution process and authentification system. Source Code Repositroy managers (org.ossmeter.repository.model.'project delta manager') : Contains configuration informations related to source codes managemeny tools. Metric Providers data models (in each metric provider plugins) : Each metric provider implementation contains his once data model.","title":"Context"},{"location":"others/development/Extend-MongoDB-Data-Model/#you-need-to-extend-an-existing-data-model","text":"The first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same.","title":"You need to Extend an Existing Data Model ?"},{"location":"others/development/Extend-MongoDB-Data-Model/#1-locate-the-emf-file-of-this-data-model","text":"A presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model. Ex : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf)","title":"1. Locate the *.emf file of this data model"},{"location":"others/development/Extend-MongoDB-Data-Model/#2-update-the-data-model-description","text":"A data model description file contains a statical description of a MongoDb document. @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . attr String stackTrace; } ... If we would like to add one more attribute to the element ProjectError , we could add it this way : @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . . . attr String stackTrace; + attr String TestAttribute; } ... You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines","title":"2. Update the Data  Model description"},{"location":"others/development/Extend-MongoDB-Data-Model/#3-generate-the-java-class-using-the-pongo-tool","text":"Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Replace the existing Java class by the new generated java class. More information about Pongo : https://github.com/kolovos/pongo/wiki","title":"3. Generate the Java Class using the Pongo Tool."},{"location":"others/development/Extend-MongoDB-Data-Model/#you-need-to-create-a-new-data-model","text":"The second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model. In this case, we invite you to create a new plugin which will contain your data model.","title":"You need to Create a new Data Model ?"},{"location":"others/development/Extend-MongoDB-Data-Model/#1-create-a-new-eclipse-plug-in","text":"Create a new Eclipse Plug-In Project ( In Eclipse Toolbar : File > New > Plug-In Project Name of the project : org.scava. mycomponent .repository.model Disable the generation of an Activator Class / contribution to the ui Edit the MANIFEST.MF file In Dependency : add a dependency to the org.eclipse.core.runtime plugin In Dependency : add a dependency to the com.googlecode.pongo.runtime plugin In Dependency : add a dependency to the org.apache.commons.lang3 plugin In Extentions : reference an extension point named com.googlecode.pongo.runtime.osgi In source directory Create a package named org.scava. mycomponent .repository.model In this package create an emf file named mycomponent.emf A presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file : ```package org.scava.mycomponent.repository.model; @db class MyComponent { .... } ``` You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines","title":"1. Create a new Eclipse Plug-In"},{"location":"others/development/Extend-MongoDB-Data-Model/#2-generate-the-java-class-using-the-pongo-tool","text":"Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Add this class in your org.scava. mycomponent .repository.model package More information about Pongo : https://github.com/kolovos/pongo/wiki","title":"2. Generate the Java Class using the Pongo Tool."},{"location":"others/development/Extend-MongoDB-Data-Model/#comment","text":"Here we learnt ways to modify model in the Scava platform. To know more about the access of data with the Pongo APIs link here .","title":"Comment"},{"location":"others/development/How-To-Develop-Metric-Provider/","text":"How to Develop a Metric Provider In this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a transient metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a historical metric provider, will count the total number of commits over time. We'll go through the steps above for each metric provider. Pre-requisites Eclipse The Emfatic plug-in should be installed in your Eclipse The Pongo plug-in should be installed in your Eclipse The OSSMETER source code should be in your workspace The Transient Metric Provider This metric provider will store a complete history of the commits in the version control system(s) used by a project. 0. Setup Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list. 1. The data model We define the data model using the Emfatic language. In your newly created plug-in, create a package called org.ossmeter.metricprovider.trans.commits.model . In that package create an empty file called commits.emf . In this file, we will define our data model. First of all, we need to state the name of the package. package org.ossmeter.metricprovider.trans.commits.model; This is used by the Pongo code generator - the generated classes will be put in this package. We then define the database for our model: @db(qualifiedCollectionNames=\"true\") class Commits { val Repository[*] repositories; val Commit[*] commits; } The @db annotation tells Pongo that this will be the container database for the data. Adding the qualifiedCollectionNames=true property will prepend the database name to all Mongo collections. The Commits class above says that we want a database with two collections, name repositories and commits . If qualifiedCollectionNames is set to true , the collections will be named Commits.repositories and Commits.commits . We now define the schema of the Commits.repositories collection: ~~~java class Repository { @searchable attr String url; attr String repoType; attr String revision; attr int totalNumberOfCommits; ref CommitData[*] commits; } This class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection. The `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time. Each commit is represented in the `Commits.commits` collection by the following model: ~~~~java class Commit { @searchable attr Date date; attr String identifier; attr String message; attr String author; } For each commit, we store its date , identifier (revision ID), the commit message , and the author . We also create an index on the date to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated findByDate(String date) or findOneByDate(String date) methods). Now we need to use Pongo to generate code from this model. Right-click on the commits.emf file and select Pongo > Generate Pongos and plugin.xml . You should see some Java classes appear in your package. Now that we have our data model, we can implement the metric provider. 2. The metric provider Create a Java class called org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider This class should extend AbstractTransientMetricProvider and specify Commits for the generic argument: public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits> The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. adapt(DB db) This method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows: @Override public Commits adapt(DB db) { return new Commits(db); } The next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider: @Override public String getShortIdentifier() { // This may be deprecated very soon return \"transient-commits\"; } @Override public String getFriendlyName() { return \"Commit History\"; } @Override public String getSummaryInformation() { return \"The commit history of the project.\"; } The next method allows you to declare whether the metric provider is applicable to a given project: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } Our metric applies to any project that has at least one VCS repository. Finally, we have the measure(...) method that performs the actual metric calculation: @Override public void measure(Project project, ProjectDelta delta, Commits db) { for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) { Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl()); if (repo == null) { repo = new Repository(); repo.setUrl(repoDelta.getRepository().getUrl()); db.getRepositories().add(repo); } for (VcsCommit commit : repoDelta.getCommits()) { Commit c = new Commit(); c.setDate(commit.getJavaDate()); c.setMessage(commit.getMessage()); c.setAuthor(commit.getAuthor()); c.setIdentifier(commit.getRevision()); repo.getCommits().add(c); db.getCommits().add(c); } } db.getCommits().sync(); db.getRepositories().sync(); } The above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new Commit object is created for each commit in the delta. 3. Make the metric provider discoverable Metric providers are registered with the OSSMETER platform using extension points : Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsTransientMetricProvider' class Now everything is ready for the metric to be executed :) The Historic Metric Provider This metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value. 0. Setup Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list. 1. The data model In your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model: package org.ossmeter.metricprovider.historic.commits.model; class HistoricCommits { attr int numberOfCommits; } The data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed. 2. The metric provider Create a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider This class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)): public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. First of all, complete the typical information-related methods: @Override public String getShortIdentifier() { return \"historicalcommits\"; } @Override public String getFriendlyName() { return \"Historical commits\"; } @Override public String getSummaryInformation() { return \"...\"; } Now complete the standard appliesTo method: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } We now need to specify a dependency on the transient metric provider that we just implemented. @Override public List<String> getIdentifiersOfUses() { return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName()); } This tells the platform that we need access to the CommitsTransientMetricProvider database. The platform will assign this to the uses field that is available to the historical metric provider, as you'll see in the measure method: @Override public Pongo measure(Project project) { Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0)); int commits = (int) transDb.getCommits().size(); HistoricCommits hist = new HistoricCommits(); hist.setNumberOfCommits(commits); return hist; } First of all, we get hold of the database of the transient commits metric provider, and simply count the size of the commits collection. We save this in an instance of the HistoricCommits Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project. 3. Make the metric provider discoverable This process is the same as the transient metric provider: Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class Now everything is ready for both metrics to be executed :) But first. Let's specify how we want this historical metric to be visualised. 4. Define a MetVis visualisation specification MetVis is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis web page has numerous examples of this. Create a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider: { \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\", \"vis\" : [ { \"id\" : \"historicalcommits\", \"name\" : \"Commits over time\", \"description\" : \"This metric shows when the projects commits occurred\", \"type\" : \"LineChart\", \"datatable\" : { \"cols\" : [ { \"name\" : \"Date\", \"field\" : \"$__date\" }, { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" } ] }, \"x\" : \"Date\", \"y\" : \"Commits\" } ] } The metricId field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use $__date . The final two fields ( x and y ) are references to column names in the datatable specification. The type states that the data should be plotted as a line chart. You can test your MetVis specifications on the MetVis playpen . 5. Make the visualisation specification discoverable As with metric providers, visualisation specifications are registered using extension points. Add the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.visualisation.metric On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'historicalcommits.json' file Good job. Running the metric providers See Running from Source Homework Adapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.","title":"How to Develop a Metric Provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#how-to-develop-a-metric-provider","text":"In this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a transient metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a historical metric provider, will count the total number of commits over time. We'll go through the steps above for each metric provider.","title":"How to Develop a Metric Provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#pre-requisites","text":"Eclipse The Emfatic plug-in should be installed in your Eclipse The Pongo plug-in should be installed in your Eclipse The OSSMETER source code should be in your workspace","title":"Pre-requisites"},{"location":"others/development/How-To-Develop-Metric-Provider/#the-transient-metric-provider","text":"This metric provider will store a complete history of the commits in the version control system(s) used by a project.","title":"The Transient Metric Provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#0-setup","text":"Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.","title":"0. Setup"},{"location":"others/development/How-To-Develop-Metric-Provider/#1-the-data-model","text":"We define the data model using the Emfatic language. In your newly created plug-in, create a package called org.ossmeter.metricprovider.trans.commits.model . In that package create an empty file called commits.emf . In this file, we will define our data model. First of all, we need to state the name of the package. package org.ossmeter.metricprovider.trans.commits.model; This is used by the Pongo code generator - the generated classes will be put in this package. We then define the database for our model: @db(qualifiedCollectionNames=\"true\") class Commits { val Repository[*] repositories; val Commit[*] commits; } The @db annotation tells Pongo that this will be the container database for the data. Adding the qualifiedCollectionNames=true property will prepend the database name to all Mongo collections. The Commits class above says that we want a database with two collections, name repositories and commits . If qualifiedCollectionNames is set to true , the collections will be named Commits.repositories and Commits.commits . We now define the schema of the Commits.repositories collection: ~~~java class Repository { @searchable attr String url; attr String repoType; attr String revision; attr int totalNumberOfCommits; ref CommitData[*] commits; } This class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection. The `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time. Each commit is represented in the `Commits.commits` collection by the following model: ~~~~java class Commit { @searchable attr Date date; attr String identifier; attr String message; attr String author; } For each commit, we store its date , identifier (revision ID), the commit message , and the author . We also create an index on the date to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated findByDate(String date) or findOneByDate(String date) methods). Now we need to use Pongo to generate code from this model. Right-click on the commits.emf file and select Pongo > Generate Pongos and plugin.xml . You should see some Java classes appear in your package. Now that we have our data model, we can implement the metric provider.","title":"1. The data model"},{"location":"others/development/How-To-Develop-Metric-Provider/#2-the-metric-provider","text":"Create a Java class called org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider This class should extend AbstractTransientMetricProvider and specify Commits for the generic argument: public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits> The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn.","title":"2. The metric provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#adaptdb-db","text":"This method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows: @Override public Commits adapt(DB db) { return new Commits(db); } The next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider: @Override public String getShortIdentifier() { // This may be deprecated very soon return \"transient-commits\"; } @Override public String getFriendlyName() { return \"Commit History\"; } @Override public String getSummaryInformation() { return \"The commit history of the project.\"; } The next method allows you to declare whether the metric provider is applicable to a given project: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } Our metric applies to any project that has at least one VCS repository. Finally, we have the measure(...) method that performs the actual metric calculation: @Override public void measure(Project project, ProjectDelta delta, Commits db) { for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) { Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl()); if (repo == null) { repo = new Repository(); repo.setUrl(repoDelta.getRepository().getUrl()); db.getRepositories().add(repo); } for (VcsCommit commit : repoDelta.getCommits()) { Commit c = new Commit(); c.setDate(commit.getJavaDate()); c.setMessage(commit.getMessage()); c.setAuthor(commit.getAuthor()); c.setIdentifier(commit.getRevision()); repo.getCommits().add(c); db.getCommits().add(c); } } db.getCommits().sync(); db.getRepositories().sync(); } The above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new Commit object is created for each commit in the delta.","title":"adapt(DB db)"},{"location":"others/development/How-To-Develop-Metric-Provider/#3-make-the-metric-provider-discoverable","text":"Metric providers are registered with the OSSMETER platform using extension points : Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsTransientMetricProvider' class Now everything is ready for the metric to be executed :)","title":"3. Make the metric provider discoverable"},{"location":"others/development/How-To-Develop-Metric-Provider/#the-historic-metric-provider","text":"This metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value.","title":"The Historic Metric Provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#0-setup_1","text":"Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.","title":"0. Setup"},{"location":"others/development/How-To-Develop-Metric-Provider/#1-the-data-model_1","text":"In your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model: package org.ossmeter.metricprovider.historic.commits.model; class HistoricCommits { attr int numberOfCommits; } The data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed.","title":"1. The data model"},{"location":"others/development/How-To-Develop-Metric-Provider/#2-the-metric-provider_1","text":"Create a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider This class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)): public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. First of all, complete the typical information-related methods: @Override public String getShortIdentifier() { return \"historicalcommits\"; } @Override public String getFriendlyName() { return \"Historical commits\"; } @Override public String getSummaryInformation() { return \"...\"; } Now complete the standard appliesTo method: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } We now need to specify a dependency on the transient metric provider that we just implemented. @Override public List<String> getIdentifiersOfUses() { return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName()); } This tells the platform that we need access to the CommitsTransientMetricProvider database. The platform will assign this to the uses field that is available to the historical metric provider, as you'll see in the measure method: @Override public Pongo measure(Project project) { Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0)); int commits = (int) transDb.getCommits().size(); HistoricCommits hist = new HistoricCommits(); hist.setNumberOfCommits(commits); return hist; } First of all, we get hold of the database of the transient commits metric provider, and simply count the size of the commits collection. We save this in an instance of the HistoricCommits Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project.","title":"2. The metric provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#3-make-the-metric-provider-discoverable_1","text":"This process is the same as the transient metric provider: Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class Now everything is ready for both metrics to be executed :) But first. Let's specify how we want this historical metric to be visualised.","title":"3. Make the metric provider discoverable"},{"location":"others/development/How-To-Develop-Metric-Provider/#4-define-a-metvis-visualisation-specification","text":"MetVis is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis web page has numerous examples of this. Create a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider: { \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\", \"vis\" : [ { \"id\" : \"historicalcommits\", \"name\" : \"Commits over time\", \"description\" : \"This metric shows when the projects commits occurred\", \"type\" : \"LineChart\", \"datatable\" : { \"cols\" : [ { \"name\" : \"Date\", \"field\" : \"$__date\" }, { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" } ] }, \"x\" : \"Date\", \"y\" : \"Commits\" } ] } The metricId field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use $__date . The final two fields ( x and y ) are references to column names in the datatable specification. The type states that the data should be plotted as a line chart. You can test your MetVis specifications on the MetVis playpen .","title":"4. Define a MetVis visualisation specification"},{"location":"others/development/How-To-Develop-Metric-Provider/#5-make-the-visualisation-specification-discoverable","text":"As with metric providers, visualisation specifications are registered using extension points. Add the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.visualisation.metric On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'historicalcommits.json' file Good job.","title":"5. Make the visualisation specification discoverable"},{"location":"others/development/How-To-Develop-Metric-Provider/#running-the-metric-providers","text":"See Running from Source","title":"Running the metric providers"},{"location":"others/development/How-To-Develop-Metric-Provider/#homework","text":"Adapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.","title":"Homework"},{"location":"others/development/Implementing-Restlet-Service/","text":"Implementing RESTLET services When to use this guideline ? This guideline present how to create a new REST service using the RESTLET framework in the Scava platform. Context Scava project manages REST services with the RESTLET framework. The usage of Restlet framework has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin : org.eclipse.crossmeter.platform.client.api. The RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in. You want to access to create a new REST Service ? 1. Create a new Route To register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service. Naming the Route The routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service : Naming-Scava-REST-Services.html Register the Route The org.scava.platform.services plug-in contained the class PlatformRoute.java responsible for declaring routes. package org.scava.platform.services; import org.restlet.Application; import org.restlet.Restlet; import org.restlet.routing.Router; public class PlatformRoute extends Application { @Override public Restlet createInboundRoot() { Router router = new Router(getContext()); router.attach(\"/\", PingResource.class); router.attach(\"/search\", SearchProjectResource.class); ... router.attach(\"/projects/p/{projectid}\", ProjectResource.class); router.attach(\"/raw/metrics\", RawMetricListResource.class); ... return router; } } Route Example : router.attach(\"/raw/metrics\", RawMetricListResource.class); \"/raw/metrics\" : Represent the route URL. \"RawMetricListResource.class\" : Represent the class where the service to be implemented for this path. A route can contained some parameters. In this case, parameters are identified by a name with curly brackets {} . 2. Implement the Service A service implementation is a Java class which extend the ServerResource class provided by the RESTLET framework. To create a new service create a new Class : * Named \" ServiceName \" + Resource. Ex : ProjectCreationResource.java * On a namespace based on the route. Ex : org.scava.platform.services.administration for platform administration services. * Who extend the org.restlet.resource.ServerResource class. GET Service To implement a service of type GET, create a new method : * Based on the following signature : public final Representation represent() * Add the @Get(\"json\") annotation @Get(\"json\") public final Representation represent() { // Initialise Response Header Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\"); if (responseHeaders == null) { responseHeaders = new Series(Header.class); getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders); } responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\")); responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\")); // Get Route parameter if required {projectid} String projectId = (String) getRequest().getAttributes().get(\"projectid\"); try { .... // Provide Result getResponse().setStatus(Status.SUCCESS_OK); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } You can also extend the AbstractApiResource , a service class provided by the platform and dedicate to services who request a connection to MongoDb database instead of the ServerResource . In this case you will have to implement the doRepresent() method. public class RawMetricListResource extends AbstractApiResource { public Representation doRepresent() { ObjectNode res = mapper.createObjectNode(); ArrayNode metrics = mapper.createArrayNode(); res.put(\"metrics\", metrics); ... return Util.createJsonRepresentation(res); } } POST Service To implement a service of type POST, crate a new method : * Based on the following signature : public Representation myServiceName (Representation entity) * Add the @Post annotation @Post public Representation myServiceName(Representation entity) { try { // Read Json Datas JsonNode json = mapper.readTree(entity.getText()); ... // Provide Result getResponse().setStatus(Status.SUCCESS_CREATED); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } DELETE Service To do .... 3. Document the Service The REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to allow an easy integration, this contract must be documented : 4. Test the Service To do .... Comment","title":"Implementing RESTLET services"},{"location":"others/development/Implementing-Restlet-Service/#implementing-restlet-services","text":"","title":"Implementing RESTLET services"},{"location":"others/development/Implementing-Restlet-Service/#when-to-use-this-guideline","text":"This guideline present how to create a new REST service using the RESTLET framework in the Scava platform.","title":"When to use this guideline ?"},{"location":"others/development/Implementing-Restlet-Service/#context","text":"Scava project manages REST services with the RESTLET framework. The usage of Restlet framework has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin : org.eclipse.crossmeter.platform.client.api. The RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in.","title":"Context"},{"location":"others/development/Implementing-Restlet-Service/#you-want-to-access-to-create-a-new-rest-service","text":"","title":"You want to access to create a new REST Service ?"},{"location":"others/development/Implementing-Restlet-Service/#1-create-a-new-route","text":"To register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service.","title":"1. Create a new Route"},{"location":"others/development/Implementing-Restlet-Service/#naming-the-route","text":"The routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service : Naming-Scava-REST-Services.html","title":"Naming the Route"},{"location":"others/development/Implementing-Restlet-Service/#register-the-route","text":"The org.scava.platform.services plug-in contained the class PlatformRoute.java responsible for declaring routes. package org.scava.platform.services; import org.restlet.Application; import org.restlet.Restlet; import org.restlet.routing.Router; public class PlatformRoute extends Application { @Override public Restlet createInboundRoot() { Router router = new Router(getContext()); router.attach(\"/\", PingResource.class); router.attach(\"/search\", SearchProjectResource.class); ... router.attach(\"/projects/p/{projectid}\", ProjectResource.class); router.attach(\"/raw/metrics\", RawMetricListResource.class); ... return router; } } Route Example : router.attach(\"/raw/metrics\", RawMetricListResource.class); \"/raw/metrics\" : Represent the route URL. \"RawMetricListResource.class\" : Represent the class where the service to be implemented for this path. A route can contained some parameters. In this case, parameters are identified by a name with curly brackets {} .","title":"Register the Route"},{"location":"others/development/Implementing-Restlet-Service/#2-implement-the-service","text":"A service implementation is a Java class which extend the ServerResource class provided by the RESTLET framework. To create a new service create a new Class : * Named \" ServiceName \" + Resource. Ex : ProjectCreationResource.java * On a namespace based on the route. Ex : org.scava.platform.services.administration for platform administration services. * Who extend the org.restlet.resource.ServerResource class.","title":"2. Implement the Service"},{"location":"others/development/Implementing-Restlet-Service/#get-service","text":"To implement a service of type GET, create a new method : * Based on the following signature : public final Representation represent() * Add the @Get(\"json\") annotation @Get(\"json\") public final Representation represent() { // Initialise Response Header Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\"); if (responseHeaders == null) { responseHeaders = new Series(Header.class); getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders); } responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\")); responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\")); // Get Route parameter if required {projectid} String projectId = (String) getRequest().getAttributes().get(\"projectid\"); try { .... // Provide Result getResponse().setStatus(Status.SUCCESS_OK); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } You can also extend the AbstractApiResource , a service class provided by the platform and dedicate to services who request a connection to MongoDb database instead of the ServerResource . In this case you will have to implement the doRepresent() method. public class RawMetricListResource extends AbstractApiResource { public Representation doRepresent() { ObjectNode res = mapper.createObjectNode(); ArrayNode metrics = mapper.createArrayNode(); res.put(\"metrics\", metrics); ... return Util.createJsonRepresentation(res); } }","title":"GET Service"},{"location":"others/development/Implementing-Restlet-Service/#post-service","text":"To implement a service of type POST, crate a new method : * Based on the following signature : public Representation myServiceName (Representation entity) * Add the @Post annotation @Post public Representation myServiceName(Representation entity) { try { // Read Json Datas JsonNode json = mapper.readTree(entity.getText()); ... // Provide Result getResponse().setStatus(Status.SUCCESS_CREATED); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } }","title":"POST Service"},{"location":"others/development/Implementing-Restlet-Service/#delete-service","text":"To do ....","title":"DELETE Service"},{"location":"others/development/Implementing-Restlet-Service/#3-document-the-service","text":"The REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to allow an easy integration, this contract must be documented :","title":"3. Document the Service"},{"location":"others/development/Implementing-Restlet-Service/#4-test-the-service","text":"To do ....","title":"4. Test the Service"},{"location":"others/development/Implementing-Restlet-Service/#comment","text":"","title":"Comment"},{"location":"others/development/Licensing/","text":"Licencing for Scava Content The Scava project is licensed under Eclipse Public License - v 2.0 license. As consequence of our status of project hosted by the eclipse foundation, all Scava components must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header. \"Eclipse Public License\" licensing file The text below must be integrated to the root folder of your project on a text file name \"LICENSE\". Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership. Source File header All sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...). Optionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization. Example of Java license header file /******************************************************************************* * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"} * This program and the accompanying materials are made * available under the terms of the Eclipse Public License 2.0 * which is available at https://www.eclipse.org/legal/epl-2.0/ * * SPDX-License-Identifier: EPL-2.0 ******************************************************************************/ Comment n Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files. This plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.","title":"Licencing for Scava"},{"location":"others/development/Licensing/#licencing-for-scava","text":"","title":"Licencing for Scava"},{"location":"others/development/Licensing/#content","text":"The Scava project is licensed under Eclipse Public License - v 2.0 license. As consequence of our status of project hosted by the eclipse foundation, all Scava components must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header.","title":"Content"},{"location":"others/development/Licensing/#eclipse-public-license-licensing-file","text":"The text below must be integrated to the root folder of your project on a text file name \"LICENSE\". Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership.","title":"\"Eclipse Public License\" licensing file"},{"location":"others/development/Licensing/#source-file-header","text":"All sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...). Optionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization. Example of Java license header file /******************************************************************************* * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"} * This program and the accompanying materials are made * available under the terms of the Eclipse Public License 2.0 * which is available at https://www.eclipse.org/legal/epl-2.0/ * * SPDX-License-Identifier: EPL-2.0 ******************************************************************************/","title":"Source File header"},{"location":"others/development/Licensing/#comment","text":"n Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files. This plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.","title":"Comment"},{"location":"others/development/Naming-Scava-REST-Services/","text":"Naming Scava REST services When to use this guideline ? This guideline present how to define the route of a new REST service provided by the Scava platform. Context The REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of platform services , we need to used a common naming schema for all REST services provided by the platform. How to name a REST service ? /{ componentid }/{ categoryname }/{ servicename } / componentid / : Name of the Architectural component which provide the service / categoryname / (Optional) : optional category of the service / servicename / : Name of the rest service Component Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration Comment","title":"Naming Scava REST services"},{"location":"others/development/Naming-Scava-REST-Services/#naming-scava-rest-services","text":"","title":"Naming Scava REST services"},{"location":"others/development/Naming-Scava-REST-Services/#when-to-use-this-guideline","text":"This guideline present how to define the route of a new REST service provided by the Scava platform.","title":"When to use this guideline ?"},{"location":"others/development/Naming-Scava-REST-Services/#context","text":"The REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of platform services , we need to used a common naming schema for all REST services provided by the platform.","title":"Context"},{"location":"others/development/Naming-Scava-REST-Services/#how-to-name-a-rest-service","text":"/{ componentid }/{ categoryname }/{ servicename } / componentid / : Name of the Architectural component which provide the service / categoryname / (Optional) : optional category of the service / servicename / : Name of the rest service","title":"How to name a REST service ?"},{"location":"others/development/Naming-Scava-REST-Services/#component","text":"Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration","title":"Component"},{"location":"others/development/Naming-Scava-REST-Services/#comment","text":"","title":"Comment"},{"location":"others/development/Repository-Organisation/","text":"The SCAVA code repository is organized by functional components with one package for each of this components. General organisation metric-platform platform : Core projects of the metric platform platform-extensions : Extensions of the metric platform metric-providers : Metric Providers implementations projects factoids : Factoids implementations projects tests : Test projects related to the metric-platform web-dashboards : DevOps Dashboard and DevOpsDashboard components knowledge-base : Knowledge Base implementation Workflow Execution Engine : Workflow Execution Engine implementation eclipse-based-ide : SCAVA Eclipse plugin api-gateway : API Gateway implementation. administration : Administration component implementation mingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project. Comments","title":"Repository Organisation"},{"location":"others/development/Repository-Organisation/#general-organisation","text":"metric-platform platform : Core projects of the metric platform platform-extensions : Extensions of the metric platform metric-providers : Metric Providers implementations projects factoids : Factoids implementations projects tests : Test projects related to the metric-platform web-dashboards : DevOps Dashboard and DevOpsDashboard components knowledge-base : Knowledge Base implementation Workflow Execution Engine : Workflow Execution Engine implementation eclipse-based-ide : SCAVA Eclipse plugin api-gateway : API Gateway implementation. administration : Administration component implementation mingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project.","title":"General organisation"},{"location":"others/development/Repository-Organisation/#comments","text":"","title":"Comments"},{"location":"others/development/Testing-Guidelines/","text":"Knowledge Base This builds needs some configuration to run successfully. In the application.properties files: * /knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties * /knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties Edit the following parameters: lucene.index.folder=/tmp/scava_lucene/ egit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57 To generate the GitHub access token you need to go to your own GitHub account and create a new one. Once it's done simply restart the tests, they should pass.","title":"Testing Guidelines"},{"location":"others/development/Testing-Guidelines/#knowledge-base","text":"This builds needs some configuration to run successfully. In the application.properties files: * /knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties * /knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties Edit the following parameters: lucene.index.folder=/tmp/scava_lucene/ egit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57 To generate the GitHub access token you need to go to your own GitHub account and create a new one. Once it's done simply restart the tests, they should pass.","title":"Knowledge Base"},{"location":"others/resources/indexing_guide/","text":"Indexing Metrics Guide This guide describes the indexing metric providers available in Scava platform. It provides the mapping so that users can query the index. org.eclipse.scava.metricprovider.indexing.bugs Short name : bug indexing metric Friendly name : Bugs tracking system indexer This metric prepares and indexes analyses documents relating to bug tracking system. Mapping Information : bug.comment String comment_Id String body List<String> emotional_dimension String sentiment String plain_text String request_reply_classification String content_class boolean contains_code String bug_Id String project_name String creator Date created_at String uid List<String> commits List<String> referring_to bug.post Date created_at String bug_summary String severity String bug_id String project_name String creator String uid boolean migration_issue List<String> problematic_changes double matching_score org.eclipse.scava.metricprovider.indexing.commits Short name : metricprovider.indexing.commits Friendly name : Commits indexer This metric prepares and indexes documents related to commits. Mapping Information : commits Date created_at String project_name String uid String repository String revision String author String author_email String body String plain_text List<String> commits_references List<String> bugs_references org.eclipse.scava.metricprovider.indexing.communicationchannels Short name : communication channels indexing metric Friendly name : communication channels indexer This metric prepares and indexes documents relating to communication channels. Mapping Information : article Long article_Id String communication_channel_id String uid List<Integer> thread_id String project_name String message_body String subject String creator Date created_at List<String> emotional_dimension String sentiment String plain_text String request_reply_classification String content_class boolean contains_code thread String communication_channel_id String uid int thread_id String project_name String subject boolean migration_issue List<String> problematic_changes double matching_score org.eclipse.scava.metricprovider.indexing.documentation Short name : communication channels indexing metric Friendly name : communication channels indexer This metric prepares and indexes documents relating to communication channels. Mapping Information : documentation_entry Date created_at String project_name String uid String documentation_id String documentation_entry_id String body String original_format_mime String sentiment String plain_text double readability List<String> documentation_type boolean licence_found String licence documentation Date last_update String uid String project_name String documentation_id List<String> documentation_entries Query Examples Here we present a series of examples of how you can use ElasticSearch queries for finding specific data or doing some aggregations. More information about ElasticSearch queries can be found at https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html Example 1 Retrieve the documentation entries that have been determined containing a Getting Started guide. GET documentation.documentation.entry.nlp/_search { \"query\": { \"match\" : { \"documentation_types\": \"Started\" } } } As documentation_types is an array, we need to use a match within the query. Example 2 Retrieve the documentation entries that have at least one of the following characteristics: - Are a Getting Started guide - Are a Development guide - Have a neutral sentiment GET documentation.documentation.entry.nlp/_search { \"query\": { \"bool\": { \"should\": [ { \"match\" : { \"documentation_types\": \"Started\" } }, { \"match\" : { \"documentation_types\": \"Development\" } }, { \"term\":{ \"sentiment\" : \"__label__neutral\" } } ] } } } In this case, the field sentiment is not an array, thus, for looking inside of it we should use a term query, while a match query for fields that are arrays. Furthermore, as we have multiple queries that are linked through OR, we need to declare a query of type bool and should . If the query would be to find the documentation entries that have a Getting Started , a Development guide and have a neutral sentiment, instead of using a should operation, it need to be used a must . GET documentation.documentation.entry.nlp/_search { \"query\": { \"bool\": { \"must\": [ { \"match\" : { \"documentation_types\": \"Started\" } }, { \"match\" : { \"documentation_types\": \"Development\" } }, { \"term\":{ \"sentiment\" : \"__label__neutral\" } } ] } } } Example 3 Searching for GitHub Issues comments containing the word issue in the field plain_text : ``` GET github.bug.comment.nlp/_search { \"query\": { \"match\": { \"plain_text\": \"issue\" } } } ### Example 4 Searching in all the indexes the string *Ast\u00e9rix*: GET _search { \"query\": { \"query_string\": { \"query\": \"Ast\u00e9rix\" } } } In this case, as we are not quering any particular field, we use the *query_string*. If we would like to search either for *Ast\u00e9rix* or *Ob\u00e9lix*, we can use boolean operators: GET _search { \"query\": { \"query_string\": { \"query\": \"Ast\u00e9rix OR Ob\u00e9lix\" } } } If both names must appear, instead of using OR, we can use AND. GET _search { \"query\": { \"query_string\": { \"query\": \"Ast\u00e9rix AND Ob\u00e9lix\" } } } Query strings such as *Ast\u00e9rix Ob\u00e9lix* will be considered to have an implicit OR, i.e. *Ast\u00e9ric OR Ob\u00e9lix*. GET _search { \"query\": { \"query_string\": { \"query\": \"Ast\u00e9rix Ob\u00e9lix\" } } } If the we are looking for the exact combination of words, such as *Ast\u00e9rix and Ob\u00e9lix*, we can use parenthesis to indicate ElasticSearch that the words should be split. GET _search { \"query\": { \"query_string\": { \"query\": \"(Ast\u00e9rix and Ob\u00e9lix) OR (Felix the Cat)\" } } } ### Example 5 ElasticSearch can do some statistics regarding the output of queries. In this example, we want extended statistics about the field readability in the documentation entries: GET documentation.documentation.entry.nlp/_search { \"size\": 0, \"aggs\" : { \"readability_stats\" : { \"extended_stats\" : { \"field\" : \"readability\" } } } } The option *size=0*, indicates that only the aggregation with the statistics must be returned, otherwise the query will return the elements that also match the searching query. If instead of wanting some statistics, we want to autogenerate histograms, then we can use the following query: POST documentation.documentation.entry.nlp/_search { \"size\": 0, \"aggs\": { \"histogram_analysis\": { \"histogram\" : { \"field\" : \"readability\", \"interval\" : 2 } } } } ```","title":"Indexing Metrics Guide"},{"location":"others/resources/indexing_guide/#indexing-metrics-guide","text":"This guide describes the indexing metric providers available in Scava platform. It provides the mapping so that users can query the index.","title":"Indexing Metrics Guide"},{"location":"others/resources/indexing_guide/#orgeclipsescavametricproviderindexingbugs","text":"Short name : bug indexing metric Friendly name : Bugs tracking system indexer This metric prepares and indexes analyses documents relating to bug tracking system. Mapping Information : bug.comment String comment_Id String body List<String> emotional_dimension String sentiment String plain_text String request_reply_classification String content_class boolean contains_code String bug_Id String project_name String creator Date created_at String uid List<String> commits List<String> referring_to bug.post Date created_at String bug_summary String severity String bug_id String project_name String creator String uid boolean migration_issue List<String> problematic_changes double matching_score","title":"org.eclipse.scava.metricprovider.indexing.bugs"},{"location":"others/resources/indexing_guide/#orgeclipsescavametricproviderindexingcommits","text":"Short name : metricprovider.indexing.commits Friendly name : Commits indexer This metric prepares and indexes documents related to commits. Mapping Information : commits Date created_at String project_name String uid String repository String revision String author String author_email String body String plain_text List<String> commits_references List<String> bugs_references","title":"org.eclipse.scava.metricprovider.indexing.commits"},{"location":"others/resources/indexing_guide/#orgeclipsescavametricproviderindexingcommunicationchannels","text":"Short name : communication channels indexing metric Friendly name : communication channels indexer This metric prepares and indexes documents relating to communication channels. Mapping Information : article Long article_Id String communication_channel_id String uid List<Integer> thread_id String project_name String message_body String subject String creator Date created_at List<String> emotional_dimension String sentiment String plain_text String request_reply_classification String content_class boolean contains_code thread String communication_channel_id String uid int thread_id String project_name String subject boolean migration_issue List<String> problematic_changes double matching_score","title":"org.eclipse.scava.metricprovider.indexing.communicationchannels"},{"location":"others/resources/indexing_guide/#orgeclipsescavametricproviderindexingdocumentation","text":"Short name : communication channels indexing metric Friendly name : communication channels indexer This metric prepares and indexes documents relating to communication channels. Mapping Information : documentation_entry Date created_at String project_name String uid String documentation_id String documentation_entry_id String body String original_format_mime String sentiment String plain_text double readability List<String> documentation_type boolean licence_found String licence documentation Date last_update String uid String project_name String documentation_id List<String> documentation_entries","title":"org.eclipse.scava.metricprovider.indexing.documentation"},{"location":"others/resources/indexing_guide/#query-examples","text":"Here we present a series of examples of how you can use ElasticSearch queries for finding specific data or doing some aggregations. More information about ElasticSearch queries can be found at https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html","title":"Query Examples"},{"location":"others/resources/indexing_guide/#example-1","text":"Retrieve the documentation entries that have been determined containing a Getting Started guide. GET documentation.documentation.entry.nlp/_search { \"query\": { \"match\" : { \"documentation_types\": \"Started\" } } } As documentation_types is an array, we need to use a match within the query.","title":"Example 1"},{"location":"others/resources/indexing_guide/#example-2","text":"Retrieve the documentation entries that have at least one of the following characteristics: - Are a Getting Started guide - Are a Development guide - Have a neutral sentiment GET documentation.documentation.entry.nlp/_search { \"query\": { \"bool\": { \"should\": [ { \"match\" : { \"documentation_types\": \"Started\" } }, { \"match\" : { \"documentation_types\": \"Development\" } }, { \"term\":{ \"sentiment\" : \"__label__neutral\" } } ] } } } In this case, the field sentiment is not an array, thus, for looking inside of it we should use a term query, while a match query for fields that are arrays. Furthermore, as we have multiple queries that are linked through OR, we need to declare a query of type bool and should . If the query would be to find the documentation entries that have a Getting Started , a Development guide and have a neutral sentiment, instead of using a should operation, it need to be used a must . GET documentation.documentation.entry.nlp/_search { \"query\": { \"bool\": { \"must\": [ { \"match\" : { \"documentation_types\": \"Started\" } }, { \"match\" : { \"documentation_types\": \"Development\" } }, { \"term\":{ \"sentiment\" : \"__label__neutral\" } } ] } } }","title":"Example 2"},{"location":"others/resources/indexing_guide/#example-3","text":"Searching for GitHub Issues comments containing the word issue in the field plain_text : ``` GET github.bug.comment.nlp/_search { \"query\": { \"match\": { \"plain_text\": \"issue\" } } } ### Example 4 Searching in all the indexes the string *Ast\u00e9rix*: GET _search { \"query\": { \"query_string\": { \"query\": \"Ast\u00e9rix\" } } } In this case, as we are not quering any particular field, we use the *query_string*. If we would like to search either for *Ast\u00e9rix* or *Ob\u00e9lix*, we can use boolean operators: GET _search { \"query\": { \"query_string\": { \"query\": \"Ast\u00e9rix OR Ob\u00e9lix\" } } } If both names must appear, instead of using OR, we can use AND. GET _search { \"query\": { \"query_string\": { \"query\": \"Ast\u00e9rix AND Ob\u00e9lix\" } } } Query strings such as *Ast\u00e9rix Ob\u00e9lix* will be considered to have an implicit OR, i.e. *Ast\u00e9ric OR Ob\u00e9lix*. GET _search { \"query\": { \"query_string\": { \"query\": \"Ast\u00e9rix Ob\u00e9lix\" } } } If the we are looking for the exact combination of words, such as *Ast\u00e9rix and Ob\u00e9lix*, we can use parenthesis to indicate ElasticSearch that the words should be split. GET _search { \"query\": { \"query_string\": { \"query\": \"(Ast\u00e9rix and Ob\u00e9lix) OR (Felix the Cat)\" } } } ### Example 5 ElasticSearch can do some statistics regarding the output of queries. In this example, we want extended statistics about the field readability in the documentation entries: GET documentation.documentation.entry.nlp/_search { \"size\": 0, \"aggs\" : { \"readability_stats\" : { \"extended_stats\" : { \"field\" : \"readability\" } } } } The option *size=0*, indicates that only the aggregation with the statistics must be returned, otherwise the query will return the elements that also match the searching query. If instead of wanting some statistics, we want to autogenerate histograms, then we can use the following query: POST documentation.documentation.entry.nlp/_search { \"size\": 0, \"aggs\": { \"histogram_analysis\": { \"histogram\" : { \"field\" : \"readability\", \"interval\" : 2 } } } } ```","title":"Example 3"},{"location":"others/resources/readers/","text":"Readers This document describes the readers used to provide data for the Scava platform. Bug Trackers The following readers retrieve data from bug tracking systems. org.eclipse.scava.platform.bugtrackingsystem.bitbucket This reader retrieves data from Bitbucket through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String URL of the repository https://bitbucket.org/fenics-project/dolfin Yes Login String Username used to log into Bitbucket admin Yes Password String Password used to log into Bitbucket admin101 Yes org.eclipse.scava.platform.bugtrackingsystem.bugzilla This reader retrieves data from Bugzilla through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String URL of the repository https://bugzilla.redhat.com/xmlrpc.cgi Yes Product String An existing project in Bugzilla Red Hat Enterprise Linux 8 Yes Component String An existing component of the project 389-ds-base Yes Project String The Project name Bugzilla Yes org.eclipse.scava.platform.bugtrackingsystem.github This reader retrieves data from GitHub through a REST API. To register a project on the platform the user must use the Import Project option and provide the following parameter: Parameter Type Description Example URL String URL of the repository https://github.com/Adrian-Berrigan/adriantest org.eclipse.scava.platform.bugtrackingsystem.gitlab This reader retrieves data from GitLab through a REST API. To register a project on the platform the user must use the Import Project option and provide the following parameter: Parameter Type Description Example URL String URL of the repository https://github.com/Adrian-Berrigan/adriantest org.eclipse.scava.platform.bugtrackingsystem.jira This reader retrieves data from Jira through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String URL of the repository https://jira.xwiki.org Yes Login String Username used to log into Jira admin No Password String Password used to log into Jira admin101 No Project String Name of the project XWIKI Yes org.eclipse.scava.platform.bugtrackingsystem.mantis This reader retrieves data from Mantis through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example URL String URL of the repository https://www.mantisbt.org/ Token String A unique token used to make authentication request 6aefdre554675bfgtrhgfy77567 org.eclipse.scava.platform.bugtrackingsystem.redmine This reader retrieves data from Redmine through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example URL String URL of the repository http://forge.modelio.org Name String Name of the user Dave Project String Name of the project intocps org.eclipse.scava.platform.bugtrackingsystem.sourceforge This reader retrieves data from SOURCEFORGE through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameter: Parameter Type Description Example URL String URL of the project repository https://sourceforge.net/rest/p/vice-emu/bugs Communication Channels The following readers retrieve data from communication channels. org.eclipse.scava.platform.communicationchannel.eclipseforums This reader retrieves data from Eclipse Forums through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameter: Parameter Type Description Example Mandatory Forum Id String The unique ID of the eclipse forum 305 Yes Forum Name String Name of the forum Andmore Yes Client Id String Unique ID provided by eclipse for authenticated access G12DrqtW86745tTre65476 No Client Secret String Access token provided by eclipse for authenticated access wp199NB564Frt5R43Ghy87 No Additional Information : - The Forum Id can be found within the forum URL. For example, the Andmore forum URL is https://www.eclipse.org/forums/index.php/f/305/ and the Forum Id is 305 shown in the last directory of the path. - Eclipse enforces a rate limit of 1,000 requests per hour for both authenticated and anonymous requests. However, authenticated users may request to increase the call limit, subject to Eclipse approval. org.eclipse.scava.platform.communicationchannel.irc The Irc reader supports data retrieval from log archive. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String The URL location of the archive http://localhost/Downloads/ Yes Name String Name of the Irc kubuntu Yes Description String Brief description of the Irc The kubuntu archive contains .... Yes Compressed File Ext. String File extension of the archive tar.gzip Yes Username String Username (for protected archive) admin No Password String Password (for protected archive) admin101 No Additional Information : - The reader currently supports \"tarballs\" (i.e., tar.gzip, tgz). - Each archive must be stored in the following name format IrcName-yyyymmdd.ext , to specify the date of analysis (e.g., kubuntu-20040306.tar ). The archive may contain one or more files stored in a folder, and each file represents interaction within a single chat room. org.eclipse.scava.platform.communicationchannel.mbox The Mbox reader supports data retrieval from log archive. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String The URL location of the archive http://localhost/Downloads/ Yes Name String Name of the Mbox mboxes Yes Description String Brief description of the Mbox The mboxes archive contains .... Yes Compressed File Ext. String File extension of the archive tar.gzip Yes Username String Username (for protected archive) admin No Password String Password (for protected archive) admin101 No Additional Information : The reader currently supports \"tarballs\" (i.e., tar.gzip, tgz). The storage location may contain one or more Mbox archive(s), each stored in the following name format MboxName-yyyymmdd.ext , to specify the date of analysis. Each Mbox archive must contain one or more email messages (of the same analysis date), stored in a folder. org.eclipse.scava.platform.communicationchannel.nntp This reader retrieves data from NNTP through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameter: Parameter Type Description Example Mandatory URL String URL of the project repository news.mozilla.org Yes Name String The newsgroup name mozilla.activity-stream Yes Username String Username used to log into the newsgroup channel admin No Password String Password used to log into the newsgroup channel admin101 No Port int The port number 119 by default Yes Interval int The frequency of calls 10000 by default No Additional Information : The port number is mandatory and defaults to 119. However, the user has the option to change this value, if their port number is different from 119. The Interval is not mandatory but defaults to 10000. However, the user has the option to lower or increase this value. org.eclipse.scava.platform.communicationchannel.sympa The Sympa reader supports data retrieval from log archive. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String The URL location of the archive http://localhost/Downloads/ Yes Name String Name of the Sympa sympa Yes Description String Brief description of the Sympa The sympa archive contains .... Yes Compressed File Ext. String File extension of the archive tar.gzip Yes Username String Username (for protected archive) admin No Password String Password (for protected archive) admin101 No Additional Information : The reader currently supports \"tarballs\" (i.e., tar.gzip, tgz). The storage location may contain one or more Sympa archive(s), each stored in the following name format SympaName-yyyymmdd.ext , to specify the date of analysis. Each Sympa archive must contain one or more email messages (of the same analysis date), stored in a folder. Documentation Readers The following readers retrieve project documentation from relevant sources. org.eclipse.scava.platform.documentation.gitbased This reader is Git-Based and thus retrieves data through a REST API. To register a project on the platform the user must use the Documentation Git-Based option and provide the following parameter: Parameter Type Description Example URL String URL of the repository https://github.com/linhr/pongo-pongo/wiki Additional Information : - This reader cannot be used along with a Git project, because internally they use the same model and the documentation.Git-Based reader extends the Git project reader. org.eclipse.scava.platform.documentation.systematic This reader uses web crawler to retrieve data from websites. To register a project on the platform the user must use the Documentation Systematic option and provide the relevant parameter: Parameter Type Description Example URL String URL of the repository https://wiki.eclipse.org/Trace_Compass#User_Guides ExecutionFrequency int Crawling frequency defined in days, e.g., 1 represents a day 1 UserName String The username used to log into the website name@domain.com Password String The password used to log into the website p$%7876 LoginURL String The URL of the login page https://accounts.eclipse.org/user/login?destination=user/login%3Ftakemeback%3Dhttp%253A//www.eclipse.org/forums/index.php%253Ft%253Dlogin UsernameFieldName String The name used to define the username field on the website name PasswordFieldName String The name used to define the password field on the website pass Additional Information : - For clarity, all examples used in the above table are derived from the Eclipse Foundation website. For example, the username field is defined as name. - The Documentation Systematic reader supports both password protected and un-protected sources. URL and ExecutionFrequency are mandatory for both sources. However, only the password protected sources require the additional paramenters. - Dates preceeding the current date are processed once because the reader does not keep track of the website evolution up to the current date. - The reader assumes that all textual information within the specified URL location are related to documentation. - The crawler is configured to comply with the Robots Exclusion protocol and thus folows the rules in the robots.txt file during crawling.","title":"Readers"},{"location":"others/resources/readers/#readers","text":"This document describes the readers used to provide data for the Scava platform.","title":"Readers"},{"location":"others/resources/readers/#bug-trackers","text":"The following readers retrieve data from bug tracking systems.","title":"Bug Trackers"},{"location":"others/resources/readers/#orgeclipsescavaplatformbugtrackingsystembitbucket","text":"This reader retrieves data from Bitbucket through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String URL of the repository https://bitbucket.org/fenics-project/dolfin Yes Login String Username used to log into Bitbucket admin Yes Password String Password used to log into Bitbucket admin101 Yes","title":"org.eclipse.scava.platform.bugtrackingsystem.bitbucket"},{"location":"others/resources/readers/#orgeclipsescavaplatformbugtrackingsystembugzilla","text":"This reader retrieves data from Bugzilla through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String URL of the repository https://bugzilla.redhat.com/xmlrpc.cgi Yes Product String An existing project in Bugzilla Red Hat Enterprise Linux 8 Yes Component String An existing component of the project 389-ds-base Yes Project String The Project name Bugzilla Yes","title":"org.eclipse.scava.platform.bugtrackingsystem.bugzilla"},{"location":"others/resources/readers/#orgeclipsescavaplatformbugtrackingsystemgithub","text":"This reader retrieves data from GitHub through a REST API. To register a project on the platform the user must use the Import Project option and provide the following parameter: Parameter Type Description Example URL String URL of the repository https://github.com/Adrian-Berrigan/adriantest","title":"org.eclipse.scava.platform.bugtrackingsystem.github"},{"location":"others/resources/readers/#orgeclipsescavaplatformbugtrackingsystemgitlab","text":"This reader retrieves data from GitLab through a REST API. To register a project on the platform the user must use the Import Project option and provide the following parameter: Parameter Type Description Example URL String URL of the repository https://github.com/Adrian-Berrigan/adriantest","title":"org.eclipse.scava.platform.bugtrackingsystem.gitlab"},{"location":"others/resources/readers/#orgeclipsescavaplatformbugtrackingsystemjira","text":"This reader retrieves data from Jira through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String URL of the repository https://jira.xwiki.org Yes Login String Username used to log into Jira admin No Password String Password used to log into Jira admin101 No Project String Name of the project XWIKI Yes","title":"org.eclipse.scava.platform.bugtrackingsystem.jira"},{"location":"others/resources/readers/#orgeclipsescavaplatformbugtrackingsystemmantis","text":"This reader retrieves data from Mantis through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example URL String URL of the repository https://www.mantisbt.org/ Token String A unique token used to make authentication request 6aefdre554675bfgtrhgfy77567","title":"org.eclipse.scava.platform.bugtrackingsystem.mantis"},{"location":"others/resources/readers/#orgeclipsescavaplatformbugtrackingsystemredmine","text":"This reader retrieves data from Redmine through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example URL String URL of the repository http://forge.modelio.org Name String Name of the user Dave Project String Name of the project intocps","title":"org.eclipse.scava.platform.bugtrackingsystem.redmine"},{"location":"others/resources/readers/#orgeclipsescavaplatformbugtrackingsystemsourceforge","text":"This reader retrieves data from SOURCEFORGE through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameter: Parameter Type Description Example URL String URL of the project repository https://sourceforge.net/rest/p/vice-emu/bugs","title":"org.eclipse.scava.platform.bugtrackingsystem.sourceforge"},{"location":"others/resources/readers/#communication-channels","text":"The following readers retrieve data from communication channels.","title":"Communication Channels"},{"location":"others/resources/readers/#orgeclipsescavaplatformcommunicationchanneleclipseforums","text":"This reader retrieves data from Eclipse Forums through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameter: Parameter Type Description Example Mandatory Forum Id String The unique ID of the eclipse forum 305 Yes Forum Name String Name of the forum Andmore Yes Client Id String Unique ID provided by eclipse for authenticated access G12DrqtW86745tTre65476 No Client Secret String Access token provided by eclipse for authenticated access wp199NB564Frt5R43Ghy87 No Additional Information : - The Forum Id can be found within the forum URL. For example, the Andmore forum URL is https://www.eclipse.org/forums/index.php/f/305/ and the Forum Id is 305 shown in the last directory of the path. - Eclipse enforces a rate limit of 1,000 requests per hour for both authenticated and anonymous requests. However, authenticated users may request to increase the call limit, subject to Eclipse approval.","title":"org.eclipse.scava.platform.communicationchannel.eclipseforums"},{"location":"others/resources/readers/#orgeclipsescavaplatformcommunicationchannelirc","text":"The Irc reader supports data retrieval from log archive. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String The URL location of the archive http://localhost/Downloads/ Yes Name String Name of the Irc kubuntu Yes Description String Brief description of the Irc The kubuntu archive contains .... Yes Compressed File Ext. String File extension of the archive tar.gzip Yes Username String Username (for protected archive) admin No Password String Password (for protected archive) admin101 No Additional Information : - The reader currently supports \"tarballs\" (i.e., tar.gzip, tgz). - Each archive must be stored in the following name format IrcName-yyyymmdd.ext , to specify the date of analysis (e.g., kubuntu-20040306.tar ). The archive may contain one or more files stored in a folder, and each file represents interaction within a single chat room.","title":"org.eclipse.scava.platform.communicationchannel.irc"},{"location":"others/resources/readers/#orgeclipsescavaplatformcommunicationchannelmbox","text":"The Mbox reader supports data retrieval from log archive. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String The URL location of the archive http://localhost/Downloads/ Yes Name String Name of the Mbox mboxes Yes Description String Brief description of the Mbox The mboxes archive contains .... Yes Compressed File Ext. String File extension of the archive tar.gzip Yes Username String Username (for protected archive) admin No Password String Password (for protected archive) admin101 No Additional Information : The reader currently supports \"tarballs\" (i.e., tar.gzip, tgz). The storage location may contain one or more Mbox archive(s), each stored in the following name format MboxName-yyyymmdd.ext , to specify the date of analysis. Each Mbox archive must contain one or more email messages (of the same analysis date), stored in a folder.","title":"org.eclipse.scava.platform.communicationchannel.mbox"},{"location":"others/resources/readers/#orgeclipsescavaplatformcommunicationchannelnntp","text":"This reader retrieves data from NNTP through a REST API. To register a project on the platform the user must use the Create Project option and provide the following parameter: Parameter Type Description Example Mandatory URL String URL of the project repository news.mozilla.org Yes Name String The newsgroup name mozilla.activity-stream Yes Username String Username used to log into the newsgroup channel admin No Password String Password used to log into the newsgroup channel admin101 No Port int The port number 119 by default Yes Interval int The frequency of calls 10000 by default No Additional Information : The port number is mandatory and defaults to 119. However, the user has the option to change this value, if their port number is different from 119. The Interval is not mandatory but defaults to 10000. However, the user has the option to lower or increase this value.","title":"org.eclipse.scava.platform.communicationchannel.nntp"},{"location":"others/resources/readers/#orgeclipsescavaplatformcommunicationchannelsympa","text":"The Sympa reader supports data retrieval from log archive. To register a project on the platform the user must use the Create Project option and provide the following parameters: Parameter Type Description Example Mandatory URL String The URL location of the archive http://localhost/Downloads/ Yes Name String Name of the Sympa sympa Yes Description String Brief description of the Sympa The sympa archive contains .... Yes Compressed File Ext. String File extension of the archive tar.gzip Yes Username String Username (for protected archive) admin No Password String Password (for protected archive) admin101 No Additional Information : The reader currently supports \"tarballs\" (i.e., tar.gzip, tgz). The storage location may contain one or more Sympa archive(s), each stored in the following name format SympaName-yyyymmdd.ext , to specify the date of analysis. Each Sympa archive must contain one or more email messages (of the same analysis date), stored in a folder.","title":"org.eclipse.scava.platform.communicationchannel.sympa"},{"location":"others/resources/readers/#documentation-readers","text":"The following readers retrieve project documentation from relevant sources.","title":"Documentation Readers"},{"location":"others/resources/readers/#orgeclipsescavaplatformdocumentationgitbased","text":"This reader is Git-Based and thus retrieves data through a REST API. To register a project on the platform the user must use the Documentation Git-Based option and provide the following parameter: Parameter Type Description Example URL String URL of the repository https://github.com/linhr/pongo-pongo/wiki Additional Information : - This reader cannot be used along with a Git project, because internally they use the same model and the documentation.Git-Based reader extends the Git project reader.","title":"org.eclipse.scava.platform.documentation.gitbased"},{"location":"others/resources/readers/#orgeclipsescavaplatformdocumentationsystematic","text":"This reader uses web crawler to retrieve data from websites. To register a project on the platform the user must use the Documentation Systematic option and provide the relevant parameter: Parameter Type Description Example URL String URL of the repository https://wiki.eclipse.org/Trace_Compass#User_Guides ExecutionFrequency int Crawling frequency defined in days, e.g., 1 represents a day 1 UserName String The username used to log into the website name@domain.com Password String The password used to log into the website p$%7876 LoginURL String The URL of the login page https://accounts.eclipse.org/user/login?destination=user/login%3Ftakemeback%3Dhttp%253A//www.eclipse.org/forums/index.php%253Ft%253Dlogin UsernameFieldName String The name used to define the username field on the website name PasswordFieldName String The name used to define the password field on the website pass Additional Information : - For clarity, all examples used in the above table are derived from the Eclipse Foundation website. For example, the username field is defined as name. - The Documentation Systematic reader supports both password protected and un-protected sources. URL and ExecutionFrequency are mandatory for both sources. However, only the password protected sources require the additional paramenters. - Dates preceeding the current date are processed once because the reader does not keep track of the website evolution up to the current date. - The reader assumes that all textual information within the specified URL location are related to documentation. - The crawler is configured to comply with the Robots Exclusion protocol and thus folows the rules in the robots.txt file during crawling.","title":"org.eclipse.scava.platform.documentation.systematic"},{"location":"others/users/Consuming-REST-Services/","text":"Consuming REST services When to use ? This guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications. REST API Reference The reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]]. API Gateway The Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway. All web service request form clients have to go through the gateway. The api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform. Platform Authentication The CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io). 1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. 1. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Authentication in Java Retrieve a Web Tokens from authentication service private String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException { // Authentication Service URI URL url = new URL(\"http://localhost:8086/api/authentication\"); // AUthentication Request HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setDoOutput(true); connection.setRequestMethod(\"POST\"); connection.setRequestProperty(\"Content-Type\", \"application/json\"); String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\"; OutputStream os = connection.getOutputStream(); os.write(input.getBytes()); os.flush(); if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) { throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode()); } connection.disconnect(); // A JWT Token is return in the Header of the response return connection.getHeaderField(\"Authorization\"); } REST Service Call curl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication Service Consumption To consume a REST service provided by the integrated platform, the client must include the Token in the header of his request. ```java // Service URL URL url = new URL(\"http://localhost:8086/api/users\"); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setRequestMethod(\"GET\"); // Add Token to the request header connection.setRequestProperty(\"Authorization\",token); ``` Comment","title":"Consuming REST services"},{"location":"others/users/Consuming-REST-Services/#consuming-rest-services","text":"","title":"Consuming REST services"},{"location":"others/users/Consuming-REST-Services/#when-to-use","text":"This guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications.","title":"When to use ?"},{"location":"others/users/Consuming-REST-Services/#rest-api-reference","text":"The reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]].","title":"REST API Reference"},{"location":"others/users/Consuming-REST-Services/#api-gateway","text":"The Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway. All web service request form clients have to go through the gateway. The api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform.","title":"API Gateway"},{"location":"others/users/Consuming-REST-Services/#platform-authentication","text":"The CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io). 1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. 1. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Authentication in Java Retrieve a Web Tokens from authentication service private String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException { // Authentication Service URI URL url = new URL(\"http://localhost:8086/api/authentication\"); // AUthentication Request HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setDoOutput(true); connection.setRequestMethod(\"POST\"); connection.setRequestProperty(\"Content-Type\", \"application/json\"); String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\"; OutputStream os = connection.getOutputStream(); os.write(input.getBytes()); os.flush(); if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) { throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode()); } connection.disconnect(); // A JWT Token is return in the Header of the response return connection.getHeaderField(\"Authorization\"); } REST Service Call curl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication","title":"Platform Authentication"},{"location":"others/users/Consuming-REST-Services/#service-consumption","text":"To consume a REST service provided by the integrated platform, the client must include the Token in the header of his request. ```java // Service URL URL url = new URL(\"http://localhost:8086/api/users\"); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setRequestMethod(\"GET\"); // Add Token to the request header connection.setRequestProperty(\"Authorization\",token); ```","title":"Service Consumption"},{"location":"others/users/Consuming-REST-Services/#comment","text":"","title":"Comment"},{"location":"others/users/REST-API-Generation/","text":"REST API Generation REST API Tutorial files Install First of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install. Execute the following steps. Clone repository: https://github.com/patrickneubauer/crossminer-workflow Import projects from the cloned repo to empty Eclipse workspace: Import root via Maven > Existing Maven Projects , then the rest via General > Existing projects into Workspace without checking Search for nested projects . Install new software packages: Install Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling [web page] Install Epsilon (Stable): http://download.eclipse.org/epsilon/updates/ [update site] Update Epsilon (Interim): http://download.eclipse.org/epsilon/interim/ [update site] Install Emfatic: http://download.eclipse.org/emfatic/update/ [update site] Note: I didn't see the the feature until Group items by category was unchecked. Install GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ [update site] Import the given json projects ( org.eclipse.epsilon.emc.json and org.eclipse.epsilon.emc.json.dt ) into the workspace via General > Existing projects into Workspace . Now right-click on org.eclipse.crossmeter.workflow project and select Maven > Update Project.... Copy the give M2M_Environment_oxygen.launch (for Windows 10) to org.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator and overwrite the old one. Refresh project org.eclipse.crossmeter.workflow.restmule.generator in Eclipse. Before running the .launch file, right-click on it, Run as > Run configurations.... Here go to Plug-ins tab and click on Add Required Plug-ins . Now you can run the .launch file TBA:How? In the Runtime Eclipse import the org.eclipse.crossmeter.workflow.restmule.generator project as Maven Project. Run generateFromOAS.launch as you can see in the videos https://youtu.be/BJXuozHJPeg. Now, you should see it generating the project. For further steps, watch Patrick's videos . github client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg github client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI github client generation example \u2014 https://youtu.be/ltSNnSZRETA kafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc kafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4 kafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA MDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk Example I've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following: /square/{number} . You can replace the {number} with any number. The repsonse for this request is a JSON object, which looks like this: { \"squared\": {squaredValue} } It contains only one field, named squared , which has the value of the square of the given number. From the specification the generator will provide us a Java Project which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines: ITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings). int number = 7; //The number to be squared. IData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server. NumberValue numberValue; //NumberValue is the class which represents the object received from the server numberValue = squared.observe().blockingSingle(); //Get the actually received object from the response. System.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer. Everything, including the IEntityApi interface and the NumberValue class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through Java to the objects/field inside of the received JSON object. And once again, they are all generated from the OpenAPI specification. So, to generate the mentioned Java Project , you have to put your OpenAPI specification file into the schemas folder inside the org.eclipse.crossmeter.workflow.restmule.generator project. I'll call mine as TestAPI.json , and it's made up of the following: { \"swagger\": \"2.0\", \"schemes\": [ \"http\" ], \"host\": \"localhost:8080\", \"basePath\": \"/\", \"info\": { \"description\": \"This is a test API, only for demonstration of the generator.\", \"termsOfService\": \"\", \"title\": \"TestAPI\", \"version\": \"v1\" }, \"consumes\": [ \"application/json\" ], \"produces\": [ \"application/json\" ], \"securityDefinitions\": { }, \"paths\": { \"/square/{number}\": { \"get\": { \"description\": \"Square a number.\", \"parameters\": [ { \"description\": \"The number to be squared\", \"in\": \"path\", \"name\": \"number\", \"required\": true, \"type\": \"integer\" } ], \"responses\": { \"200\": { \"description\": \"OK\", \"schema\": { \"$ref\": \"#/definitions/NumberValue\" } } } } } }, \"definitions\": { \"NumberValue\": { \"properties\": { \"squared\": { \"type\": \"integer\" } }, \"type\": \"object\" } } } You can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own. Then we have to set the generator to use our new schema. To do this, open the build.xml in the generator project and modify the api and json.model.file property to testapi and schemas/TestAPI.json , respectively. <!--API Variables --> <property name=\"api\" value=\"testapi\" /> <property name=\"json.model.file\" value=\"schemas/TestAPI.json\" /> We are almost done, but we have to take one more small step. We have to provide an .eol file in the epsilon/util/fix/ folder in the generator project. Its name must be the same as the value of the api property in the previous step, so for this example we use the testapi.eol . The content of this file: import \"../restmule.eol\"; var api = RestMule!API.all.first(); // RATE LIMITS var search = new RestMule!RatePolicyScope; search.scope = \"Search\"; var entity = new RestMule!RatePolicyScope; entity.scope = \"Entity\"; // RATE POLICY var policy = new RestMule!RatePolicy; policy.scopes.add(entity); policy.scopes.add(search); var reset = new RestMule!ResponseHeader; var resetInt = new RestMule!TInteger; resetInt.label = \"X-RateLimit-Reset\"; reset.type = resetInt; policy.reset = reset; var limit = new RestMule!ResponseHeader; var limitInt = new RestMule!TInteger; limitInt.label = \"X-RateLimit-Limit\"; limit.type = limitInt; policy.limit = limit; var remaining = new RestMule!ResponseHeader; var remainingInt = new RestMule!TInteger; remainingInt.label = \"X-RateLimit-Remaining\"; remaining.type = remainingInt; policy.remaining = remaining; api.ratePolicy = policy; // PAGINATION var pagination= new RestMule!PaginationPolicy; pagination.start = 1; pagination.max = 10; pagination.increment = 1; pagination.maxPerIteration = 100; var perIteration = new RestMule!Query; perIteration.description = \"Items per page\"; perIteration.required = false; var type = new RestMule!TInteger; type.label = \"per_page\"; type.name = type.label; perIteration.type = type; pagination.perIteration = perIteration; var page = new RestMule!Query; page.description = \"Page identifier\"; page.required = false; var type1 = new RestMule!TInteger; type1.label = \"page\"; type1.name = type1.label; page.type = type1; pagination.page = page; var link = new RestMule!ResponseHeader; link.description = \"Page links\"; var format = new RestMule!TFormattedString; format.label = \"Link\"; format.name = format.label; link.type = format; pagination.links = link; api.pagination = pagination; // WRAPPER var wrapper = new RestMule!Wrapper; wrapper.name = \"Wrapper\"; var items = new RestMule!ListType; items.label = \"items\"; wrapper.items = items; wrapper.totalLabel= \"total_count\"; wrapper.incompleteLabel = \"incomplete_results\"; api.pageWrapper = wrapper; // ADD RATE & WRAPPER TO REQUESTS (FIXME) for (r in RestMule!Request.all){ if (r.parent.path.startsWith(\"/search\")){ r.scope = search; } else { r.scope = entity; } r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader))); for (resp in r.responses.select(s|s.responseType <> null)){ resp.unwrap(); } } /* ////////// * OPERATIONS *////////// operation RestMule!ObjectType hasWrapper() : Boolean { var wrapper = RestMule!Wrapper.all.first; var lists = self.listFields.collect(a|a.label); return (not lists.isEmpty()) and lists .includes(wrapper.items.label); } operation RestMule!Response unwrap() : RestMule!ObjectType{ if (self.responseType.instanceOf(ObjectType)){ if (self.responseType.hasWrapper()){ (\"Unwrapping : \"+ self.responseType.name).println; var wrapper = RestMule!Wrapper.all.first; var name = self.responseType.name.println; self.responseType = self.responseType.println.listFields .select(b| b.label == wrapper.items.label).first.elements.first; self.responseType.name = name; self.pageWrapped = true; self.responseType.description = \"UNWRAPPED: \" + self.responseType.description; } } } After this, we can run the generator and it will generate our Java Project from the specification. In the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.","title":"REST API Generation"},{"location":"others/users/REST-API-Generation/#rest-api-generation","text":"REST API Tutorial files","title":"REST API Generation"},{"location":"others/users/REST-API-Generation/#install","text":"First of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install. Execute the following steps. Clone repository: https://github.com/patrickneubauer/crossminer-workflow Import projects from the cloned repo to empty Eclipse workspace: Import root via Maven > Existing Maven Projects , then the rest via General > Existing projects into Workspace without checking Search for nested projects . Install new software packages: Install Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling [web page] Install Epsilon (Stable): http://download.eclipse.org/epsilon/updates/ [update site] Update Epsilon (Interim): http://download.eclipse.org/epsilon/interim/ [update site] Install Emfatic: http://download.eclipse.org/emfatic/update/ [update site] Note: I didn't see the the feature until Group items by category was unchecked. Install GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ [update site] Import the given json projects ( org.eclipse.epsilon.emc.json and org.eclipse.epsilon.emc.json.dt ) into the workspace via General > Existing projects into Workspace . Now right-click on org.eclipse.crossmeter.workflow project and select Maven > Update Project.... Copy the give M2M_Environment_oxygen.launch (for Windows 10) to org.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator and overwrite the old one. Refresh project org.eclipse.crossmeter.workflow.restmule.generator in Eclipse. Before running the .launch file, right-click on it, Run as > Run configurations.... Here go to Plug-ins tab and click on Add Required Plug-ins . Now you can run the .launch file TBA:How? In the Runtime Eclipse import the org.eclipse.crossmeter.workflow.restmule.generator project as Maven Project. Run generateFromOAS.launch as you can see in the videos https://youtu.be/BJXuozHJPeg. Now, you should see it generating the project. For further steps, watch Patrick's videos . github client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg github client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI github client generation example \u2014 https://youtu.be/ltSNnSZRETA kafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc kafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4 kafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA MDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk","title":"Install"},{"location":"others/users/REST-API-Generation/#example","text":"I've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following: /square/{number} . You can replace the {number} with any number. The repsonse for this request is a JSON object, which looks like this: { \"squared\": {squaredValue} } It contains only one field, named squared , which has the value of the square of the given number. From the specification the generator will provide us a Java Project which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines: ITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings). int number = 7; //The number to be squared. IData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server. NumberValue numberValue; //NumberValue is the class which represents the object received from the server numberValue = squared.observe().blockingSingle(); //Get the actually received object from the response. System.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer. Everything, including the IEntityApi interface and the NumberValue class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through Java to the objects/field inside of the received JSON object. And once again, they are all generated from the OpenAPI specification. So, to generate the mentioned Java Project , you have to put your OpenAPI specification file into the schemas folder inside the org.eclipse.crossmeter.workflow.restmule.generator project. I'll call mine as TestAPI.json , and it's made up of the following: { \"swagger\": \"2.0\", \"schemes\": [ \"http\" ], \"host\": \"localhost:8080\", \"basePath\": \"/\", \"info\": { \"description\": \"This is a test API, only for demonstration of the generator.\", \"termsOfService\": \"\", \"title\": \"TestAPI\", \"version\": \"v1\" }, \"consumes\": [ \"application/json\" ], \"produces\": [ \"application/json\" ], \"securityDefinitions\": { }, \"paths\": { \"/square/{number}\": { \"get\": { \"description\": \"Square a number.\", \"parameters\": [ { \"description\": \"The number to be squared\", \"in\": \"path\", \"name\": \"number\", \"required\": true, \"type\": \"integer\" } ], \"responses\": { \"200\": { \"description\": \"OK\", \"schema\": { \"$ref\": \"#/definitions/NumberValue\" } } } } } }, \"definitions\": { \"NumberValue\": { \"properties\": { \"squared\": { \"type\": \"integer\" } }, \"type\": \"object\" } } } You can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own. Then we have to set the generator to use our new schema. To do this, open the build.xml in the generator project and modify the api and json.model.file property to testapi and schemas/TestAPI.json , respectively. <!--API Variables --> <property name=\"api\" value=\"testapi\" /> <property name=\"json.model.file\" value=\"schemas/TestAPI.json\" /> We are almost done, but we have to take one more small step. We have to provide an .eol file in the epsilon/util/fix/ folder in the generator project. Its name must be the same as the value of the api property in the previous step, so for this example we use the testapi.eol . The content of this file: import \"../restmule.eol\"; var api = RestMule!API.all.first(); // RATE LIMITS var search = new RestMule!RatePolicyScope; search.scope = \"Search\"; var entity = new RestMule!RatePolicyScope; entity.scope = \"Entity\"; // RATE POLICY var policy = new RestMule!RatePolicy; policy.scopes.add(entity); policy.scopes.add(search); var reset = new RestMule!ResponseHeader; var resetInt = new RestMule!TInteger; resetInt.label = \"X-RateLimit-Reset\"; reset.type = resetInt; policy.reset = reset; var limit = new RestMule!ResponseHeader; var limitInt = new RestMule!TInteger; limitInt.label = \"X-RateLimit-Limit\"; limit.type = limitInt; policy.limit = limit; var remaining = new RestMule!ResponseHeader; var remainingInt = new RestMule!TInteger; remainingInt.label = \"X-RateLimit-Remaining\"; remaining.type = remainingInt; policy.remaining = remaining; api.ratePolicy = policy; // PAGINATION var pagination= new RestMule!PaginationPolicy; pagination.start = 1; pagination.max = 10; pagination.increment = 1; pagination.maxPerIteration = 100; var perIteration = new RestMule!Query; perIteration.description = \"Items per page\"; perIteration.required = false; var type = new RestMule!TInteger; type.label = \"per_page\"; type.name = type.label; perIteration.type = type; pagination.perIteration = perIteration; var page = new RestMule!Query; page.description = \"Page identifier\"; page.required = false; var type1 = new RestMule!TInteger; type1.label = \"page\"; type1.name = type1.label; page.type = type1; pagination.page = page; var link = new RestMule!ResponseHeader; link.description = \"Page links\"; var format = new RestMule!TFormattedString; format.label = \"Link\"; format.name = format.label; link.type = format; pagination.links = link; api.pagination = pagination; // WRAPPER var wrapper = new RestMule!Wrapper; wrapper.name = \"Wrapper\"; var items = new RestMule!ListType; items.label = \"items\"; wrapper.items = items; wrapper.totalLabel= \"total_count\"; wrapper.incompleteLabel = \"incomplete_results\"; api.pageWrapper = wrapper; // ADD RATE & WRAPPER TO REQUESTS (FIXME) for (r in RestMule!Request.all){ if (r.parent.path.startsWith(\"/search\")){ r.scope = search; } else { r.scope = entity; } r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader))); for (resp in r.responses.select(s|s.responseType <> null)){ resp.unwrap(); } } /* ////////// * OPERATIONS *////////// operation RestMule!ObjectType hasWrapper() : Boolean { var wrapper = RestMule!Wrapper.all.first; var lists = self.listFields.collect(a|a.label); return (not lists.isEmpty()) and lists .includes(wrapper.items.label); } operation RestMule!Response unwrap() : RestMule!ObjectType{ if (self.responseType.instanceOf(ObjectType)){ if (self.responseType.hasWrapper()){ (\"Unwrapping : \"+ self.responseType.name).println; var wrapper = RestMule!Wrapper.all.first; var name = self.responseType.name.println; self.responseType = self.responseType.println.listFields .select(b| b.label == wrapper.items.label).first.elements.first; self.responseType.name = name; self.pageWrapped = true; self.responseType.description = \"UNWRAPPED: \" + self.responseType.description; } } } After this, we can run the generator and it will generate our Java Project from the specification. In the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.","title":"Example"},{"location":"others/users/Running-Scava-in-Eclipse/","text":"Running Scava in Eclipse This page gives general guidelines for running Ossmeter platform in Eclipse The screeshot below will be used as a reference : (Eclipse version used in screenshot : Eclipse Java EE IDE for Web developers Luna 2 (4.4.2) ) We compile with Ossmeterfromfeature.product file in org.ossmeter.platform.osgi (highlighted above) In the file we put program arguements : -apiServer , -master , -slave . More information on https://github.com/crossminer/crossminer/wiki/Running-the-platform We must validate with the button on corner top right (highlighted above). This will add the required packages. If the required packages are still missing, go to Run Configurations -> Plug-ins and click on Add Required Plugins . You could also check the general information , if its the same (as in the screenshot below). These were some of the things for running the platform. If there is any issue still, do not hesitate to contact us.","title":"Running Scava in Eclipse"},{"location":"others/users/Running-Scava-in-Eclipse/#running-scava-in-eclipse","text":"This page gives general guidelines for running Ossmeter platform in Eclipse The screeshot below will be used as a reference : (Eclipse version used in screenshot : Eclipse Java EE IDE for Web developers Luna 2 (4.4.2) ) We compile with Ossmeterfromfeature.product file in org.ossmeter.platform.osgi (highlighted above) In the file we put program arguements : -apiServer , -master , -slave . More information on https://github.com/crossminer/crossminer/wiki/Running-the-platform We must validate with the button on corner top right (highlighted above). This will add the required packages. If the required packages are still missing, go to Run Configurations -> Plug-ins and click on Add Required Plugins . You could also check the general information , if its the same (as in the screenshot below). These were some of the things for running the platform. If there is any issue still, do not hesitate to contact us.","title":"Running Scava in Eclipse"},{"location":"others/users/Scava-Metrics/","text":"Metrics computed by Scava Metric-platform OSGi metrics are defined in: https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.osgi/src/OSGi.rsc List of metrics: allOSGiBundleDependencies -- Retrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies). allOSGiPackageDependencies -- Retrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). allOSGiDynamicImportedPackages -- Retrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file. numberOSGiPackageDependencies -- Retrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). numberOSGiBundleDependencies -- Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies). unusedOSGiImportedPackages -- Retrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell). ratioUnusedOSGiImportedPackages -- Retrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages. ratioUsedOSGiImportedPackages -- Retrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code. numberOSGiSplitImportedPackages -- Retrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest. numberOSGiSplitExportedPackages -- Retrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest.. unversionedOSGiRequiredBundles -- Retrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header. ratioUnversionedOSGiRequiredBundles -- Retrieves the ratio of unversioned OSGi required bundles. unversionedOSGiImportedPackages -- Retrieves the set of unversioned OSGi imported packages (declared in the Import-Package header). If returned value != {} there is a smell in the Manifest. ratioUnversionedOSGiImportedPackages -- Retrieves the ratio of unversioned OSGi imported packages. unversionedOSGiExportedPackages -- Retrieves the set of unversioned OSGi exported packages (declared in the Export-Package header). If returned value != {} there is a smell in the Manifest. ratioUnversionedOSGiExportedPackages -- Retrieves the ratio of unversioned OSGi exported packages. Maven metrics are defined in https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.maven/src/Maven.rsc List of metrics: allMavenDependencies -- Retrieves all the Maven dependencies. allOptionalMavenDependencies -- Retrieves all the optional Maven dependencies. numberMavenDependencies -- Retrieves the number of Maven dependencies. numberUniqueMavenDependencies -- Retrieves the number of unique Maven dependencies. ratioOptionalMavenDependencies -- Retrieves the ratio of optional Maven dependencies. isUsingTycho -- Checks if the current project is a Tycho project. IDE plugin All metrics are defined in metrics_definition_ide_plugin.docx scava-lib-usage -- Level of using CROSSMINER library change function. scava-search-usage -- Level of using CROSSMINER search function. scava-search-success -- Level of successfull free text serach using CROSSMINER plug-in. modifictaion-rate -- Rate of changes applied to document. gui-usage-rate -- Rate of activly using the GUI of Eclipse. testing-rate -- Count of executing tests. working-time -- Average working time for Java source files. file-access-rate -- Average count of Java source files open and brought to top. Natural Language Processing All metrics are defined in metrics_definition_nlp.docx . Issue tracking metrics: Number of bugs: It indicates how many bugs, through the time, are in total all the issue tracking systems related to a project. Number of comments: This metric indicates, through the time, the evolution in the quantity of comments for all the bugs related to a project. Emotions: This metric expresses, grosso modo, the emotions found in the issue tracker for a given project. Number new bugs: It indicates how many bugs are created every delta through a lapse of time. Number of new users: How many users are new in the bug tracking system. Open time: This metric shows the average open time of an issue. Number of patches: It indicates the number of patches located in the issue tracking system. Number of request and replies: From all the comments in the issue tracking system, how many of them are replies and how many are requests. Response time: It presents the average time to reply a comment in an issue tracker. Sentiments: Which sentiments are found in a issue tracking system for a project. Severity: The severity level of the bugs found in the issue tracker. This metric classifies a bug report into 1 of 7 severity levels such as Blocker, Critical, Major, Normal, Minor, Trivial or Enhancement. Status: It indicates the issues status, like open or closed. Topics: Using clustering methods, this metric indicates the topics that are discussed through the time. Unasnwered bugs: It indicates how many issues do not have any reply. Users: How many users are found in a issue tracker system. Newsgroup metrics Number of articles; It determines the number of articles found in a newsgroup. Emotions: Which emotions are found in the newgroup. Number of threads: How many threads a newsgroup contains. In other words, how many different branches have been created since the first email was sent to the newsgroup. Number of new users: How many users make use of the newsgroup. Number of request and replies: From the total number of emails belonging to the newsgroup, how many of them are request and how many are replies. Response time: The average time to reply a message. Sentiments: The sentiments found in the newsgroup. Severity: The severity level of the messages exchanges in the newsgroup. The severity is measured in terms of how severe an issue is. Number of new threads: How many threads are created per delta. Topics: Using the clustering algorithm, we determine which are the most frequent topics discussed in the newsgroup. Number of unanswered topics: How many messages haven\u2019t been answered. Number of users: Number of users that make use of the newsgroup. Forums metrics Number of posts: For every topic (i.e. forum thread), how many posts in total exist. Emotions: Which are the emotions located in the forum. Number of topics: How many forum threads contain the forum. Number of request and replies: From all the posts in the forum, how many are request and how many are reply. Sentiments: Which are the sentiments that can be found in a forum. Severity: In the case the forums are used to express issues, we can determine their severity. Number of new topics: Number of new forum threads created. Topics: Number of content topics, i.e. subjects, are discussed in the forum. Number of unanswered topics: Number of forums threads which do not have a reply. Configuration analysis metrics Metrics of Puppet are defined in detail in Deliverable 4.2 . Puppet Design Metrics: numberOfMultifacetedSmells -- The number of Multifaceted Abstraction smells numberOfUnnecessarySmells -- The number of Unnecessary Abstraction smells numberOfImperativeSmells -- The number of Imperative Abstraction smells numberOfMissAbSmells -- The number of Missing Abstraction smells numberOfInsufficientSmells -- The number of Insufficient Modularization smells numberOfUnstructuredSmells -- The number of Unstructured Module smells numberOfTightSmells -- The number of Tightly-coupled Module smells numberOfBrokenSmells -- The number of Broken Hierarchy smells numberOfMissingDepSmells -- The number of Missing Dependency smells numberOfHairballSmells -- The number of Hairball Structure smells numberOfDeficientSmells -- The number of Deficient Encapsulation smells numberOfWeakenSmells -- The number of Weaken Modularity smells cumulativeNumberOfDesignSmells -- The number of design smells Puppet Implementation Metrics: numberOfMissingDefaultCaseSmells -- Smell exists when a default case is missing in a case or selector statement. numberOfInconsistentNamingConventionSmells -- Smell exists when the used naming convention deviates from the recommended naming convention. numberOfComplexExpressionSmells -- Smell exists when a program contains a difficult to understand complex expression. numberOfDuplicateEntitySmells -- Smell exists duplicate parameters are present in the configuration code. numberOfMisplacedAttributeSmells -- Smell exists when attribute placement within a resource or a class has not followed a recommended order. numberOfImproperAlignmentSmells -- Smell exists when the code is not properly aligned or tabulation characters are used numberOfInvalidPropertyValueSmells -- Smell exists when an invalid value of a property or attribute is used. numberOfIncompleteTasksSmells -- Smell exists when the code has \u201cfixme\u201d and \u201ctodo\u201d tags indicating incomplete tasks. numberOfDeprecatedStatementUsageSmells -- Smell exists when the configuration code uses one of the deprecated statements. numberOfImproperQuoteUsageSmells -- Smell exists when single and double quotes are not used properly. numberOfLongStatementSmells -- Smell exists when the code contains long statements. numberOfIncompleteConditionalSmells -- Smell exists when an \u201cif..elseif\u201d construct used without a terminating \u201celse\u201d clause. numberOfUnguardedVariableSmells -- Smell exists when a variable is not enclosed in braces when being interpolated in a string. Docker Metrics are based on the following rules . Docker Metrics: numberOfUpgradeSmells -- Smell exists when apt-get or apk commands are used problematically. numberOfPinVersionSmells -- Smell exists when version of packages is not explicitly declared. numberOfUntaggedImageSmells -- Smell exists when version of images is not explicitly declared. numberOfSudoSmells -- Smell exists when sudo command and root user are used inappropriately. numberOfCopySmells -- Smell exists when COPY instruction is not used properly. numberOfFromSmells -- Smell exists when FROM instruction is not used properly. numberOfCmdSmells -- Smell exists when CMD and ENTRYPOINT instruction is not used properly. numberOfAddSmells -- Smell exists when ADD instruction is not used properly. numberOfMeaninglessCommandsSmells -- Smell exists when there are commands that makes no sense running them in a Docker container. numberOfInvalidPortsSmells -- Smell exists when invalid UNIX ports range is used. numberOfShellSmells -- Smell exists when SHELL instruction is not used when it should. All of the above metrics have their corresponding allOfSmells metrics (e.g. numberOfMultifacetedSmells -> allOfMultifacetedSmells) that lists the actual smells. Pattern and Anti-pattern Metrics: puppetPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a puppet file. puppetAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a puppet file. dockerPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a Dockerfile. dockerAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a Dockerfile. Components metrics: allDockerLibraries: Retrieves all the libraries that are installed with the use of apt-get install or apk install in Dockerfiles numberOfDockerLibraries: Retrieves the number of the above libraries allDockerImages: Retrieves all the images that are used with the use of FROM instruction in Dockerfiles numberOfDockerImages: Retrieves the number of the above images New versions Metrics: newVersionFound: The metric will list the new versions (if any) of libraries that are used in the project.","title":"Metrics computed by Scava"},{"location":"others/users/Scava-Metrics/#metrics-computed-by-scava","text":"","title":"Metrics computed by Scava"},{"location":"others/users/Scava-Metrics/#metric-platform","text":"OSGi metrics are defined in: https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.osgi/src/OSGi.rsc List of metrics: allOSGiBundleDependencies -- Retrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies). allOSGiPackageDependencies -- Retrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). allOSGiDynamicImportedPackages -- Retrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file. numberOSGiPackageDependencies -- Retrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). numberOSGiBundleDependencies -- Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies). unusedOSGiImportedPackages -- Retrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell). ratioUnusedOSGiImportedPackages -- Retrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages. ratioUsedOSGiImportedPackages -- Retrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code. numberOSGiSplitImportedPackages -- Retrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest. numberOSGiSplitExportedPackages -- Retrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest.. unversionedOSGiRequiredBundles -- Retrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header. ratioUnversionedOSGiRequiredBundles -- Retrieves the ratio of unversioned OSGi required bundles. unversionedOSGiImportedPackages -- Retrieves the set of unversioned OSGi imported packages (declared in the Import-Package header). If returned value != {} there is a smell in the Manifest. ratioUnversionedOSGiImportedPackages -- Retrieves the ratio of unversioned OSGi imported packages. unversionedOSGiExportedPackages -- Retrieves the set of unversioned OSGi exported packages (declared in the Export-Package header). If returned value != {} there is a smell in the Manifest. ratioUnversionedOSGiExportedPackages -- Retrieves the ratio of unversioned OSGi exported packages. Maven metrics are defined in https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.maven/src/Maven.rsc List of metrics: allMavenDependencies -- Retrieves all the Maven dependencies. allOptionalMavenDependencies -- Retrieves all the optional Maven dependencies. numberMavenDependencies -- Retrieves the number of Maven dependencies. numberUniqueMavenDependencies -- Retrieves the number of unique Maven dependencies. ratioOptionalMavenDependencies -- Retrieves the ratio of optional Maven dependencies. isUsingTycho -- Checks if the current project is a Tycho project.","title":"Metric-platform"},{"location":"others/users/Scava-Metrics/#ide-plugin","text":"All metrics are defined in metrics_definition_ide_plugin.docx scava-lib-usage -- Level of using CROSSMINER library change function. scava-search-usage -- Level of using CROSSMINER search function. scava-search-success -- Level of successfull free text serach using CROSSMINER plug-in. modifictaion-rate -- Rate of changes applied to document. gui-usage-rate -- Rate of activly using the GUI of Eclipse. testing-rate -- Count of executing tests. working-time -- Average working time for Java source files. file-access-rate -- Average count of Java source files open and brought to top.","title":"IDE plugin"},{"location":"others/users/Scava-Metrics/#natural-language-processing","text":"All metrics are defined in metrics_definition_nlp.docx . Issue tracking metrics: Number of bugs: It indicates how many bugs, through the time, are in total all the issue tracking systems related to a project. Number of comments: This metric indicates, through the time, the evolution in the quantity of comments for all the bugs related to a project. Emotions: This metric expresses, grosso modo, the emotions found in the issue tracker for a given project. Number new bugs: It indicates how many bugs are created every delta through a lapse of time. Number of new users: How many users are new in the bug tracking system. Open time: This metric shows the average open time of an issue. Number of patches: It indicates the number of patches located in the issue tracking system. Number of request and replies: From all the comments in the issue tracking system, how many of them are replies and how many are requests. Response time: It presents the average time to reply a comment in an issue tracker. Sentiments: Which sentiments are found in a issue tracking system for a project. Severity: The severity level of the bugs found in the issue tracker. This metric classifies a bug report into 1 of 7 severity levels such as Blocker, Critical, Major, Normal, Minor, Trivial or Enhancement. Status: It indicates the issues status, like open or closed. Topics: Using clustering methods, this metric indicates the topics that are discussed through the time. Unasnwered bugs: It indicates how many issues do not have any reply. Users: How many users are found in a issue tracker system. Newsgroup metrics Number of articles; It determines the number of articles found in a newsgroup. Emotions: Which emotions are found in the newgroup. Number of threads: How many threads a newsgroup contains. In other words, how many different branches have been created since the first email was sent to the newsgroup. Number of new users: How many users make use of the newsgroup. Number of request and replies: From the total number of emails belonging to the newsgroup, how many of them are request and how many are replies. Response time: The average time to reply a message. Sentiments: The sentiments found in the newsgroup. Severity: The severity level of the messages exchanges in the newsgroup. The severity is measured in terms of how severe an issue is. Number of new threads: How many threads are created per delta. Topics: Using the clustering algorithm, we determine which are the most frequent topics discussed in the newsgroup. Number of unanswered topics: How many messages haven\u2019t been answered. Number of users: Number of users that make use of the newsgroup. Forums metrics Number of posts: For every topic (i.e. forum thread), how many posts in total exist. Emotions: Which are the emotions located in the forum. Number of topics: How many forum threads contain the forum. Number of request and replies: From all the posts in the forum, how many are request and how many are reply. Sentiments: Which are the sentiments that can be found in a forum. Severity: In the case the forums are used to express issues, we can determine their severity. Number of new topics: Number of new forum threads created. Topics: Number of content topics, i.e. subjects, are discussed in the forum. Number of unanswered topics: Number of forums threads which do not have a reply.","title":"Natural Language Processing"},{"location":"others/users/Scava-Metrics/#configuration-analysis-metrics","text":"Metrics of Puppet are defined in detail in Deliverable 4.2 . Puppet Design Metrics: numberOfMultifacetedSmells -- The number of Multifaceted Abstraction smells numberOfUnnecessarySmells -- The number of Unnecessary Abstraction smells numberOfImperativeSmells -- The number of Imperative Abstraction smells numberOfMissAbSmells -- The number of Missing Abstraction smells numberOfInsufficientSmells -- The number of Insufficient Modularization smells numberOfUnstructuredSmells -- The number of Unstructured Module smells numberOfTightSmells -- The number of Tightly-coupled Module smells numberOfBrokenSmells -- The number of Broken Hierarchy smells numberOfMissingDepSmells -- The number of Missing Dependency smells numberOfHairballSmells -- The number of Hairball Structure smells numberOfDeficientSmells -- The number of Deficient Encapsulation smells numberOfWeakenSmells -- The number of Weaken Modularity smells cumulativeNumberOfDesignSmells -- The number of design smells Puppet Implementation Metrics: numberOfMissingDefaultCaseSmells -- Smell exists when a default case is missing in a case or selector statement. numberOfInconsistentNamingConventionSmells -- Smell exists when the used naming convention deviates from the recommended naming convention. numberOfComplexExpressionSmells -- Smell exists when a program contains a difficult to understand complex expression. numberOfDuplicateEntitySmells -- Smell exists duplicate parameters are present in the configuration code. numberOfMisplacedAttributeSmells -- Smell exists when attribute placement within a resource or a class has not followed a recommended order. numberOfImproperAlignmentSmells -- Smell exists when the code is not properly aligned or tabulation characters are used numberOfInvalidPropertyValueSmells -- Smell exists when an invalid value of a property or attribute is used. numberOfIncompleteTasksSmells -- Smell exists when the code has \u201cfixme\u201d and \u201ctodo\u201d tags indicating incomplete tasks. numberOfDeprecatedStatementUsageSmells -- Smell exists when the configuration code uses one of the deprecated statements. numberOfImproperQuoteUsageSmells -- Smell exists when single and double quotes are not used properly. numberOfLongStatementSmells -- Smell exists when the code contains long statements. numberOfIncompleteConditionalSmells -- Smell exists when an \u201cif..elseif\u201d construct used without a terminating \u201celse\u201d clause. numberOfUnguardedVariableSmells -- Smell exists when a variable is not enclosed in braces when being interpolated in a string. Docker Metrics are based on the following rules . Docker Metrics: numberOfUpgradeSmells -- Smell exists when apt-get or apk commands are used problematically. numberOfPinVersionSmells -- Smell exists when version of packages is not explicitly declared. numberOfUntaggedImageSmells -- Smell exists when version of images is not explicitly declared. numberOfSudoSmells -- Smell exists when sudo command and root user are used inappropriately. numberOfCopySmells -- Smell exists when COPY instruction is not used properly. numberOfFromSmells -- Smell exists when FROM instruction is not used properly. numberOfCmdSmells -- Smell exists when CMD and ENTRYPOINT instruction is not used properly. numberOfAddSmells -- Smell exists when ADD instruction is not used properly. numberOfMeaninglessCommandsSmells -- Smell exists when there are commands that makes no sense running them in a Docker container. numberOfInvalidPortsSmells -- Smell exists when invalid UNIX ports range is used. numberOfShellSmells -- Smell exists when SHELL instruction is not used when it should. All of the above metrics have their corresponding allOfSmells metrics (e.g. numberOfMultifacetedSmells -> allOfMultifacetedSmells) that lists the actual smells. Pattern and Anti-pattern Metrics: puppetPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a puppet file. puppetAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a puppet file. dockerPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a Dockerfile. dockerAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a Dockerfile. Components metrics: allDockerLibraries: Retrieves all the libraries that are installed with the use of apt-get install or apk install in Dockerfiles numberOfDockerLibraries: Retrieves the number of the above libraries allDockerImages: Retrieves all the images that are used with the use of FROM instruction in Dockerfiles numberOfDockerImages: Retrieves the number of the above images New versions Metrics: newVersionFound: The metric will list the new versions (if any) of libraries that are used in the project.","title":"Configuration analysis metrics"},{"location":"others/users/Scava-Resources/","text":"Accessing Scava generated data Knowledge-base API description: http:// :8080/swagger-ui.html Get value of a metric for a single project: http:// :8182/projects/p/ /m/ List of metrics: http:// :8182/metrics List of factoids: http:// :8182/factoids","title":"Accessing Scava generated data"},{"location":"others/users/Scava-Resources/#accessing-scava-generated-data","text":"Knowledge-base API description: http:// :8080/swagger-ui.html Get value of a metric for a single project: http:// :8182/projects/p/ /m/ List of metrics: http:// :8182/metrics List of factoids: http:// :8182/factoids","title":"Accessing Scava generated data"},{"location":"user-guide/","text":"SCAVA User Guide The SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin. Quick Start Guide [TODO] Overivew Required Platform Administration and Project Analysis The SCAVA Administration application provide services to analyse open source software projects repository and provide several general administration feature including user managements services and platform configuration services. Visualisation Dashboard This Visualisation Dashboard guide describes the different dashboards that are available with the platform. Eclipse Plugin [TODO] Overivew Required Workflow Engine [TODO] Overivew Required Metrics Reference Guide This Metrics Reference Guide describes the historic and transient metric providers provided by the Scava platform.","title":"SCAVA User Guide"},{"location":"user-guide/#scava-user-guide","text":"The SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin.","title":"SCAVA User Guide"},{"location":"user-guide/#quick-start-guide","text":"[TODO] Overivew Required","title":"Quick Start Guide"},{"location":"user-guide/#platform-administration-and-project-analysis","text":"The SCAVA Administration application provide services to analyse open source software projects repository and provide several general administration feature including user managements services and platform configuration services.","title":"Platform Administration and Project Analysis"},{"location":"user-guide/#visualisation-dashboard","text":"This Visualisation Dashboard guide describes the different dashboards that are available with the platform.","title":"Visualisation Dashboard"},{"location":"user-guide/#eclipse-plugin","text":"[TODO] Overivew Required","title":"Eclipse Plugin"},{"location":"user-guide/#workflow-engine","text":"[TODO] Overivew Required","title":"Workflow Engine"},{"location":"user-guide/#metrics-reference-guide","text":"This Metrics Reference Guide describes the historic and transient metric providers provided by the Scava platform.","title":"Metrics Reference Guide"},{"location":"user-guide/administration/","text":"Platform Administration User Guide The guide show up the different features available through the Administration Dashboard, including: * The Login feature * The Projects management feature * The Users management feature * The Workers feature * The Properties management feature * The Stack Traces feature The Login feature The first time the user access the administration dashboard, a login form will be shown in order to authenticate the associated user. The Projects management feature The Projects view displays the list of the OSS projects downloaded from the OSS forges. The administration dashboard provides two operations: * Import Project: If your project is hosted on an OSS forge, you can simply paste the URL on the field and add it. Create Project: The second operation provides an extra-options to customize the project creation alongside to the metadata available from differents related sources, eg., communication channels and bug tracking systems. Once the project has been registred, it's possible to configure inside it some analysis tasks. An analysis task is consist of: * Label name: the analysis task name. * Task type: the scheduling tasks execution mechanism which could be: * Single Execution: which allows to execute a task between a start date and an end date. * Monitoring Execution: which permits to run a task from a start date until the current date then to schedule the task execution each new day. * Start date: the start time range of the analysis process. * End date: the end time range of the analysis process. * Metric Providers: the metrics available on the metric-platform through the extension points mechanism. These tasks will be executed later to compute/calculate some metrics that will be used to measure the quality of the OSS projects during various period of time. The Users management feature The Users view allows to manage the differents users of the the administration dashboards. It provides three levels of roles: USER ROLE: The user profile is enable to shows up the basic administration dashboard features including: Show up the list of registred projects. Show up the analysis tasks status. Show up the list of metric-providers. Manage its own token authorities. Show up the stack traces. PROJECT MANAGER ROLE: The project manager profile is enable to manage the main administration dashboard features including: Import/create new projects. Show up the list of registred projects. Manage new analysis tasks. Show up the analysis tasks status. Show up the list of metric-providers. Manage its own token authorities. Show up the stack traces. ADMIN ROLE: The admin profile is the super-user of the admin-ui which is enable to get access to almost all the features come with the administration dashboard, including: Import/create new projects. Show up the list of registred projects. Manage new analysis tasks. Show up the analysis tasks status. Show up the list of metric-providers. Manage the admin-ui users and their roles. Monitor the workers and theirs analysis tasks. Manage configuration properties. Show up the stack traces. The profile view Both the user and the project manager profiles have the ability to edit its own account and manage the token authorities related the Eclipse Integrated Development Environments (IDEs) component. The generated token authorities is assigned and available only on current profile. The Workers feature The workers view is dedicated to the dashboard administrators which allows them to monitor the status of the analysis tasks processes of the metric-platform. The Platform Workers section shows up the workers with theirs assigned analysis tasks. The Pending Tasks presents the analysis tasks waiting for a free worker. The Properties management feature The Properties feature allows to configure generic configurations applied to the metric-platform, eg., Github OAuth tokens, .. The Stack Traces feature The Stack traces feature allows to display the errors/stracktraces produced during the metric-platform analysis process in the admin UI to ease debugging.","title":"Platform Administration User Guide"},{"location":"user-guide/administration/#platform-administration-user-guide","text":"The guide show up the different features available through the Administration Dashboard, including: * The Login feature * The Projects management feature * The Users management feature * The Workers feature * The Properties management feature * The Stack Traces feature","title":"Platform Administration User Guide"},{"location":"user-guide/administration/#the-login-feature","text":"The first time the user access the administration dashboard, a login form will be shown in order to authenticate the associated user.","title":"The Login feature"},{"location":"user-guide/administration/#the-projects-management-feature","text":"The Projects view displays the list of the OSS projects downloaded from the OSS forges. The administration dashboard provides two operations: * Import Project: If your project is hosted on an OSS forge, you can simply paste the URL on the field and add it. Create Project: The second operation provides an extra-options to customize the project creation alongside to the metadata available from differents related sources, eg., communication channels and bug tracking systems. Once the project has been registred, it's possible to configure inside it some analysis tasks. An analysis task is consist of: * Label name: the analysis task name. * Task type: the scheduling tasks execution mechanism which could be: * Single Execution: which allows to execute a task between a start date and an end date. * Monitoring Execution: which permits to run a task from a start date until the current date then to schedule the task execution each new day. * Start date: the start time range of the analysis process. * End date: the end time range of the analysis process. * Metric Providers: the metrics available on the metric-platform through the extension points mechanism. These tasks will be executed later to compute/calculate some metrics that will be used to measure the quality of the OSS projects during various period of time.","title":"The Projects management feature"},{"location":"user-guide/administration/#the-users-management-feature","text":"The Users view allows to manage the differents users of the the administration dashboards. It provides three levels of roles: USER ROLE: The user profile is enable to shows up the basic administration dashboard features including: Show up the list of registred projects. Show up the analysis tasks status. Show up the list of metric-providers. Manage its own token authorities. Show up the stack traces. PROJECT MANAGER ROLE: The project manager profile is enable to manage the main administration dashboard features including: Import/create new projects. Show up the list of registred projects. Manage new analysis tasks. Show up the analysis tasks status. Show up the list of metric-providers. Manage its own token authorities. Show up the stack traces. ADMIN ROLE: The admin profile is the super-user of the admin-ui which is enable to get access to almost all the features come with the administration dashboard, including: Import/create new projects. Show up the list of registred projects. Manage new analysis tasks. Show up the analysis tasks status. Show up the list of metric-providers. Manage the admin-ui users and their roles. Monitor the workers and theirs analysis tasks. Manage configuration properties. Show up the stack traces.","title":"The Users management feature"},{"location":"user-guide/administration/#the-profile-view","text":"Both the user and the project manager profiles have the ability to edit its own account and manage the token authorities related the Eclipse Integrated Development Environments (IDEs) component. The generated token authorities is assigned and available only on current profile.","title":"The profile view"},{"location":"user-guide/administration/#the-workers-feature","text":"The workers view is dedicated to the dashboard administrators which allows them to monitor the status of the analysis tasks processes of the metric-platform. The Platform Workers section shows up the workers with theirs assigned analysis tasks. The Pending Tasks presents the analysis tasks waiting for a free worker.","title":"The Workers feature"},{"location":"user-guide/administration/#the-properties-management-feature","text":"The Properties feature allows to configure generic configurations applied to the metric-platform, eg., Github OAuth tokens, ..","title":"The Properties management feature"},{"location":"user-guide/administration/#the-stack-traces-feature","text":"The Stack traces feature allows to display the errors/stracktraces produced during the metric-platform analysis process in the admin UI to ease debugging.","title":"The Stack Traces feature"},{"location":"user-guide/dashboard/","text":"Visualisation Dashboard This guide describes the different dashboards that are available with the platform. Project Dashboard (dev/debug) The Project Dashboard (dev/debug) shows all the metrics available in CROSSMINER, fetched from the SCAVA API. It displays the global value and the evolution in time of the metrics. It includes also filters to ease the analysis of a specific group of metrics. The dashboard is composed of different tables and visualizations, the tables show some statistics on the metrics (e.g., number of metrics per type, name and project), while the visualizations plot the average, sum and max values for every snapshots of a given metric. The Project Dashboard (dev/debug) allows to understand the SCAVA metrics in terms of global and time values. A screenshot of the dashboard is shown below, as can be seen, the tables on the left contain global values, while the visualizations on the right allow to understand the metrics evolution. Finally, the last row of the dashboard shows different numeric values as a result of applying different metrics to the metric values. Overview dashboard The Overview dashboard, starting from top to bottom and from left to right, includes the following visualizations: First of all, there is a table that shows the list of last facts. Then there are two \"selection\" pies, a pie chart that shows the projects. It can be used to filter by project and a pie chart that shows the top projects. It can be used to filter by top project. To follow, there is a table with the last recommendations (they come from the trans.rascal.OO metrics) and a heat map that allows to compare the projects wrt to attributes of the quality model defined in prosoul . Then, there is a table that shows the similar projects in terms of recommendation, it describes if it's active, the type and the number of recommendations. The next row shows a bar chart that shows the evolution of bugs (fixed vs closed/non resolved) and another bar chart that shows the evolution of commits. The next two rows show two bar charts as well, starting with a bar chart that shows the evolution of percentage emotions (anger, joy, sadness, surprise, love) in bugs and a bar chart that shows the evolution of active and inactive users on bug tracker. Finally, the last row includes information about a pie chart and a bar chart that show the top 10 topics on comments and their evolution. The figures below show an example of the dashboard: Factoids dashboard The Factoids dashboard, starting from top to bottom and from left to right, includes the following visualizations in order to understand the factoids that are in the projects: The first table shows the number of projects that have from one to four factoids and the next two \"selection\" pies in order to filter the dashboard by project or top project if it is required. The following radar shows an overview of the project and how the number of factoids are distributed on them, the table shows the last factoids added. The following three heat maps show different specific factoids with the projects, grouped them by bugs, commits and code quality. The last large heat map shows all the projects and all the factoids with the corresponding values. The figures below show an example of the dashboard: Dependency dashboard This Dependency dashboard is useful to show the dependecies. It contains the following visualizations/tables (from left to right, top to bottom): It starts with a visualization that shows the number of total deps and two pie charts, the first one is a pie chart that shows the deps grouped by project and the second one is a pie chart that shows the deps grouped by top project. Then, there are two tables, one that shows the deps grouped by type (e.g., osgi and maven) and project and other table that shows the deps grouped by type and top project. Below the tables, there is a graph that relates projects to dependency names and a bar chart that shows the evolution of project deps. To finish the dashboard has a table that provides details (name, versions, project and datetime) of the project deps and a table that shows the old and new version of each dependency. The figures below show an example of the dashboard: DevOps dependencies Dashboard The Devops Dependencies dashboard, starting from top to bottom and from left to right, includes the following visualizations in order to understand the devops dependencies that are included in the project: First of all, it has a pie chart that shows the deps grouped by project, it has also a pie chart that shows the deps grouped by top project, these two pies can be used to filter the dashboard. then there are two tables, the first one shows the deps grouped by type (e.g., puppet and docker) and project, and the second one shows the deps grouped by type and top project. Below the tables, there is a graph that relates projects to dependency names and a bar chart that shows the evolution of project deps. Then, there are two tables, the first one provides details (name and versions) of the project deps and the second one shows the old and new version of each dependency. To finish, there is a graph that shows the relation between projects that are using configuration files, the relation can be derived from Puppet or Docker, the table shows the type of relation in addition. The figures below show an example of the dashboard: DevOps smells dashboard The DevOps smells dashboard includes two visualizations to focus on a specific project and/or top project. Two bar charts show the evolution of the smells extracted from puppet and docker. They rely on the following historic metrics: - puppet.designsmells.smells - puppet.implementation.smells - docker.smells Two tables allow to focus on the smells details. The former shows information about design and configuration smells, while the latter gives insights about antipatterns smells. Both of them have the information of the file and the line. The figures below show an example of the dashboard: Quality model dashboard The Quality model dashboard shows the quality model assessment based on the model available in prosoul repo It starts with a table that ranks the projects based on the sum of their scores, which are the normalized values (0-5) of the metrics based on the threshold defined in the quality model and a pie chart that shows the projects. It can be used to filter by project. Then there is a heat map that allows to compare projects based on the qm metric scores and a bar chart that shows the qm metric scores (useful to focus on one project). Following, it has a heat map that allows to compare projects based on the qm attributes (median of the corresponding metric scores) and a bar chart that shows the qm attributes (useful to focus on one project). It continues with another heat map that allows to compare projects based on the qm goals (median of the corresponding metric scores) and a bar chart that shows the qm goals (useful to focus on one project). To finish, there are three bar charts that show the evolution in time of the quality models by quarters. The figures below show an example of the dashboard: Users dashboard The Users dashboard is useful to show user activity. It contains the following visualizations/tables (from left to right, top to bottom): First, it has two pie charts, a pie chart that shows the projects (it can be used to filter by project) and a pie chart that shows the top projects (it can be used to filter by top project). Then, there is a table that shows the top users based on the sum of their churns and a bar chart that shows the churn evolution (it can be filtered by user). To finish, there is a bar chart that shows the evolution of active and inactive users on bug tracker. The figure below shows an example of the dashboard: Sentiment and emotion dashboard The Sentiment and emotion dashboard includes 2 visualizations to select a project and top project, a visualization that summarizes the number of emotions, 2 visualizations that show the emotion trend per project based on weighted values and the corresponding details, 2 visualizations that show the sentiment trend per project and details about the sentiment at the beginning and end of the threads. The figure below shows an example of the dashboard:","title":"Visualisation Dashboard"},{"location":"user-guide/dashboard/#visualisation-dashboard","text":"This guide describes the different dashboards that are available with the platform.","title":"Visualisation Dashboard"},{"location":"user-guide/dashboard/#project-dashboard-devdebug","text":"The Project Dashboard (dev/debug) shows all the metrics available in CROSSMINER, fetched from the SCAVA API. It displays the global value and the evolution in time of the metrics. It includes also filters to ease the analysis of a specific group of metrics. The dashboard is composed of different tables and visualizations, the tables show some statistics on the metrics (e.g., number of metrics per type, name and project), while the visualizations plot the average, sum and max values for every snapshots of a given metric. The Project Dashboard (dev/debug) allows to understand the SCAVA metrics in terms of global and time values. A screenshot of the dashboard is shown below, as can be seen, the tables on the left contain global values, while the visualizations on the right allow to understand the metrics evolution. Finally, the last row of the dashboard shows different numeric values as a result of applying different metrics to the metric values.","title":"Project Dashboard (dev/debug)"},{"location":"user-guide/dashboard/#overview-dashboard","text":"The Overview dashboard, starting from top to bottom and from left to right, includes the following visualizations: First of all, there is a table that shows the list of last facts. Then there are two \"selection\" pies, a pie chart that shows the projects. It can be used to filter by project and a pie chart that shows the top projects. It can be used to filter by top project. To follow, there is a table with the last recommendations (they come from the trans.rascal.OO metrics) and a heat map that allows to compare the projects wrt to attributes of the quality model defined in prosoul . Then, there is a table that shows the similar projects in terms of recommendation, it describes if it's active, the type and the number of recommendations. The next row shows a bar chart that shows the evolution of bugs (fixed vs closed/non resolved) and another bar chart that shows the evolution of commits. The next two rows show two bar charts as well, starting with a bar chart that shows the evolution of percentage emotions (anger, joy, sadness, surprise, love) in bugs and a bar chart that shows the evolution of active and inactive users on bug tracker. Finally, the last row includes information about a pie chart and a bar chart that show the top 10 topics on comments and their evolution. The figures below show an example of the dashboard:","title":"Overview dashboard"},{"location":"user-guide/dashboard/#factoids-dashboard","text":"The Factoids dashboard, starting from top to bottom and from left to right, includes the following visualizations in order to understand the factoids that are in the projects: The first table shows the number of projects that have from one to four factoids and the next two \"selection\" pies in order to filter the dashboard by project or top project if it is required. The following radar shows an overview of the project and how the number of factoids are distributed on them, the table shows the last factoids added. The following three heat maps show different specific factoids with the projects, grouped them by bugs, commits and code quality. The last large heat map shows all the projects and all the factoids with the corresponding values. The figures below show an example of the dashboard:","title":"Factoids dashboard"},{"location":"user-guide/dashboard/#dependency-dashboard","text":"This Dependency dashboard is useful to show the dependecies. It contains the following visualizations/tables (from left to right, top to bottom): It starts with a visualization that shows the number of total deps and two pie charts, the first one is a pie chart that shows the deps grouped by project and the second one is a pie chart that shows the deps grouped by top project. Then, there are two tables, one that shows the deps grouped by type (e.g., osgi and maven) and project and other table that shows the deps grouped by type and top project. Below the tables, there is a graph that relates projects to dependency names and a bar chart that shows the evolution of project deps. To finish the dashboard has a table that provides details (name, versions, project and datetime) of the project deps and a table that shows the old and new version of each dependency. The figures below show an example of the dashboard:","title":"Dependency dashboard"},{"location":"user-guide/dashboard/#devops-dependencies-dashboard","text":"The Devops Dependencies dashboard, starting from top to bottom and from left to right, includes the following visualizations in order to understand the devops dependencies that are included in the project: First of all, it has a pie chart that shows the deps grouped by project, it has also a pie chart that shows the deps grouped by top project, these two pies can be used to filter the dashboard. then there are two tables, the first one shows the deps grouped by type (e.g., puppet and docker) and project, and the second one shows the deps grouped by type and top project. Below the tables, there is a graph that relates projects to dependency names and a bar chart that shows the evolution of project deps. Then, there are two tables, the first one provides details (name and versions) of the project deps and the second one shows the old and new version of each dependency. To finish, there is a graph that shows the relation between projects that are using configuration files, the relation can be derived from Puppet or Docker, the table shows the type of relation in addition. The figures below show an example of the dashboard:","title":"DevOps dependencies Dashboard"},{"location":"user-guide/dashboard/#devops-smells-dashboard","text":"The DevOps smells dashboard includes two visualizations to focus on a specific project and/or top project. Two bar charts show the evolution of the smells extracted from puppet and docker. They rely on the following historic metrics: - puppet.designsmells.smells - puppet.implementation.smells - docker.smells Two tables allow to focus on the smells details. The former shows information about design and configuration smells, while the latter gives insights about antipatterns smells. Both of them have the information of the file and the line. The figures below show an example of the dashboard:","title":"DevOps smells dashboard"},{"location":"user-guide/dashboard/#quality-model-dashboard","text":"The Quality model dashboard shows the quality model assessment based on the model available in prosoul repo It starts with a table that ranks the projects based on the sum of their scores, which are the normalized values (0-5) of the metrics based on the threshold defined in the quality model and a pie chart that shows the projects. It can be used to filter by project. Then there is a heat map that allows to compare projects based on the qm metric scores and a bar chart that shows the qm metric scores (useful to focus on one project). Following, it has a heat map that allows to compare projects based on the qm attributes (median of the corresponding metric scores) and a bar chart that shows the qm attributes (useful to focus on one project). It continues with another heat map that allows to compare projects based on the qm goals (median of the corresponding metric scores) and a bar chart that shows the qm goals (useful to focus on one project). To finish, there are three bar charts that show the evolution in time of the quality models by quarters. The figures below show an example of the dashboard:","title":"Quality model dashboard"},{"location":"user-guide/dashboard/#users-dashboard","text":"The Users dashboard is useful to show user activity. It contains the following visualizations/tables (from left to right, top to bottom): First, it has two pie charts, a pie chart that shows the projects (it can be used to filter by project) and a pie chart that shows the top projects (it can be used to filter by top project). Then, there is a table that shows the top users based on the sum of their churns and a bar chart that shows the churn evolution (it can be filtered by user). To finish, there is a bar chart that shows the evolution of active and inactive users on bug tracker. The figure below shows an example of the dashboard:","title":"Users dashboard"},{"location":"user-guide/dashboard/#sentiment-and-emotion-dashboard","text":"The Sentiment and emotion dashboard includes 2 visualizations to select a project and top project, a visualization that summarizes the number of emotions, 2 visualizations that show the emotion trend per project based on weighted values and the corresponding details, 2 visualizations that show the sentiment trend per project and details about the sentiment at the beginning and end of the threads. The figure below shows an example of the dashboard:","title":"Sentiment and emotion dashboard"},{"location":"user-guide/metrics/","text":"Metrics Reference Guide This guide describes the historic and transient metric providers, as well as factoids, provided by the Scava platform. Historic Metric Providers for: Bug Trackers Newsgroups and Forums Commits and Committers Documentation Generic Source Code Java Code OSGi Dependencies Maven Dependencies Docker Dependencies Puppet Dependencies Docker Smells Puppet Smells Transient Metric Providers for: Bug Trackers Newsgroups and forums Documentation Natural Language Processing Commits and Committers Generic Source Code Java Code OSGi Dependencies Maven Dependencies Docker Dependencies Puppet Dependencies Docker Smells Puppet Smells Docker Antipatterns Puppet Antipatterns Projects Relations New Versions Indexing API Factoids for: Bug Trackers Newsgroups and Forums Historic Metric Providers Historic metrics maintain a record of various heuristics associated with a specific open source project over its lifetime. They typically depend on the results from one or more transient metrics and are typically displayed in the Scava dashboards. Historic Metric Providers for Bug Trackers The following Historic Metric Providers are associated with Issue trackers Back to top org.eclipse.scava.metricprovider.historic.bugs.bugs Short name : historic.bugs.bugs Friendly name : Number of bugs per day per bug tracker This metric computes the number of bugs per day for each bug tracker seperately. It also computes additional information such as average comments per bug, average comments per user, average requests and/or replies per user and bug. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata , org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsBugsHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackerData> numberOfBugs int averageCommentsPerBug float averageRequestsPerBug float averageRepliesPerBug float averageCommentsPerUser float averageRequestsPerUser float averageRepliesPerUser float Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfBugs Visualisation Output Information : BugsHistoricMetricProvider : id bugs.bugs id bugs.comments-bugaverage id bugs.comments-useraverage id bugs.requests-bugaverage id bugs.requests-useraverage id bugs.replies-bugaverage id bugs.replies-useraverage id bugs.requestsreplies-useraverage id bugs.requestsreplies-bugaverage Back to top org.eclipse.scava.metricprovider.historic.bugs.comments Short name : historic.bugs.comments Friendly name : Number of bug comments per day per bug tracker This metric computes the number of bug comments submitted by the community (users) per day for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.comments Returns : BugsCommentsHistoricMetric which contains: Variable Type Bugs List<DailyBugData> numberOfComments int cumulativeNumberOfComments int Additional Information : DailyBugData : String bugTrackerID int numberOfComments int cumulativeNumberOfComments Visualisation Output Information : CommentsHistoricMetricProvider : id bugs.comments id bugs.cumulativeComments Back to top org.eclipse.scava.metricprovider.historic.bugs.emotions Short name : historic.bugs.emotions Friendly name : Number of emotions per day per bug tracker This metric computes the emotional dimensions present in bug comments submitted by the community (users) per day for each bug tracker. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise). Depends-on : org.eclipse.scava.metricprovider.trans.bugs.emotions Returns : BugsEmotionsHistoricMetric which contains: Variable Type bugData List<BugData> Dimensions List<Dimensions> Additional Information : BugData : String bugTrackerID int numberOfComments int cumulativeNumberOfComments Dimensions : String bugTrackerId String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfComments int cumulativeNumberOfComments float percentage float cumulativePercentage Visualisation Output Information : EmotionsHistoricMetricProvider : id bugs.emotions.cumulativeComments id bugs.emotions.cumulativeCommentPercentages id bugs.emotions.comments id bugs.emotions.commentPercentages Back to top org.eclipse.scava.metricprovider.historic.bugs.migrationissues Short name : historic.bugs.migrationissues Friendly name : Migration Issues Detection in Bug Trackers per day per bug tracker This metric stores how many migration issues have been found per day for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.migrationissues Returns : BugTrackerMigrationIssueHistoricMetric which contains: Variable Type dailyBugTrackerMigrationData List<DailyBugTrackerMigrationData> Additional Information : DailyBugTrackerMigrationData : String bugTrackerId List<String> bugsId; int numberOfBugs Visualisation Output Information : BugTrackerMigrationIssueHistoricMetricProvider : id bugs.dailymigrationissues Back to top org.eclipse.scava.metricprovider.historic.bugs.migrationissuesmaracas Short name : historic.bugs.migrationissuesmaracas Friendly name : Migration Issues Detection along with Maracas in Bug Trackers per day per bug tracker This metric stores how many migration issues have been found containing changes detected with MARACAS per day for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.migrationissuesmaracas Returns : BugTrackerMigrationIssueMaracasHistoricMetric which contains: Variable Type dailyBugTrackerMigrationMaracasData List<DailyBugTrackerMigrationMaracasData> bugTrackerMigrationMaracasData List<BugTrackerMigrationMaracasData> Additional Information : DailyBugTrackerMigrationMaracasData : String bugTrackerId List<String> bugsId; int numberOfIssues BugTrackerMigrationMaracasData : String bugTrackerId String bugId; List<String> changesAndMatchingPercentage Visualisation Output Information : BugTrackerMigrationIssueMaracasHistoricMetricProvider : id bugs.dailymigrationissuesmaracas id bugs.migrationissuesmaracas.changes Back to top org.eclipse.scava.metricprovider.historic.bugs.newbugs Short name : historic.bugs.newbugs Friendly name : Number of new bugs per day per bug tracker This metric computes the number of new bugs reported by the community (users) per day for each bug tracker. A small number of bug reports can indicate either a bug-free, robust project or a project with a small/inactive user community. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.newbugs Returns : BugsNewBugsHistoricMetric which contains: Variable Type dailyBugData List<DailyBugData> numberOfBugs int cumulativeNumberOfBugs int Additional Information : DailyBugData : String bugTrackerId int numberOfBugs int cumulativeNumberOfBugs Visualisation Output Information : NewUsersHistoricMetricProvider : id bugs.cumulativeNewUsers id bugs.newUsers Back to top org.eclipse.scava.metricprovider.historic.bugs.newusers Short name : historic.bugs.newusers Friendly name : Number of new users per day per bug tracker This metric computes the number of new users per day for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsNewUsersHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackerData> numberOfNewUsers int cumulativeNumberOfNewUsers int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfNewUsers int cumulativeNumberOfNewUsers Visualisation Output Information : NewUsersHistoricMetricProvider : id bugs.cumulativeNewUsers id bugs.newUsers Back to top org.eclipse.scava.metricprovider.historic.bugs.opentime Short name : historic.bugs.opentime Friendly name : Average duration to close an open bug This metric computes the average duration between creating and closing bugs. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : OpenTimeHistoricMetricProvider which contains: Variable Type avgBugOpenTime String avgBugOpenTimeInDays double bugsConsidered int Visualisation Output Information : OpenTimeHistoricMetricProvider : id bugs.bugOpenTime id bugs.bugOpenTime-bugs Back to top org.eclipse.scava.metricprovider.historic.bugs.patches Short name : historic.bugs.patches Friendly name : Number of bug patches per day This class computes the number of bug patches per day, for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.patches Returns : PatchesHistoricMetricProvider which contains: Variable Type numberOfPatches int cumulativeNumberOfPatches int bugs List<DailyBugData> Additional Information : DailyBugData : String bugTrackerId int numberOfPatches int cumulativeNumberOfPatches Visualisation Output Information : PatchesHistoricMetricProvider : id bugs.cumulativePatches id bugs.patches Back to top org.eclipse.scava.metricprovider.historic.bugs.requestsreplies Short name : historic.bugs.requestsreplies Friendly name : Number of request and replies in bug comments per bug tracker This metric computes the number of requests and replies realting to comments posted to bugs by the community (users) per day for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsRequestsRepliesHistoricMetric which contains: Variable Type Bugs List<DailyBugTrackerData> numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies Visualisation Output Information : RequestsRepliesHistoricMetricProvider : id bugs.replies id bugs.cumulativereplies id bugs.requests id bugs.cumulativerequests id bugs.requestsreplies id bugs.cumulativerequestsreplies Back to top org.eclipse.scava.metricprovider.historic.bugs.requestsreplies.average Short name : historic.bugs.requestsreplies.average Friendly name : Average number of requests and replies in bug comments per bug tracker This metric computes the average number of bug comments considered as request and reply for each bug tracker per day. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsRequestsRepliesHistoricMetric which contains: Variable Type bugs List<DailyBugTrackerData> numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies Visualisation Output Information : RequestsRepliesHistoricMetricProvider : id bugs.requests-averageperday id bugs.requestsreplies-averageperday id bugs.comments-averageperday id bugs.replies-averageperday Back to top org.eclipse.scava.metricprovider.historic.bugs.responsetime Short name : historic.bugs.responsetime Friendly name : Average response time to open bugs per bug tracker This metric computes the average time in which the community (users) responds to open bugs per day for each bug tracker seperately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsResponseTimeHistoricMetric which contains: Variable Type bugTrackerId String avgResponseTimeFormatted String cumulativeAvgResponseTimeFormatted String avgResponseTime float cumulativeAvgResponseTime float bugsConsidered int cumulativeBugsConsidered int Visualisation Output Information : ResponseTimeHistoricMetricProvider : id bugs.averageResponseTime id bugs.cumulativeAverageResponseTime id bugs.cumulativeAverageResponseTime-bugs id bugs.averageResponseTime-bugs Back to top org.eclipse.scava.metricprovider.historic.bugs.sentiment Short name : historic.bugs.sentiment Friendly name : Overall sentiment per bug tracker This metric computes the overall sentiment per bug tracker up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score for each bug contributes equally, regardless of it's size. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSentimentHistoricMetric which contains: Variable Type overallAverageSentiment float overallSentimentAtThreadBeggining float overallSentimentAtThreadEnd float Visualisation Output Information : SentimentHistoricMetricProvider : id bugs.averageSentiment id bugs.sentimentAtThreadEnd id bugs.sentimentAtThreadBeggining id bugs.sentiment The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. Back to top org.eclipse.scava.metricprovider.historic.bugs.severity Short name : historic.bugs.severity Friendly name : Number of bugs per severity level per bug tracker This metric computes the number of severity levels for bugs submitted by the community (users) every day for each bug tracker. Specifically, it calculates the number and percentage of bugs that have been categorised into 1 of 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification Returns : BugsSeveritiesHistoricMetric which contains: Variable Type bugData List<BugData> severityLevels List<ServerityLevel> Additional Information : BugData : String bugTrackerId int numberOfBugs SeverityLevel : String bugTrackerId String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs int percentage Visualisation Output Information : SeverityHistoricMetricProvider : id bugs.severity id bugs.severity.percentages Back to top org.eclipse.scava.metricprovider.historic.bugs.severitybugstatus Short name : historic.bugs.severitybugstatus Friendly name : Number of each bug status per bug severity level This metric computes the total number and percentage of each bug status per severity level, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate) and 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs int numberOfWontFixBugs int numberOfWorksForMeBugs int numberOfNonResolvedClosedBugs int numberOfInvalidBugs int numberOfFixedBugs int numberOfDuplicateBugs float percentageOfResolvedClosedBugs float percentageOfWontFixBugs float percentageOfWorksForMeBugs float percentageOfNonResolvedClosedBugs float percentageOfInvalidBugs float percentageOfFixedBugs float percentageOfDuplicateBugs Visualisation Output : SeverityBugStatusHistoricMetricProvider : id bugs.severity.duplicateBugs id bugs.severity.duplicateBugs.percentages id bugs.severity.fixedBugs id bugs.severity.fixedBugs.percentages id bugs.severity.invalidBugs id bugs.severity.invalidBugs.percentages id bugs.severity.nonResolvedClosedBugs id bugs.severity.nonResolvedClosedBugs.percentages id bugs.severity.resolvedClosedBugs id bugs.severity.resolvedClosedBugs.percentages id bugs.severity.wontFixBugs id bugs.severity.wontFixBugs.percentages id bugs.severity.worksForMeBugs id bugs.severity.worksForMeBugs.percentages Back to top org.eclipse.scava.metricprovider.historic.bugs.severityresponsetime Short name : historic.bugs.severityresponsetime Friendly name : Average response time to bugs per severity level per day This metric computes the average time in which the community (users) responds to open bugs per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) String avgResponseTimeFormatted int numberOfBugs long avgResponseTime Visualisation Output Information : SeverityResponseTimeHistoricMetricProvider : id bugs.severity.averageResponseTime Back to top org.eclipse.scava.metricprovider.historic.bugs.severitysentiment Short name : historic.bugs.severitysentiment Friendly name : Average sentiment per bugs severity level per day This metric computes for each bug severity level, the average sentiment, sentiment at the begining and end of bug comments posted by the community (users) every day for each bug tracker. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). There are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs float averageSentiment float sentimentAtThreadBeggining float sentimentAtThreadEnd Visualisation Output Information : SeveritySentimentHistoricMetricProvider : id bugs.severity.sentiment id bugs.severity.averageSentiment id bugs.severity.sentimentAtThreadBeggining id bugs.severity.sentimentAtThreadEnd The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. Back to top org.eclipse.scava.metricprovider.historic.bugs.status Short name : historic.bugs.status Friendly name : Number of bugs per bug status per day This metric computes the total number of bugs that corresponds to each bug status, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate). Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsStatusHistoricMetric which contains: Variable Type numberOfBugs long numberOfResolvedClosedBugs int numberOfWontFixBugs int numberOfWorksForMeBugs int numberOfNonResolvedClosedBugs int numberOfInvalidBugs int numberOfFixedBugs int numberOfDuplicateBugs int Visualisation Output Information : StatusHistoricMetricProvider : id bugs.duplicateBugs id bugs.fixedBugs id bugs.invalidBugs id bugs.nonResolvedClosedBugs id bugs.wontFixBugs id bugs.worksForMeBugs id bugs.resolvedClosedBugs Back to top org.eclipse.scava.metricprovider.historic.bugs.topics Short name : historic.bugs.topics Friendly name : Labels of topic clusters in bug comments per bug tracker This metric computes the labels of topic clusters extracted from bug comments submitted by the community (users), per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.topics Returns : BugsTopicsHistoricMetric which contains: Variable Type bugTopics List<BugTopic> Additional Information : SeverityLevel : String bugTrackerId List<String> labels float numberOfDocuments List<String> commentsId Back to top org.eclipse.scava.metricprovider.historic.bugs.unansweredbugs Short name : historic.bugs.unansweredbugs Friendly name : Number of unanswered bugs per day This metric computes the number of unanswered bugs per day. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsUnansweredBugsHistoricMetric which contains: Variable Type numberOfUnansweredBugs int Visualisation Output Information : UnansweredThreadsHistoricMetricProvider : id bugs.unansweredBugs Back to top org.eclipse.scava.metricprovider.historic.bugs.users Short name : historic.bugs.users Friendly name : Number of users, active and inactive per day per bug tracker This metric computes the number of users, number of active and inactive users per day for each bug tracker separately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsUsersHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackingData> numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers int Additional Information : DailyBugTrackingData : String bugTrackerId int numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers Visualisation Output Information : UsersHistoricMetricProvider : id bugs.users id bugs.activeusers id bugs.inactiveusers Back to top Historic Metric Providers for Newsgroups and Forums The following Historic Metric Providers are associated with newsgroups. Back to top org.eclipse.scava.metricprovider.historic.newsgroups.articles Short name : historic.newsgroups.articles Friendly name : Number of articles per day per news group This metric computes the number of articles submitted by the community (users) per day for each newsgroup separately Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.articles Returns : NewsgroupsArticlesHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfArticles int cummulativeNumberOfArticles Visualisation Output Information : ArticlesHistoricMetricProvider : id newsgroups.articles id newsgroups.cumulativeArticles Back to top org.eclipse.scava.metricprovider.historic.newsgroups.emotions Short name : historic.newsgroups.emotions Friendly name : Number of emotions per day per newsgroup This metric computes the emotional dimensions present in newsgroup comments submitted by the community (users) per day for each newsgroup. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise). Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.emotions Returns : NewsgroupsEmotionsHistoricMetric which contains: Variable Type newsgroupsData List<Newsgroups> emotionDimension List<Emotion> Additional Information : NewsgroupsData : String newsgroupName int numberOfArticles int cummulativeNumberOfArticles EmotionDimension : String newsgroupName String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfArticles int cumulativeNumberOfArticles float percentage float cumulativePercentage Visualisation Output Information : EmotionsHistoricMetricProvider : id newsgroups.emotions.articlePercentages id newsgroups.emotions.cumulativeArticles id newsgroups.emotions.cumulativeArticlePercentages id newsgroups.emotions.articles Back to top org.eclipse.scava.metricprovider.historic.newsgroups.migrationissues Short name : historic.newsgroups.migrationissues Friendly name : Migration Issues Detection in articles per day per newsgroup This metric detects migration issues in articles per day for each newgroup. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.migrationissues Returns : NewsgroupsMigrationIssueHistoricMetric which contains: Variable Type dailyNewsgroupsMigrationData List<DailyNewsgroupsMigrationData> Additional Information : DailyBugTrackerMigrationData : String newsgroupName List<Integer> threadsId int numberOfIssues Visualisation Output Information : NewsgroupsMigrationIssueHistoricMetricProvider : id newsgroups.dailymigrationissues Back to top org.eclipse.scava.metricprovider.historic.newsgroups.migrationissuesmaracas Short name : historic.newsgroups.migrationissuesmaracas Friendly name : Migration Issues Detection along with Maracas in articles per day per newsgroup This metric stores how many migration issues have been found containing changes detected with MARACAS per day for each newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.migrationissuesmaracas Returns : NewsgroupMigrationIssueMaracasHistoricMetric which contains: Variable Type dailyNewsgroupMigrationMaracasData List<DailyNewsgroupMigrationMaracasData> newsgroupMigrationMaracasData List<NewsgroupMigrationMaracasData> Additional Information : DailyNewsgroupMigrationMaracasData : String newsgroupName List<Integer> threadsId; int numberOfIssues NewsgroupMigrationMaracasData : String newsgroupName int threadId; List<String> changesAndMatchingPercentage Visualisation Output Information : NewsgroupMigrationIssueMaracasHistoricMetricProvider : id newsgroups.dailymigrationissuesmaracas id newsgroups.migrationissuesmaracas.changes Back to top org.eclipse.scava.metricprovider.historic.newsgroups.newthreads Short name : historic.newsgroups.newthreads Friendly name : Number of new threads per day per newsgroup This metric computes the number of new threads submitted by the community (users) per day for each newsgroup separately Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : NewsgroupsNewThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfNewThreads int cummulativeNumberOfNewThreads Visualisation Output Information : NewThreadsHistoricMetricProvider : id newsgroups.newThreads id newsgroups.cumulativeNewThreads Back to top org.eclipse.scava.metricprovider.historic.newsgroups.newusers Short name : historic.newsgroups.newusers Friendly name : Number of new users per day per newsgroup This metric computes the number of new users per day for each newsgroup seperately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsNewUsersHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfNewUsers int cummulativeNumberOfNewUsers Visualisation Output Information : NewUsersHistoricMetricProvider: id newsgroups.cumulativeNewUsers id newsgroups.newUsers Back to top org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies Short name : historic.newsgroups.requestsreplies Friendly name : Number of requests and replies in articles per day This metric computes the number of requests and replies in newsgroup articles submitted by the community (users) per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsRequestsRepliesHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies Visualisation Output Information : RequestsRepliesHistoricMetricProvider : id newsgroups.requests id newsgroups.cumulativerequests id newsgroups.replies id newsgroups.cumulativereplies id newsgroups.requestsreplies id newsgroups.cumulativerequestsreplies Back to top org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies.average Short name : historic.newsgroups.requestsreplies.average Friendly name : Average number of articles, requests and replies per day This metric computes the average number of newsgroup articles, including the number of requests and replies within the newsgroup articles per day. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsRequestsRepliesAverageHistoricMetric which contains: Variable Type averageArticlesPerDay float averageRequestsPerDay float averageRepliesPerDay float Visualisation Output Information : RequestsRepliesAverageHistoricMetricProvider : id newsgroups.requestsreplies-averageperday id newsgroups.requests-averageperday id newsgroups.replies-averageperday id newsgroups.comments-averageperday Back to top org.eclipse.scava.metricprovider.historic.newsgroups.responsetime Short name : historic.newsgroups.responsetime Friendly name : Average response time to threads per day per newsgroup This metric computes the average time in which the community responds to open threads per day for each newsgroup separately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsResponseTimeHistoricMetric which contains: Variable Type newsgroupName String avgResponseTime long avgResponseTimeFormatted String threadsConsidered int cumulativeAvgResponseTimeFormatted String cumulativeThreadsConsidered int Visualisation Output Information : ResponseTimeHistoricMetricProvider : id newsgroups.averageResponseTime id newsgroups.cumulativeAverageResponseTime id newsgroups.cumulativeAverageResponseTime-threads id newsgroups.averageResponseTime-threads Back to top org.eclipse.scava.metricprovider.historic.newsgroups.sentiment Short name : historic.newsgroups.sentiment Friendly name : Overall sentiment of newsgroup articles This metric computes the overall sentiment per newsgroup repository up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score of each thread contributes equally, irrespective of its size. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.sentiment Returns : NewsgroupsSentimentHistoricMetric which contains: Variable Type overallAverageSentiment float overallSentimentAtThreadBegining float overallSentimentAtThreadEnd float Visualisation Output Information : SentimentHistoricMetricProvider : id newsgroups.averageSentiment id newsgroups.sentimentAtThreadEnd id newsgroups.sentimentAtThreadBeggining id newsgroups.sentiment The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. Back to top org.eclipse.scava.metricprovider.historic.newsgroups.severity Short name : historic.newsgroups.severity Friendly name : Number of each severity level in newsgroup threads per day This metric computes the number of each severity levels in threads submitted every day, per newsgroup. There are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification Returns : NewsgroupsSeveritiesHistoricMetric which contains: Variable Type newsgroupData List<Newsgroups> severityLevel List<SeverityLevel> Additional Information : NewsgroupData : String newsgroupName int numberThreads SeverityLevel : String newsgroupName String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads float percentage Visualisation Output Information : SeverityHistoricMetricProvider : id newsgroups.severity id newsgroups.severity.percentages Back to top org.eclipse.scava.metricprovider.historic.newsgroups.severityresponsetime Short name : historic.newsgroups.severityresponsetime Friendly name : Average response time to threads per severity level per day This metric computes the average time in which the community (users) responds to open threads per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Average response time to threads per severity level This metric computes the average response time for newsgroup threads submitted every day, based on their severity levels. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsSeverityResponseTimeHistoricMetric which contains: Variable Type severityLevel List<SeverityLevel> Additional Information : SeverityLevel : String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads long avgResponseTime String avgResponseTimeFormatted Visualisation Output Information : SeverityResponseTimeHistoricMetricProvider : id newsgroups.severity.averageResponseTime Back to top org.eclipse.scava.metricprovider.historic.newsgroups.severitysentiment Short name : historic.newsgroups.severitysentiment Friendly name : Average sentiment in threads per severity level per day This metric computes the average sentiment, the sentiment at the beginning of threads and the sentiment at the end of threads; for each severity level in newsgroup threads submitted every day. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). Note: there are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.newsgroups.sentiment Returns : NewsgroupsSeveritySentimentHistoricMetric which contains: Variable Type severityLevel List<SeverityLevel> Additional Information : SeverityLevel : String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads float avgSentiment float avgSentimentThreadBeginning float avgSentimentThreadEnd Visualisation Output Information : SeveritySentimentHistoricMetricProvider : id newsgroups.severity.averageSentiment id newsgroups.severity.sentiment id newsgroups.severity.sentimentAtThreadEnd id newsgroups.severity.sentimentAtThreadBeggining The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. Back to top org.eclipse.scava.metricprovider.historic.newsgroups.threads Short name : historic.newsgroups.threads Friendly name : Number of threads per day per newsgroup This metric computes the number of threads per day for each newsgroup separately. The metric also computes average values for articles per thread, requests per thread, replies per thread, articles per user, requests per user and replies per user. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfThreads float averageArticlesPerThread float averageRequestsPerThread float averageRepliesPerThread float averageArticlesPerUser float averageRequestsPerUser float averageRepliesPerUser Visualisation Output Information : ThreadsHistoricMetricProvider : id newsgroups.threads id newsgroups.articles-threadaverage id newsgroups.articles-useraverage id newsgroups.requests-threadaverage id newsgroups.requests-useraverage id newsgroups.replies-threadaverage id newsgroups.replies-useraverage id newsgroups.requestsreplies-threadaverage id newsgroups.requestsreplies-useraverage Back to top org.eclipse.scava.metricprovider.historic.newsgroups.topics Short name : historic.newsgroups.topics Friendly name : Labels of newsgroup topics per newsgroup This metric computes the labels of topics clusters in articles submitted by the community (users), for each newsgroup seperately. Depends-on : org.eclipse.scava.metricprovider.trans.topics Returns : NewsgroupTopicsHistoricMetric which contains: Variable Type newsgroupTopic List<NewsgrpTopic> Additional Information : NewsgroupTopic : String newsgroupName List<String> labels int numberOfDocuments List<Integer> articlesId Back to top org.eclipse.scava.metricprovider.historic.newsgroups.unansweredthreads Short name : historic.newsgroups.unansweredthreads Friendly name : Number of unanswered threads per day per newsgroup This metric computes the number of unanswered threads per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsUnansweredThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfUnansweredThreads Visualisation Output Information : UnansweredThreadsHistoricMetricProvider : id newsgroups.unansweredThreads Back to top org.eclipse.scava.metricprovider.historic.newsgroups.users Short name : historic.newsgroups.users Friendly name : Number of users, active and inactive per day per newsgroup This metric computes the number of users, including active and inactive users per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsUsersHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers Visualisation Output Information : UsersHistoricMetricProvider : id newsgroups.users id newsgroups.activeusers id newsgroups.inactiveusers id newsgroups.activeinactiveusers Back to top Historic Metric Providers for Commits and Committers The following Historic Metric Providers are related to the commits and committers of a project. Back to top trans.rascal.activecommitters.committersoverfile.historic Short name : giniCommittersOverFile.historic Friendly name : Historic giniCommittersOverFile Historic version of : trans.rascal.activecommitters.committersoverfile Calculates the gini coefficient of committers per file Depends-on : trans.rascal.activecommitters.committersoverfile Returns : real Back to top trans.rascal.activecommitters.percentageOfWeekendCommits.historic Short name : percentageOfWeekendCommits.historic Friendly name : Historic percentageOfWeekendCommits Historic version of : trans.rascal.activecommitters.percentageOfWeekendCommits Percentage of commits made during the weekend Depends-on : trans.rascal.activecommitters.percentageOfWeekendCommits Returns : int Back to top trans.rascal.activecommitters.commitsPerDeveloper.historic Short name : commitsPerDeveloper.historic Friendly name : Historic commitsPerDeveloper Historic version of : trans.rascal.activecommitters.commitsPerDeveloper The number of commits per developer indicates not only the volume of the contribution of an individual but also the style in which he or she commits, when combined with other metrics such as churn. Few and big commits are different from many small commits. This metric is used downstream by other metrics as well. Depends-on : trans.rascal.activecommitters.commitsPerDeveloper Returns : map[str, int] Back to top trans.rascal.activecommitters.numberOfActiveCommittersLongTerm.historic Short name : numberOfActiveCommittersLongTerm.historic Friendly name : Historic numberOfActiveCommittersLongTerm Historic version of : trans.rascal.activecommitters.numberOfActiveCommittersLongTerm Number of long time active committers over time (active in last year). This measures a smooth window of one year, where every day we report the number of developers active in the previous 365 days. Depends-on : trans.rascal.activecommitters.numberOfActiveCommittersLongTerm Returns : int Back to top trans.rascal.activecommitters.numberOfActiveCommitters.historic Short name : numberOfActiveCommitters.historic Friendly name : Historic numberOfActiveCommitters Historic version of : trans.rascal.activecommitters.numberOfActiveCommitters Number of active committers over time (active in last two weeks). This measures a smooth window of two weeks, where every day we report the number of developers in the previous 14 days. Depends-on : trans.rascal.activecommitters.numberOfActiveCommitters Returns : int Back to top rascal.generic.churn.commitsToday.historic Short name : commitsToday.historic Friendly name : Historic commitsToday Historic version of : rascal.generic.churn.commitsToday Counts the number of commits made today. Depends-on : rascal.generic.churn.commitsToday Returns : int Back to top rascal.generic.churn.churnToday.historic Short name : commitsToday.historic Friendly name : Historic commitsToday Historic version of : rascal.generic.churn.churnToday Counts the churn for today: the total number of lines of code added and deleted. This metric is used further downstream to analyze trends. Depends-on : rascal.generic.churn.churnToday Returns : int Back to top rascal.generic.churn.churnPerCommitInTwoWeeks.historic Short name : churnPerCommitInTwoWeeks.historic Friendly name : Historic churnPerCommitInTwoWeeks Historic version of : rascal.generic.churn.churnPerCommitInTwoWeeks The ratio between the churn and the number of commits indicates how large each commit is on average. We compute this as a sliding average over two weeks which smoothens exceptions and makes it possible to see a trend historically. Commits should not be to big all the time, because that would indicate either that programmers are not focusing on well-defined tasks or that the system architecture does not allow for separation of concerns. Depends-on : rascal.generic.churn.churnPerCommitInTwoWeeks Returns : int Back to top rascal.generic.churn.filesPerCommit.historic Short name : numberOfFilesPerCommit.historic Friendly name : Historic numberOfFilesPerCommit Historic version of : rascal.generic.churn.filesPerCommit Counts the number of files per commit to find out about the separation of concerns in the architecture or in the tasks the programmers perform. This metric is used further downstream. Depends-on : rascal.generic.churn.filesPerCommit Returns : map[loc, int] Back to top rascal.generic.churn.churnPerCommit.historic Short name : churnPerCommit.historic Friendly name : Historic churnPerCommit Historic version of : rascal.generic.churn.churnPerCommit Count churn. Churn is the number lines added or deleted. We measure this per commit because the commit is a basic unit of work for a programmer. This metric computes a table per commit for today and is not used for comparison between projects. It is used further downstream to analyze activity. Depends-on : rascal.generic.churn.churnPerCommit Returns : map[loc, int] Back to top rascal.generic.churn.churnPerCommitter.historic Short name : churnPerCommitter.historic Friendly name : Historic churnPerCommitter Historic version of : rascal.generic.churn.churnPerCommitter Count churn per committer: the number of lines of code added and deleted. It zooms in on the single committer producing a table which can be used for downstream processing. Depends-on : rascal.generic.churn.churnPerCommitter Returns : map[str author, int churn] Back to top rascal.generic.churn.commitsInTwoWeeks.historic Short name : commitsInTwoWeeks.historic Friendly name : Historic commitsInTwoWeeks Historic version of : rascal.generic.churn.commitsInTwoWeeks Churn in the last two weeks: aggregates the number of commits over a 14-day sliding window. Depends-on : rascal.generic.churn.commitsInTwoWeeks Returns : int Back to top rascal.generic.churn.churnInTwoWeeks.historic Short name : churnInTwoWeeks.historic Friendly name : Historic churnInTwoWeeks Historic version of : rascal.generic.churn.churnInTwoWeeks Churn in the last two weeks: aggregates the lines of code added and deleted over a 14-day sliding window. Depends-on : rascal.generic.churn.churnInTwoWeeks Returns : int Back to top org.eclipse.scava.metricprovider.historic.commits.messages.topics Short name : historic.commits.messages.topics Friendly name : Labels of topics in commits messages analyzed in the last 30 days This metric computes the labels of topic clusters in commits messages pushed by users in the last 30 days Depends-on : org.eclipse.scava.metricprovider.trans.commits.message.topics Returns : CommitsMessagesTopicsHistoricMetric which contains: Variable Type commitMessageTopics List<CommitMessageTopic> Additional Information : CommitMessageTopic : String repository String labels int numberOfMessages List<String> commitsMessageId Visualisation Output Information : CommitsMessagesTopicsHistoricMetricProvider : id commits.topics.messages Back to top Historic Metric Providers for Documentation The following Historic Metric Providers are associated with documentation analyses. Back to top org.eclipse.scava.metricprovider.historic.documentation.readability Short name : historic.documentation.readability Friendly name : Documentation readability Historic Metric Historic metric that stores the evolution of the documentation readability. The higher the readability score, the harder to understand the text. Depends-on : org.eclipse.scava.metricprovider.trans.documentation.readability Returns : DocumentationReadabilityHistoricMetric which contains: Variable Type documentationReadability List<DocumentationHistoricReadability> documentationEntriesReadability List<DocumentationEntryHistoricReadability> Additional Information : DocumentationHistoricReadability : String documentationId int numberOfDocumentationEntries double averageDocumentationReadability DocumentationEntryHistoricReadability : String documentationId String entryId double readability Visualisation Output Information : readability : id documentation.readability.entries id documentation.readability Back to top org.eclipse.scava.metricprovider.historic.documentation.sentiment Short name : historic.documentation.sentiment Friendly name : Documentation sentiment polarity Historic Metric Historic metric that stores the evolution of the documentation sentiment polarity. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment) Depends-on : org.eclipse.scava.metricprovider.trans.documentation.sentiment Returns : DocumentationSentimentHistoricMetric which contains: Variable Type documentationSentiment List<DocumentationHistoricSentiment> documentationEntriesSentiment List<DocumentationEntryHistoricSentiment> Additional Information : DocumentationHistoricSentiment : String documentationId int numberOfDocumentationEntries double averageDocumentationSentiment DocumentationEntryHistoricSentiment : String documentationId String entryId String polarity Visualisation Output Information : sentiment : id documentation.sentiment.entries id documentation.sentiment Back to top Historic Metric Providers for Generic Source Code These metrics are related to the source code of analyzed projects, regardless of the language(s) they are written in. Back to top trans.rascal.clones.cloneLOCPerLanguage.historic Short name : cloneLOCPerLanguage.historic Friendly name : Historic cloneLOCPerLanguage Historic version of : trans.rascal.clones.cloneLOCPerLanguage Lines of code in Type I clones larger than 6 lines, per language. A Type I clone is a literal clone. A large number of literal clones is considered to be bad. This metric is not easily compared between systems because it is not size normalized yet. We use it for further processing downstream. You can analyze the trend over time using this metric. Depends-on : trans.rascal.clones.cloneLOCPerLanguage Returns : map[str, int] Back to top trans.rascal.readability.fileReadabilityQuartiles.historic Short name : fileReadabilityQ.historic Friendly name : Historic fileReadabilityQ Historic version of : trans.rascal.readability.fileReadabilityQuartiles We measure file readability by counting exceptions to common usage of whitespace in source code, such as spaces after commas. The quartiles represent how many of the files have how many of these deviations. A few deviations per file is ok, but many files with many deviations indicates a lack of attention to readability. Depends-on : trans.rascal.readability.fileReadabilityQuartiles Returns : map[str, real] Back to top trans.rascal.comments.commentLinesPerLanguage.historic Short name : commentLinesPerLanguage.historic Friendly name : Historic commentLinesPerLanguage Historic version of : trans.rascal.comments.commentLinesPerLanguage Number of lines containing comments per language (excluding headers). The balance between comments and code indicates understandability. Too many comments are often not maintained and may lead to confusion, not enough means the code lacks documentation explaining its intent. This is a basic fact collection metric which is used further downstream. Depends-on : trans.rascal.comments.commentLinesPerLanguage Returns : map[str, int] Back to top trans.rascal.comments.commentedOutCodePerLanguage.historic Short name : commentedOutCodePerLanguage.historic Friendly name : Historic commentedOutCodePerLanguage Historic version of : trans.rascal.comments.commentedOutCodePerLanguage Lines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how much source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator. Depends-on : trans.rascal.comments.commentedOutCodePerLanguage Returns : map[str, int] Back to top trans.rascal.comments.headerPercentage.historic Short name : headerPercentage.historic Friendly name : Historic headerPercentage Historic version of : trans.rascal.comments.headerPercentage Percentage of files with headers is an indicator for the amount of files which have been tagged with a copyright statement (or not). If the number is low this indicates a problem with the copyright of the program. Source files without a copyright statement are not open-source, they are owned, in principle, by the author and may not be copied without permission. Note that the existence of a header does not guarantee the presence of an open-source license, but its absence certainly is telling. Depends-on : trans.rascal.comments.headerPercentage Returns : real Back to top trans.rascal.LOC.genericLOCoverFiles.historic Short name : giniLOCOverFiles.historic Friendly name : Historic giniLOCOverFiles Historic version of : trans.rascal.LOC.genericLOCoverFiles We find out how evenly the code is spread over files. The number should be quite stable over time. A jump in this metric indicates a large change in the code base. If the code is focused in only a few very large files then this may be a contra-indicator for quality. Depends-on : trans.rascal.LOC.genericLOCoverFiles Returns : real Back to top trans.rascal.LOC.locPerLanguage.historic Short name : locPerLanguage.historic Friendly name : Historic locPerLanguage Historic version of : trans.rascal.LOC.locPerLanguage Physical lines of code simply counts the number of newline characters (OS independent) in a source code file. We accumulate this number per programming language. The metric can be used to compare the volume between two systems and to assess in which programming language the bulk of the code is written. Depends-on : trans.rascal.LOC.locPerLanguage Returns : map[str, int] Back to top Historic Metric Providers for Java code These metrics are related to the Java source code of analyzed projects. Back to top style.filesWithErrorProneness.historic Short name : filesWithErrorProneness.historic Friendly name : Historic filesWithErrorProneness Historic version of : style.filesWithErrorProneness Percentage of files with error proneness Depends-on : style.filesWithErrorProneness Returns : int Back to top style.filesWithUnderstandabilityIssues.historic Short name : filesWithUnderstandabilityIssues.historic Friendly name : Historic filesWithUnderstandabilityIssues Historic version of : style.filesWithUnderstandabilityIssues Percentage of files with understandability issues. This is a basic metric which can not be easily compared between projects. Depends-on : style.filesWithUnderstandabilityIssues Returns : int Back to top style.spreadOfStyleViolations.historic Short name : spreadOfStyleViolations.historic Friendly name : Historic spreadOfStyleViolations Historic version of : style.spreadOfStyleViolations Between 0 and 1 how evenly spread are the style violations. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.spreadOfStyleViolations Returns : real Back to top style.filesWithInefficiencies.historic Short name : filesWithInefficiencies.historic Friendly name : Historic filesWithInefficiencies Historic version of : style.filesWithInefficiencies Percentage of files with inefficiencies Depends-on : style.filesWithInefficiencies Returns : int Back to top style.filesWithStyleViolations.historic Short name : filesWithStyleViolations.historic Friendly name : Historic filesWithStyleViolations Historic version of : style.filesWithStyleViolations Percentage of files with style violations Depends-on : style.filesWithStyleViolations Returns : int Back to top style.spreadOfUnderstandabilityIssues.historic Short name : spreadOfUnderstandabilityIssues.historic Friendly name : Historic spreadOfUnderstandabilityIssues Historic version of : style.spreadOfUnderstandabilityIssues Between 0 and 1 how evenly spread are the understandability issues. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.spreadOfUnderstandabilityIssues Returns : real Back to top style.spreadOfInefficiencies.historic Short name : spreadOfInefficiencies.historic Friendly name : Historic spreadOfInefficiencies Historic version of : style.spreadOfInefficiencies Between 0 and 1 how evenly spread are the style violations which indicate inefficiencies. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.spreadOfInefficiencies Returns : real Back to top style.spreadOfErrorProneness.historic Short name : spreadOfErrorProneness.historic Friendly name : Historic spreadOfErrorProneness Historic version of : style.spreadOfErrorProneness Between 0 and 1 how evenly spread are the style violations which indicate error proneness. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.spreadOfErrorProneness Returns : real Back to top rascal.testability.java.TestOverPublicMethods.historic Short name : percentageOfTestedPublicMethods.historic Friendly name : Historic percentageOfTestedPublicMethods Historic version of : rascal.testability.java.TestOverPublicMethods Number of JUnit tests averaged over the total number of public methods. Ideally all public methods are tested. With this number we compute how far from the ideal situation the project is. Depends-on : rascal.testability.java.TestOverPublicMethods Returns : real Back to top rascal.testability.java.NumberOfTestMethods.historic Short name : numberOfTestMethods.historic Friendly name : Historic numberOfTestMethods Historic version of : rascal.testability.java.NumberOfTestMethods Number of JUnit test methods Depends-on : rascal.testability.java.NumberOfTestMethods Returns : int Back to top rascal.testability.java.TestCoverage.historic Short name : estimateTestCoverage.historic Friendly name : Historic estimateTestCoverage Historic version of : rascal.testability.java.TestCoverage This is a static over-estimation of test coverage: which code is executed in the system when all JUnit test cases are executed? We approximate this by using the static call graphs and assuming every method which can be called, will be called. This leads to an over-approximation, as compared to a dynamic code coverage analysis, but the static analysis does follow the trend and a low code coverage here is an good indicator for a lack in testing effort for the project. Depends-on : rascal.testability.java.TestCoverage Returns : real Back to top trans.rascal.OO.java.Ca-Java-Quartiles.historic Short name : Ca_Java_Q.historic Friendly name : Historic Ca_Java_Q Historic version of : trans.rascal.OO.java.Ca-Java-Quartiles Afferent coupling quartiles (Java) Depends-on : trans.rascal.OO.java.Ca-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.CF-Java.historic Short name : CF_Java.historic Friendly name : Historic CF_Java Historic version of : trans.rascal.OO.java.CF-Java Coupling factor (Java) Depends-on : trans.rascal.OO.java.CF-Java Returns : real Back to top trans.rascal.OO.java.DAC-Java-Quartiles.historic Short name : DAC_Java_Q.historic Friendly name : Historic DAC_Java_Q Historic version of : trans.rascal.OO.java.DAC-Java-Quartiles Data abstraction coupling quartiles (Java) Depends-on : trans.rascal.OO.java.DAC-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.MPC-Java-Quartiles.historic Short name : MPC_Java_Q.historic Friendly name : Historic MPC_Java_Q Historic version of : trans.rascal.OO.java.MPC-Java-Quartiles Message passing coupling quartiles (Java) Depends-on : trans.rascal.OO.java.MPC-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.PF-Java.historic Short name : PF_Java.historic Friendly name : Historic PF_Java Historic version of : trans.rascal.OO.java.PF-Java Polymorphism factor (Java) Depends-on : trans.rascal.OO.java.PF-Java Returns : real Back to top trans.rascal.OO.java.RFC-Java-Quartiles.historic Short name : RFC_Java_Q.historic Friendly name : Historic RFC_Java_Q Historic version of : trans.rascal.OO.java.RFC-Java-Quartiles Response for class quartiles (Java) Depends-on : trans.rascal.OO.java.RFC-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.I-Java-Quartiles.historic Short name : I_Java_Q.historic Friendly name : Historic I_Java_Q Historic version of : trans.rascal.OO.java.I-Java-Quartiles Instability quartiles (Java) Depends-on : trans.rascal.OO.java.I-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.MIF-Java-Quartiles.historic Short name : MIF_Java_Q.historic Friendly name : Historic MIF_Java_Q Historic version of : trans.rascal.OO.java.MIF-Java-Quartiles Method inheritance factor quartiles (Java) Depends-on : trans.rascal.OO.java.MIF-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.MHF-Java.historic Short name : MHF_Java.historic Friendly name : Historic MHF_Java Historic version of : trans.rascal.OO.java.MHF-Java Method hiding factor (Java) Depends-on : trans.rascal.OO.java.MHF-Java Returns : real Back to top trans.rascal.OO.java.AHF-Java.historic Short name : AHF_Java.historic Friendly name : Historic AHF_Java Historic version of : trans.rascal.OO.java.AHF-Java Attribute hiding factor (Java) Depends-on : trans.rascal.OO.java.AHF-Java Returns : real Back to top trans.rascal.OO.java.LCOM-Java-Quartiles.historic Short name : LCOM_Java_Q.historic Friendly name : Historic LCOM_Java_Q Historic version of : trans.rascal.OO.java.LCOM-Java-Quartiles Lack of cohesion in methods quartiles (Java) Depends-on : trans.rascal.OO.java.LCOM-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.A-Java.historic Short name : A_Java.historic Friendly name : Historic A_Java Historic version of : trans.rascal.OO.java.A-Java Abstractness (Java) Depends-on : trans.rascal.OO.java.A-Java Returns : real Back to top trans.rascal.OO.java.DIT-Java-Quartiles.historic Short name : DIT_Java_Q.historic Friendly name : Historic DIT_Java_Q Historic version of : trans.rascal.OO.java.DIT-Java-Quartiles Depth of inheritance tree quartiles (Java) Depends-on : trans.rascal.OO.java.DIT-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.TCC-Java-Quartiles.historic Short name : TCC_Java_Q.historic Friendly name : Historic TCC_Java_Q Historic version of : trans.rascal.OO.java.TCC-Java-Quartiles Tight class cohesion quartiles (Java) Depends-on : trans.rascal.OO.java.TCC-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.LCOM4-Java-Quartiles.historic Short name : LCOM4_Java_Q.historic Friendly name : Historic LCOM4_Java_Q Historic version of : trans.rascal.OO.java.LCOM4-Java-Quartiles Lack of cohesion in methods 4 quartiles (Java) Depends-on : trans.rascal.OO.java.LCOM4-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.SR-Java.historic Short name : SR_Java.historic Friendly name : Historic SR_Java Historic version of : trans.rascal.OO.java.SR-Java Specialization ratio (Java) Depends-on : trans.rascal.OO.java.SR-Java Returns : real Back to top trans.rascal.OO.java.AIF-Java-Quartiles.historic Short name : AIF_Java_Q.historic Friendly name : Historic AIF_Java_Q Historic version of : trans.rascal.OO.java.AIF-Java-Quartiles Attribute inheritance factor quartiles (Java) Depends-on : trans.rascal.OO.java.AIF-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.NOC-Java-Quartiles.historic Short name : NOC_Java_Q.historic Friendly name : Historic NOC_Java_Q Historic version of : trans.rascal.OO.java.NOC-Java-Quartiles Number of children quartiles (Java) Depends-on : trans.rascal.OO.java.NOC-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.RR-Java.historic Short name : RR_Java.historic Friendly name : Historic RR_Java Historic version of : trans.rascal.OO.java.RR-Java Reuse ratio (Java) Depends-on : trans.rascal.OO.java.RR-Java Returns : real Back to top trans.rascal.OO.java.LCC-Java-Quartiles.historic Short name : LCC_Java_Q.historic Friendly name : Historic LCC_Java_Q Historic version of : trans.rascal.OO.java.LCC-Java-Quartiles Loose class cohesion quartiles (Java) Depends-on : trans.rascal.OO.java.LCC-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.Ce-Java-Quartiles.historic Short name : Ce_Java_Q.historic Friendly name : Historic Ce_Java_Q Historic version of : trans.rascal.OO.java.Ce-Java-Quartiles Efferent coupling quartiles (Java) Depends-on : trans.rascal.OO.java.Ce-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.NOM-Java-Quartiles.historic Short name : NOM_Java_Q.historic Friendly name : Historic NOM_Java_Q Historic version of : trans.rascal.OO.java.NOM-Java-Quartiles Number of methods quartiles (Java) Depends-on : trans.rascal.OO.java.NOM-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.NOA-Java-Quartiles.historic Short name : NOA_Java_Q.historic Friendly name : Historic NOA_Java_Q Historic version of : trans.rascal.OO.java.NOA-Java-Quartiles Number of attributes quartiles (Java) Depends-on : trans.rascal.OO.java.NOA-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.OO.java.CBO-Java-Quartiles.historic Short name : CBO_Java_Q.historic Friendly name : Historic CBO_Java_Q Historic version of : trans.rascal.OO.java.CBO-Java-Quartiles Coupling between objects quartiles (Java) Depends-on : trans.rascal.OO.java.CBO-Java-Quartiles Returns : map[str, real] Back to top trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles.historic Short name : countUsesOfAdvancedLanguageFeaturesQ.historic Friendly name : Historic countUsesOfAdvancedLanguageFeaturesQ Historic version of : trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles Quartiles of counts of advanced Java features (wildcards, union types and anonymous classes). The numbers indicate the thresholds that delimit the first 25%, 50% and 75% of the data as well as the maximum and minumum values. Depends-on : trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles Returns : map[str, real] Back to top trans.rascal.CC.java.CCHistogramJava.historic Short name : CCHistogramJava.historic Friendly name : Historic CCHistogramJava Historic version of : trans.rascal.CC.java.CCHistogramJava Number of Java methods per CC risk factor, counts the number of methods which are in a low, medium or high risk factor. The histogram can be compared between projects to indicate which is probably easier to maintain on a method-by-method basis. Depends-on : trans.rascal.CC.java.CCHistogramJava Returns : map[str, int] Back to top trans.rascal.CC.java.CCOverJavaMethods.historic Short name : giniCCOverMethodsJava.historic Friendly name : Historic giniCCOverMethodsJava Historic version of : trans.rascal.CC.java.CCOverJavaMethods Calculates how cyclomatic complexity is spread over the methods of a system. If high CC is localized, then this may be easily fixed but if many methods have high complexity, then the project may be at risk. This metric is good to compare between projects. Depends-on : trans.rascal.CC.java.CCOverJavaMethods Returns : real Historic Metric Providers for OSGi Dependencies These metrics are related to OSGi dependencies declared in MANIFEST.MF files. Back to top trans.rascal.dependency.osgi.numberOSGiBundleDependencies.historic Short name : numberOSGiBundleDependencies.historic Friendly name : Historic numberOSGiBundleDependencies Historic version of : trans.rascal.dependency.osgi.numberOSGiBundleDependencies Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies). Depends-on : trans.rascal.dependency.osgi.numberOSGiBundleDependencies Returns : int Historic Metric Providers for Maven Dependencies These metrics are related to Maven dependencies declared in pom.xml files. Back to top trans.rascal.dependency.maven.numberMavenDependencies.historic Short name : numberMavenDependencies.historic Friendly name : Historic numberMavenDependencies Historic version of : trans.rascal.dependency.maven.numberMavenDependencies Retrieves the number of Maven dependencies. Depends-on : trans.rascal.dependency.maven.numberMavenDependencies Returns : int Back to top Historic Metric Providers for Docker Dependencies The following Historic Metric Provider is associated with Docker Dependencies Back to top org.eclipse.scava.metricprovider.historic.configuration.docker.dependencies Short name : historic.configuration.docker.dependencies Friendly name : Number of dependencies defined in Dockerfiles per day This metric computes the number of the dependencies that are defined in the Dockerfiles of a project per day. It also computes additional information such as the number of each version of the dependencies (image/package). Depends-on : org.eclipse.scava.metricprovider.trans.configuration.docker.dependencies Returns : DockerDependenciesHistoricMetric which contains: Variable Type numberOfDockerDependencies int numberOfDockerPackageDependencies int numberOfDockerImageDependencies int Visualisation Output Information : DockerDependenciesHistoricMetric : id docker.dependencies id docker.packageDependencies id docker.imageDependencies Back to top Historic Metric Providers for Puppet Dependencies The following Historic Metric Provider is associated with Puppet Dependencies Back to top org.eclipse.scava.metricprovider.historic.configuration.puppet.dependencies Short name : historic.configuration.puppet.dependencies Friendly name : Number of dependencies defined in Puppet manifests per day This metric computes the number of the dependencies that are defined in the Puppet manifests of a project per day. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies Returns : PuppetDependenciesHistoricMetric which contains: Variable Type numberOfPuppetDependencies int Visualisation Output Information : DockerDependenciesHistoricMetric : id puppet.dependencies Back to top Historic Metric Providers for Docker Smells The following Historic Metric Provider is associated with Docker Smells Back to top org.eclipse.scava.metricprovider.historic.configuration.docker.smells Short name : historic.configuration.docker.smells Friendly name : Number of smells detected in Dockerfiles per day This metric computes the number of the smells that are detected in the Dockerfiles of a project per day. It also computes additional information such as the number of each type of the smell. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.docker.smells Returns : DockerSmellsHistoricMetric which contains: Variable Type cumulativeNumberOfDockerSmells int numberOfImproperUpgradeSmells int numberOfUnknownPackageVersionSmells int numberOfUntaggedImageSmells int numberOfImproperSudoSmells int numberOfImproperCopySmells int numberOfImproperFromSmells int numberOfImproperCmdSmells int numberOfMeaninglessSmells int numberOfInvalidPortsSmells int numberOfImproperShellSmells int numberOfImproperEntrypointSmells int numberOfDeprecatedInstructionSmells int Visualisation Output Information : DockerDependenciesHistoricMetric : id docker.smells id docker.smells.improperUpgradeSmells id docker.smells.unknownPackageVersionSmells id docker.smells.untaggedImageSmells id docker.smells.improperSudoSmells id docker.smells.improperCopySmells id docker.smells.improperFromSmells id docker.smells.improperCmdSmells id docker.smells.meaninglessSmells id docker.smells.invalidPortsSmells id docker.smells.improperShellSmells id docker.smells.improperEntrypointSmells id docker.smells.deprecatedInstructionSmells Back to top Historic Metric Providers for Puppet Smells The following Historic Metric Providers are associated with Puppet Smells Back to top org.eclipse.scava.metricprovider.historic.configuration.puppet.designsmells Short name : historic.configuration.puppet.designsmells Friendly name : Number of design smells detected in Puppet manifests per day This metric computes the number of the design smells that are detected in the Puppet manifests of a project per day. It also computes additional information such as the number of each type of the smell. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.puppet.designsmells Returns : PuppetDesignSmellsHistoricMetric which contains: Variable Type cumulativeNumberOfDesignSmells int numberOfMultifacetedSmells int numberOfUnnecessarySmells int numberOfImperativeSmells int numberOfMissAbSmells int numberOfInsufficientSmells int numberOfUnstructuredSmells int numberOfTightSmells int numberOfBrokenSmells int numberOfMissingDepSmells int numberOfHairballSmells int numberOfDeficientSmells int numberOfWeakenSmells int Visualisation Output Information : DockerDependenciesHistoricMetric : id puppet.design.smells id puppet.design.multifacetedSmells id puppet.design.unnecessarySmells id puppet.design.imperativeSmells id puppet.design.missAbSmells id puppet.design.insufficientSmells id puppet.design.unstructuredSmells id puppet.design.tightSmells id puppet.design.brokenSmells id puppet.design.missingDepSmells id puppet.design.hairballSmells id puppet.design.deficientSmells id puppet.design.weakenSmells Back to top org.eclipse.scava.metricprovider.historic.configuration.puppet.implementationsmells Short name : historic.configuration.puppet.implementationsmells Friendly name : Number of implementation smells detected in Puppet manifests per day This metric computes the number of the implementation smells that are detected in the Puppet manifests of a project per day. It also computes additional information such as the number of each type of the smell. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.puppet.implementationsmells Returns : PuppetImplementationSmellsHistoricMetric which contains: Variable Type cumulativeNumberOfImplementationSmells int numberOfMissingDefaultCaseSmells int numberOfInconsistentNamingSmells int numberOfDuplicateEntitySmells int numberOfMisplacedAttributeSmells int numberOfImproperAlignment int numberOfInvalidPropertySmells int numberOfImproperQuoteSmells int numberOfLongStatementsSmells int numberOfUnguardedVariableSmells int numberOfMissingDocSmells int numberOfDeprecatedStatementsSmells int numberOfIncompleteTasksSmells int numberOfComplexExpressionSmells int numberOfMissingElseSmells int Visualisation Output Information : DockerDependenciesHistoricMetric : id puppet.implementation.smells id puppet.implementation.missingDefaultCaseSmells id puppet.implementation.inconsistentNamingSmells id puppet.implementation.duplicateEntitySmells id puppet.implementation.misplacedAttributeSmells id puppet.implementation.improperAlignmentSmells id puppet.implementation.invalidPropertySmells id puppet.implementation.improperQuoteSmells id puppet.implementation.longStatementsSmells id puppet.implementation.unguardedVariableSmells id puppet.implementation.missingDocSmells id puppet.implementation.deprecatedStatementsSmells id puppet.implementation.incompleteTasksSmells id puppet.implementation.complexExpressionSmells id puppet.implementation.missingElseSmells Back to top Transient Metric Providers Transient metrics are used to calculate heuristics that are associated with a particular period in time, i.e. a single day. Transient Metrics are stored temporarily within the knowledge base and their output is passed as parameters in the calculation of other transient and historic metrics. Depending on the complexity, a transient metric can depend on the output from other tools, other transient metircs or have no dependencies at all. Back to top Transient Metric Providers for Bug Trackers The following Transient Metric Providers are associated with Issue trackers. Back to top org.eclipse.scava.metricprovider.trans.bugs.activeusers Short name : trans.bugs.activeusers Friendly name : Number of users with new bug comment in the last 15 days This metric computes the number of users that submitted new bug comments in the last 15 days, for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsActiveUsersTransMetric which contains: Variable Type bugs List<BugsData> users List<User> Additional Information : BugData : String bugTrackerId int activeUsers int inactiveUsers int previousUsers int users int days User : String bugTrackerId String userId String lastActivityDate int comments int requests int replies Back to top org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Short name : trans.bugs.bugmetadata Friendly name : Bug header metadata This metric computes various metadata in bug header, i.e. priority, status, operation system and resolution. Other values computed by this metric includes average sentiment, content class and requests/replies. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification , org.eclipse.scava.metricprovider.trans.sentimentclassification , org.eclipse.scava.metricprovider.trans.detectingcode Returns : BugsBugMetadataTransMetric which contains: Variable Type BugData List<BugData> CommentData List<CommentData> Additional Information : BugData : String bugTrackerId String bugId String status List<String> resolution String operatingSystem String priority String creationTime String lastClosedTime String startSentiment String endSentiment float averageSentiment int commentSum int sentimentSum String firstCommentId String lastCommentId CommentData : String bugTrackerId String bugId String commentId String creationTime String creator String contentClass String requestReplyPrediction Back to top org.eclipse.scava.metricprovider.trans.bugs.comments Short name : trans.bugs.comments Friendly name : Number of bug comments This metric computes the number of bug comments, per bug tracker. Depends-on : None Returns : BugsCommentsTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> Additional Information : BugTrackerData: String bugTrackerId int numberOfComments int cumulativeNumberOfComments Back to top org.eclipse.scava.metricprovider.trans.bugs.contentclasses Short name : trans.bugs.contentclasses Friendly name : Content classes in bug comments This metric computes the frequency and percentage of content Classes in bug comments, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsContentClassesTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> contentClasses List<ContentClass> Additional Information : BugTrackerData : String bugTrackerId int numberOfComments ContentClass : String bugTrackerId String classLabel int numberOfComments float percentage Back to top org.eclipse.scava.metricprovider.trans.bugs.dailyrequestsreplies Short name : trans.bugs.dailyrequestsreplies Friendly name : Number of bug comments, requests and replies per day This metric computes the number of bug comments, including those regarded as requests and replies each day, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsDailyRequestsRepliesTransMetric which contains: Variable Type dayComments List<DayComments> Additional Information : DayComments : String name String bugTrackerId int numberOfComments int numberOfRequests int numberOfReplies float percentageOfComments float percentageOfRequests float percentageOfReplies Back to top org.eclipse.scava.metricprovider.trans.bugs.emotions Short name : trans.bugs.emotions Friendly name : Emotions in bug comments This metric computes the emotional dimensions in bug comments, per bug tracker. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.emotionclassification Returns : BugsEmotionsTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> dimensions List<EmotionDimension> Additional Information : BugTrackerData : String bugTrackerId int numberOfComments int cumulativeNumberOfComments EmotionDimension : String bugTrackerId String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfComments int cumulativeNumberOfComments float percentage float cumulativePercentage Back to top org.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies Short name : trans.bugs.hourlyrequestsreplies Friendly name : Number of bug comments, requests and replies per hour This metric computes the number of bug comments, including those regarded as requests and replies, every hour of the day, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsHourlyRequestsRepliesTransMetric which contains: Variable Type hourComments List<HourComments> Additional Information : HourComments : String bugTrackerId String hour int numberOfComments int numberOfRequests int numberOfReplies float percentageOfComments float percentageOfRequests float percentageOfReplies Back to top org.eclipse.scava.metricprovider.trans.bugs.migrationissues Short name : trans.bugs.migrationissues Friendly name : Migration Issues Detection in Bug Trackers This metric detects migration issues in Bug Tracking Systems. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.topics Returns : BugTrackerMigrationIssueTransMetric which contains: Variable Type bugTrackerMigrationIssues List<BugTrackerMigrationIssue> Additional Information : BugTrackerMigrationIssue : String bugTrackerId String bugId String summary Back to top org.eclipse.scava.metricprovider.trans.bugs.migrationissuesmaracas Short name : trans.bugs.migrationissuesmaracas Friendly name : Migration Issues Detection Maracas in Bug Trackers This metric detects migration issues in Bug Tracking Systems along with data from Maracas. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.bugs.migrationissues , org.eclipse.scava.metricprovider.trans.migrationissuesmaracas , org.eclipse.scava.metricprovider.trans.plaintextprocessing Returns : BugTrackerMigrationIssueMaracasTransMetric which contains: Variable Type bugTrackerMigrationIssuesMaracas List<BugTrackerMigrationIssueMaracas> Additional Information : BugTrackerMigrationIssueMaracas : String bugTrackerId String bugId List<String> changes List<Double> matchingPercentage Back to top org.eclipse.scava.metricprovider.trans.bugs.newbugs Short name : trans.bugs.newbugs Friendly name : Number of new bugs This metric computes the number of new bugs over time, per bug tracker. Depends-on : None Returns : BugsNewBugsTransMetric which contains: Variable Type bugTrackerData type Additional Information : BugTrackerData : String bugTrackerId int numberOfBugs int cumulativeNumberOfBugs Back to top org.eclipse.scava.metricprovider.trans.bugs.patches Short name : trans.bugs.patches Friendly name : Number of patches per bug This metric computes the number of patches submitted by the community (users) for each bug. Depends-on : None Returns : BugsPatchesTransMetric which contains: Variable Type bugTrackerData type Additional Information : BugTrackerData : String bugTrackerId int numberOfPatches int cumulativeNumberOfPatches Back to top org.eclipse.scava.metricprovider.trans.bugs.references Short name : trans.bugs.references Friendly name : Bugs References This metrics search for references of commits or bugs within comments comming from bugs comments. Depends-on : None Returns : BugsReferenceTransMetric which contains: Variable Type bugs List<BugReferringTo> Additional Information : BugReferringTo : String bugTrackerId String bugId String commentId List<String> bugsReferred (URLs) List<String> commitsReferred (URLs) Note : When this metric is used on GitHub, it should be noted that some references of bugs will be in fact pull requests. The reason is that GitHub considers pull requests equally as issues. Back to top org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Short name : trans.bugs.requestreplies Friendly name : Bug statistics (answered?, response time) This metric computes for each bug, whether it was answered. If so, it computes the time taken to respond. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsRequestsRepliesTransMetric which contains: Variable Type bugs List<BugStatistics> Additional Information : BugStatistics : String bugTrackerId String bugId boolean answered long responseDurationSec String responseDate Back to top Transient Metric Providers for Newsgroups and Forums The following Transient Metric Providers are associated with communication channels in general, either newsgroups or forums. Despite the name of the metrics are newsgroups, all the metrics are valid for communication channels. Back to top org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Short name : trans.newsgroups.activeusers Friendly name : Number of users with new comment in the last 15 days This metric computes the number of users that submitted news comments in the last 15 days, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsActiveUsersTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> user List<User> Additional Information : NewsgroupData : String newsgroupName int activeUsers int inactiveUsers int previousUsers int users int days User : String newsgroupName String userId String lastActiveDate int articles int requests int replies Back to top org.eclipse.scava.metricprovider.trans.newsgroups.articles Short name : trans.newsgroups.articles Friendly name : Number of articles per newsgroup This metric computes the number of articles, per newsgroup. Depends-on : None Returns : NewsgroupsArticlesTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> Additional Information : NewsgroupData : String newsgroupName int numberOfArticles int cumulativeNumberOfArticles Back to top org.eclipse.scava.metricprovider.trans.newsgroups.contentclasses Short name : trans.newsgroups.contentclasses Friendly name : Content classes in newsgroup articles This metric computes the content classes in newgroup articles, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : NewsgroupsContentClassesTransMetric which contains: Variable Type newsGroupData List<NewsgroupData> contentClass List< ContentClass> Additional Information : NewsGroupData: String newsgroupName int numberOfArticles ContentClass: String newsgroupName String classLabel int numberOfArticles float percentage Back to top org.eclipse.scava.metricprovider.trans.newsgroups.dailyrequestsreplies Short name : trans.newsgroups.dailyrequestsreplies Friendly name : Number of articles, requests and replies per day This metric computes the number of articles, including those regarded as requests and replies for each day of the week, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsDailyRequestsRepliesTransMetric which contains: Variable Type dailyArticles List<DailyArticles> Additional Information : DailyArticles : String name int numberOfArticles int numberOfRequests int numberOfReplies float percentageOfArticles float percentageOfRequests float percentageOfReplies Back to top org.eclipse.scava.metricprovider.trans.newsgroups.emotions Short name : trans.newsgroups.emotions Friendly name : Emotions in newsgroup articles This metric computes the emotional dimensions in newsgroup articles, per newsgroup. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.emotionclassification Returns : NewsgroupsEmotionsTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> emotionDimension List<EmotionDimension> Additional Information : NewsgroupData : String newsgroupName int numberOfArticles int cumulativeNumberOfArticles EmotionDimension : String newsgroupName String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfArticles int cumulativeNumberOfArticles float percentage float cumulativePercentage Back to top org.eclipse.scava.metricprovider.trans.newsgroups.hourlyrequestsreplies Short name : trans.newsgroups.hourlyrequestsreplies Friendly name : Number of articles, requests and replies per hour This metric computes the number of articles, including those regarded as requests and replies for each hour of the day, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsHourlyRequestsRepliesTransMetric which contains: Variable Type hourArticles List<HourArticles> Additional Information : HourArticles : String hour int numberOfArticles int numberOfRequests int numberOfReplies float percentageOfArticles float percentageOfRequests float percentageOfReplies Back to top org.eclipse.scava.metricprovider.trans.newsgroups.migrationissues Short name : trans.newsgroups.migrationissues Friendly name : Migration Issues Detection in Communication Channels This metric detects migration issues in Communication Channels articles. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.topics , org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : NewsgroupsMigrationIssueTransMetric which contains: Variable Type newsgroupsMigrationIssues List<NewsgroupsMigrationIssue> Additional Information : NewsgroupsMigrationIssue : String newsgroupName int threadId long articleId Back to top org.eclipse.scava.metricprovider.trans.newsgroups.migrationissuesmaracas Short name : trans.newsgroups.migrationissuesmaracas Friendly name : Migration Issues Detection Maracas in Newsgroups This metric detects migration issues in Newsgroups along with data from Maracas. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.newsgroups.migrationissues , org.eclipse.scava.metricprovider.trans.migrationissuesmaracas , org.eclipse.scava.metricprovider.trans.plaintextprocessing Returns : BugTrackerMigrationIssueMaracasTransMetric which contains: Variable Type newsgroupsMigrationIssuesMaracas List<NewsgroupMigrationIssueMaracas> Additional Information : NewsgroupMigrationIssueMaracas : String newsgrupName int threadId List<String> changes List<Double> matchingPercentage Back to top org.eclipse.scava.metricprovider.trans.newsgroups.sentiment Short name : trans.newsgroups.sentiment Friendly name : Average sentiment in newsgroup threads The metric computes the average sentiment, including sentiment at the beginning and end of each thread, per newsgroup. Sentiment polarity value could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment) Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.sentimentclassification Returns : NewsgroupsSentimentTransMetric which contains: Variable Type threadStatistics List<ThreadStatistics> Additional Information : ThreadStatistics : String newsgroupName int threadId float averageSentiment String startSentiment String endSentiment Back to top org.eclipse.scava.metricprovider.trans.newsgroups.threads Short name : trans.newsgroups.threads Friendly name : Assigns newsgroup articles to threads This metric holds information for assigning newsgroup articles to threads. The threading algorithm is executed from scratch every time. Depends-on : None , Returns : NewsgroupsThreadsTransMetric which contains: Variable Type articleData List<ArticleData> threadData List<ThreadData> newsgroupData List<NewsgroupData> currentDate List<CurrentDate> Additional Information : ArticleData : String newsgroupName int articleNumber String articlesId String date String from String subject String contentClass String references ThreadData : int threadId NewsgroupData : String newsgroupName int threads int previousThreads CurrentDate : String date Back to top org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Short name : trans.newsgroups.threadsrequestsreplies Friendly name : Thread statistics (answered?, response time) The metric computes for each thread whether it is answered. If so, it computes the response time. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsThreadsRequestsRepliesTransMetric which contains: Variable Type threadStatistics List<ThreadStatistics> Additional Information : ThreadStatistics : String newsgroupName int threadId boolean firstRequest boolean answered long responseDurationSec String responseDate Back to top Transient Metric Providers for Documentation The following Transient Metric Providers are associated with documentation analyses. Back to top org.eclipse.scava.metricprovider.trans.documentation Short name : trans.documentation Friendly name : Documentation processing This metric process the files returned from the documentation readers and extracts the body (in format HTML or text) Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : DocumentationTransMetric which contains: Variable Type documentationEntries List<DocumentationEntry> documentation List<Documentation> Additional Information : DocumentationEntry : String documentationId String entryId String body String originalFormatName String originalFormatMime boolean htmlFormatted Documentation : String documentationId List<String> entriesId List<String> removedEntriesId String lastUpdateDate String lastRevisionAnalyzed String nextUpdateDate Back to top org.eclipse.scava.metricprovider.trans.documentation.classification Short name : trans.documentation.classification Friendly name : Documentation classification This metric determines which type of documentation is present. The possible types are: API , Development , Installation , Started , User . Depends-on : org.eclipse.scava.metricprovider.trans.documentation , org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : DocumentationClassificationTransMetric which contains: Variable Type documentationEntriesClassification List<DocumentationEntryClassification> Additional Information : DocumentationEntryClassification : String documentationId String entryId List<String> types Back to top org.eclipse.scava.metricprovider.trans.documentation.detectingcode Short name : trans.documentation.detectingcode Friendly name : Documentation detection of code This metric process the plain text from documentation and detects the portions corresponding to code and natural language Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.documentation.plaintext Returns : DocumentationDetectingCodeTransMetric which contains: Variable Type documentationEntriesDetectingCode List<DocumentationEntryDetectingCode> Additional Information : DocumentationEntryDetectingCode : String documentationId String entryId String naturalLanguage String code Back to top org.eclipse.scava.metricprovider.trans.documentation.plaintext Short name : trans.documentation.plaintext Friendly name : Documentation plain text processor This metric process the body of each documentation entry and extracts the plain text Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.documentation Returns : DocumentationPlainTextTransMetric which contains: Variable Type documentationEntriesPlainText List<DocumentationEntryPlainText> Additional Information : DocumentationEntryPlainText : String documentationId String entryId List<String> plainText Back to top org.eclipse.scava.metricprovider.trans.documentation.readability Short name : trans.documentation.readability Friendly name : Documentation calculation of readability This metric calculates the readability of each documentation entry. The higher the score, the more difficult to understand the text. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.documentation , org.eclipse.scava.metricprovider.trans.documentation.detectingcode Returns : DocumentationReadabilityTransMetric which contains: Variable Type documentationEntriesReadability List<DocumentationEntryReadability> Additional Information : DocumentationEntryReadability : String documentationId String entryId double readability Back to top org.eclipse.scava.metricprovider.trans.documentation.sentiment Short name : trans.documentation.sentiment Friendly name : Documentation Sentiment Analysis This metric calculates the sentiment polarity of each documentation entry. Sentiment polarity value could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment) Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.documentation , org.eclipse.scava.metricprovider.trans.documentation.detectingcode Returns : DocumentationSentimentTransMetric which contains: Variable Type documentationEntriesSentiment List<DocumentationEntrySentiment> Additional Information : DocumentationEntrySentiment : String documentationId String entryId String polarity Back to top Transient Metric Providers for Natural Language Processing The following Transient Metric Providers are associated with Natural Language Processing tools. Back to top org.eclipse.scava.metricprovider.trans.detectingcode Short name : trans.detectingcode Friendly name : Distinguishes between code and natural language This metric determines the parts of a bug comment or a newsgroup article that contains code or natural language. Depends-on : org.eclipse.scava.metricprovider.trans.plaintextprocessing Returns : DetectingCodeTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerCommentDetectingCode> newsgroupArticles List<NewsgroupArticleDetectingCode> forumPosts List<ForumPostsDetectingCode> Additional Information : BugTrackerCommentDetectingCode : String bugTrackerId String bugId String commentId String naturalLanguage String code NewsgroupArticleDetectingCode : String newsgroupName String articleNumber String naturalLanguage String code ForumPostsDetectingCode : String forumId String topicId String postId String naturalLanguage String code Back to top org.eclipse.scava.metricprovider.trans.emotionclassification Short name : trans.emotionclassification Friendly name : Emotion classifier This metric computes the emotions present in each bug comment, newsgroup article or forum post. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : EmotionClassificationTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerCommentEmotionClassification> newsgroupArticles List<NewsgroupArticleEmotionClassification> forumPosts List<ForumPostsEmotionClassification> Additional Information : BugTrackerCommentEmotionClassification : String bugTrackerId String bugId String commentId String emotions NewsgroupArticleEmotionClassification : String newsgroupName String articleNumber String emotions ForumPostsEmotionClassification : String forumId String topicId String postId String emotions Back to top org.eclipse.scava.metricprovider.trans.plaintextprocessing Short name : trans.plaintextprocessing Friendly name : Plain text processing This metric preprocess each bug comment, newsgroup article or forum post into a split plain text format. Depends-on : None Returns : PlainTextProcessingTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerCommentPlainTextProcessing> newsgroupArticles List<NewsgroupArticlePlainTextProcessing> forumPosts List<ForumPostsPlainTextProcessing> Additional Information : BugTrackerCommentPlainTextProcessing : String bugTrackerId String bugId String commentId String plainText boolean hadReplies NewsgroupArticlePlainTextProcessing : String newsgroupName String articleNumber String plainText boolean hadReplies ForumPostsPlainTextProcessing : String forumId String topicId String postId String plainText boolean hadReplies Back to top org.eclipse.scava.metricprovider.trans.requestreplyclassification Short name : trans.requestreplyclassification Friendly name : Request/Reply classification This metric computes if a bug comment, newsgroup article or forum post is a request of a reply. Depends-on : org.eclipse.scava.metricprovider.trans.plaintextprocessing , org.eclipse.scava.metricprovider.trans.detectingcode Returns : RequestReplyClassificationTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerComments> newsgroupArticles List<NewsgroupArticles> forumPosts List<ForumPosts> Additional Information : BugTrackerComments : String bugTrackerId String bugId String commentId String classificationResult String date NewsgroupArticles : String newsgroupName String articleNumber String classificationResult String date ForumPosts : String forumId String topicId String postId String classificationResult String date Back to top org.eclipse.scava.metricprovider.trans.sentimentclassification Short name : trans.sentimentclassification Friendly name : Sentiment classification This metric computes the sentiment of each bug comment, newsgroup article or forum post. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or 1 (positive sentiment). Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : SentimentClassificationTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerCommentsSentimentClassification> newsgroupArticles List<NewsgroupArticlesSentimentClassification> forumPosts List<ForumPostSentimentClassification> Additional Information : BugTrackerCommentsSentimentClassification : String bugTrackerId String bugId String commentId String polarity ( negative (-1) , neutral (0) or positive (1) ) NewsgroupArticlesSentimentClassification : String newsgroupName String articleNumber String polarity ( negative (-1) , neutral (0) or positive (1) ) ForumPostSentimentClassification : String forumId String topicId String postId String polarity ( negative (-1) , neutral (0) or positive (1) ) Back to top org.eclipse.scava.metricprovider.trans.severityclassification Short name : trans.severityclassification Friendly name : Severity classification This metric computes the severity of each bug comment, newsgroup article or forum post. Severity could be blocker, critical, major, minor, enhancement, normal). For bug comments, there is an additional severity level called unknown . A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode , org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : SeverityClassificationTransMetric which contains: Variable Type bugTrackerBugs List<BugTrackerBugsData> newsgroupArticles List<NewsgroupArticleData> newsgroupThreads List<NewsgroupThreadData> forumPosts ForumPostData> Additional Information : BugTrackerBugsData : String bugTrackerId String bugId String severity int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams NewsgroupArticleData : String NewsgroupName long articleNumber int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams NewsgroupThreadData : String newsgroupName int threadId String severity BugTrackerBugsData : String forumId String topicId String severity int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams Back to top org.eclipse.scava.metricprovider.trans.topics Short name : trans.topics Friendly name : Topic clustering This metric computes topic clusters for each bug comment, newsgroup article or forum post in the last 30 days. Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : TopicsTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerCommentsData> bugTrackerTopics List<BugTrackerTopic> newsgroupArticles List<NewsgroupArticlesData> newsgroupTopics List<NewsgroupTopic> Additional Information : BugTrackerCommentsData : String bugTrackerId String bugId String commentId String subject String text String date NewsgroupArticlesData : String newsgroupName long articleNumber String subject String text String date BugTrackerTopic : String bugTrackerId List<String> labels int numberOfDocuments List<String> commentsId NewsgroupTopic : String newsgroupName List<String> labels int numberOfDocuments List<Long> articlesId Transient Metric Providers for Commits and Committers These metrics are related to the commits and committers of a project. Back to top org.eclipse.scava.metricprovider.trans.commits.message.plaintext Short name : trans.commits.message.plaintext Friendly name : Commits message plain text This metric preprocess each commit message to get a split plain text version. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : CommitsMessagePlainTextTransMetric which contains: Variable Type commitsMessagesPlainText List<CommitMessagePlainText> Additional Information : CommitMessagePlainText : String repository (URL) String revision (Commit SHA) List<String> plainText Back to top org.eclipse.scava.metricprovider.trans.commits.messagereferences Short name : trans.commits.messagereferences Friendly name : Commits Messages References This metrics search for references of commits or bugs within the messages of commits. In order to detect bugs references, it is necessary to use at the same time one Bug Tracker, as the retrieval of references are based on patterns defined by bug trackers. If multiple or zero Bug Trackers are defined in the project, the metric will only search for commits (alphanumeric strings of 40 characters). Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : CommitsMessageReferenceTransMetric which contains: Variable Type commitsMessagesReferringTo List<CommitMessageReferringTo> Additional Information : BugReferringTo : String repository (URL) String revision (Commit SHA) List<String> bugsReferred (URLs) List<String> commitsReferred (URLs) Note : When this metric is used on GitHub, it should be noted that some references of bugs will be in fact pull requests. The reason is that GitHub considers pull requests equally as issues. Back to top org.eclipse.scava.metricprovider.trans.commits.message.topics Short name : trans.commits.message.topics Friendly name : Commits Messages Topic Clustering This metric computes topic clusters for each commit message. Depends-on : org.eclipse.scava.metricprovider.trans.commits.message.plaintext Returns : CommitsMessageTopicsTransMetric which contains: Variable Type commitsMessages List<CommitMessage> commitsTopics List<CommitsTopic> Additional Information : CommitMessage : String repository (URL) String revision (Commit SHA) String subject String message Date date CommitsTopic : String repository (URL) List<String> labels int numberOfMessages List<String> commitsMessageId Back to top trans.rascal.activecommitters.activeCommitters Short name : activeCommitters Friendly name : Committers of last two weeks A list of committers who have been active the last two weeks. This metric is meant for downstream processing. Depends-on : trans.rascal.activecommitters.committersToday Returns : rel[datetime,set[str]] Back to top trans.rascal.activecommitters.committersoverfile Short name : giniCommittersOverFile Friendly name : Committers over file Calculates the gini coefficient of committers per file Depends-on : trans.rascal.activecommitters.countCommittersPerFile Returns : real Back to top trans.rascal.activecommitters.countCommittersPerFile Short name : countCommittersPerFile Friendly name : Number of committers per file Count the number of committers that have touched a file. Depends-on : trans.rascal.activecommitters.committersPerFile Returns : map[loc file, int numberOfCommitters] Back to top trans.rascal.activecommitters.firstLastCommitDatesPerDeveloper Short name : firstLastCommitDates Friendly name : First and last commit dates per developer Collects per developer the first and last dates on which he or she contributed code. This basic metric is used downstream for other metrics, but it is also used to drill down on the membership of specific individuals of the development team. Depends-on : trans.rascal.activecommitters.committersToday Returns : map[str, tuple[datetime,datetime]] Back to top trans.rascal.activecommitters.developmentTeam Short name : developmentTeam Friendly name : Development team Lists the names of people who have been contributing code at least once in the history of the project. Depends-on : trans.rascal.activecommitters.committersToday Returns : set[str] Back to top trans.rascal.activecommitters.percentageOfWeekendCommits Short name : percentageOfWeekendCommits Friendly name : Percentage of weekend commits Percentage of commits made during the weekend Depends-on : trans.rascal.activecommitters.commitsPerWeekDay Returns : int Back to top trans.rascal.activecommitters.maximumActiveCommittersEver Short name : maximumActiveCommittersEver Friendly name : Maximum active committers ever What is the maximum number of committers who have been active together in any two week period? Depends-on : trans.rascal.activecommitters.numberOfActiveCommitters.historic Returns : int Back to top trans.rascal.activecommitters.developmentTeamEmails Short name : developmentTeamEmails Friendly name : Development team Lists the names of people who have been contributing code at least once in the history of the project. Depends-on : trans.rascal.activecommitters.committersEmailsToday Returns : set[str] Back to top trans.rascal.activecommitters.developmentDomainNames Short name : developmentDomainNames Friendly name : Development team domain names Lists the domain names of email addresses of developers if such information is present. Depends-on : trans.rascal.activecommitters.developmentTeamEmails Returns : set[str] Back to top trans.rascal.activecommitters.committersPerFile Short name : committersPerFile Friendly name : Committers per file Register which committers have contributed to which files Depends-on : - Returns : rel[loc,str] Back to top trans.rascal.activecommitters.longerTermActiveCommitters Short name : longerTermActiveCommitters Friendly name : Committers of last year Committers who have been active the last 12 months. This metric is meant for downstream processing. Depends-on : trans.rascal.activecommitters.committersToday Returns : rel[datetime,set[str]] Back to top trans.rascal.activecommitters.commitsPerDeveloper Short name : commitsPerDeveloper Friendly name : Number of commits per developer The number of commits per developer indicates not only the volume of the contribution of an individual but also the style in which he or she commits, when combined with other metrics such as churn. Few and big commits are different from many small commits. This metric is used downstream by other metrics as well. Depends-on : - Returns : map[str, int] Back to top trans.rascal.activecommitters.committersAge Short name : ageOfCommitters Friendly name : Developer experience in project Measures in days the amount of time between the first and last contribution of each developer. Depends-on : trans.rascal.activecommitters.firstLastCommitDatesPerDeveloper Returns : rel[str,int] Back to top trans.rascal.activecommitters.committersToday Short name : committersToday Friendly name : Active committers Who have been active today? Depends-on : - Returns : set[str] Back to top trans.rascal.activecommitters.projectAge Short name : projectAge Friendly name : Age of the project (nr of days between first and last commit) Age of the project (nr of days between first and last commit) Depends-on : trans.rascal.activecommitters.firstLastCommitDatesPerDeveloper Returns : int Back to top trans.rascal.activecommitters.commitsPerWeekDay Short name : commitsPerWeekDay Friendly name : Commits per week day On which day of the week do commits take place? Depends-on : - Returns : map[str, int] Back to top trans.rascal.activecommitters.committersEmailsToday Short name : committersEmailsToday Friendly name : Active committers Who have been active today? Depends-on : - Returns : set[str] Back to top trans.rascal.activecommitters.sizeOfDevelopmentTeam Short name : sizeOfDevelopmentTeam Friendly name : Size of development team How many people have ever contributed code to this project? Depends-on : trans.rascal.activecommitters.developmentTeam Returns : int Back to top trans.rascal.activecommitters.numberOfActiveCommittersLongTerm Short name : numberOfActiveCommittersLongTerm Friendly name : Number of active committers long term Number of long time active committers over time (active in last year). This measures a smooth window of one year, where every day we report the number of developers active in the previous 365 days. Depends-on : trans.rascal.activecommitters.longerTermActiveCommitters Returns : int Back to top trans.rascal.activecommitters.numberOfActiveCommitters Short name : numberOfActiveCommitters Friendly name : Number of active committers Number of active committers over time (active in last two weeks). This measures a smooth window of two weeks, where every day we report the number of developers in the previous 14 days. Depends-on : trans.rascal.activecommitters.activeCommitters Returns : int Back to top rascal.generic.churn.commitsToday Short name : commitsToday Friendly name : Number of commits today Counts the number of commits made today. Depends-on : - Returns : int Back to top rascal.generic.churn.churnToday Short name : commitsToday Friendly name : Churn of today Counts the churn for today: the total number of lines of code added and deleted. This metric is used further downstream to analyze trends. Depends-on : - Returns : int Back to top rascal.generic.churn.churnPerCommitInTwoWeeks Short name : churnPerCommitInTwoWeeks Friendly name : Churn per commit in two weeks The ratio between the churn and the number of commits indicates how large each commit is on average. We compute this as a sliding average over two weeks which smoothens exceptions and makes it possible to see a trend historically. Commits should not be to big all the time, because that would indicate either that programmers are not focusing on well-defined tasks or that the system architecture does not allow for separation of concerns. Depends-on : rascal.generic.churn.churnInTwoWeeks rascal.generic.churn.commitsInTwoWeeks Returns : int Back to top rascal.generic.churn.churnActivity Short name : churnActivity Friendly name : Churn over the last two weeks Churn in the last two weeks: collects the lines of code added and deleted over a 14-day sliding window. Depends-on : rascal.generic.churn.churnToday Returns : rel[datetime,int] Back to top rascal.generic.churn.commitActivity Short name : commitActivity Friendly name : Commits in last two weeks Number of commits in the last two weeks: collects commit activity over a 14-day sliding window. Depends-on : rascal.generic.churn.commitsToday Returns : rel[datetime,int] Back to top rascal.generic.churn.coreCommittersChurn Short name : coreCommittersChurn Friendly name : Churn per core committer Find out about the committers what their total number of added and deleted lines for this system. Depends-on : rascal.generic.churn.churnPerCommitter Returns : map[str, int] Back to top rascal.generic.churn.filesPerCommit Short name : numberOfFilesPerCommit Friendly name : Number of files per commit Counts the number of files per commit to find out about the separation of concerns in the architecture or in the tasks the programmers perform. This metric is used further downstream. Depends-on : - Returns : map[loc, int] Back to top rascal.generic.churn.churnPerCommit Short name : churnPerCommit Friendly name : Counts number of lines added and deleted per commit. Count churn. Churn is the number lines added or deleted. We measure this per commit because the commit is a basic unit of work for a programmer. This metric computes a table per commit for today and is not used for comparison between projects. It is used further downstream to analyze activity. Depends-on : - Returns : map[loc, int] Back to top rascal.generic.churn.churnPerCommitter Short name : churnPerCommitter Friendly name : Churn per committer Count churn per committer: the number of lines of code added and deleted. It zooms in on the single committer producing a table which can be used for downstream processing. Depends-on : - Returns : map[str author, int churn] Back to top rascal.generic.churn.churnPerFile Short name : churnPerFile Friendly name : Churn per file Churn per file counts the number of files added and deleted for a single file. This is a basic metric to indicate hotspots in the design of the system which is changed often. This metric is used further downstream. Depends-on : - Returns : map[loc file, int churn] Back to top rascal.generic.churn.commitsInTwoWeeks Short name : commitsInTwoWeeks Friendly name : Number of commits in the last two weeks Churn in the last two weeks: aggregates the number of commits over a 14-day sliding window. Depends-on : rascal.generic.churn.commitActivity Returns : int Back to top rascal.generic.churn.churnInTwoWeeks Short name : churnInTwoWeeks Friendly name : Sum of churn in the last two weeks Churn in the last two weeks: aggregates the lines of code added and deleted over a 14-day sliding window. Depends-on : rascal.generic.churn.churnActivity Returns : int Back to top Transient Metric Providers for Generic Source Code These metrics are related to the source code of analyzed projects, regardless of the language(s) they are written in. Back to top trans.rascal.readability.fileReadability Short name : fileReadability Friendly name : File readability Code readability per file, measured by use of whitespace measures deviations from common usage of whitespace in source code, such as spaces after commas. This is a basic collection metric which is used further downstream. Depends-on : - Returns : map[loc, real] Back to top trans.rascal.readability.fileReadabilityQuartiles Short name : fileReadabilityQ Friendly name : File readability quartiles We measure file readability by counting exceptions to common usage of whitespace in source code, such as spaces after commas. The quartiles represent how many of the files have how many of these deviations. A few deviations per file is ok, but many files with many deviations indicates a lack of attention to readability. Depends-on : trans.rascal.readability.fileReadability Returns : map[str, real] Back to top trans.rascal.comments.headerCounts Short name : headerCounts Friendly name : Number of appearances of estimated unique headers In principle it is expected for the files in a project to share the same license. The license text in the header of each file may differ slightly due to different copyright years and or lists of contributors. The heuristic allows for slight differences. The metric produces the number of different types of header files found. A high number is a contra-indicator, meaning either a confusing licensing scheme or the source code of many different projects is included in the code base of the analyzed system. Depends-on : - Returns : list[int] Back to top trans.rascal.comments.commentedOutCode Short name : commentedOutCode Friendly name : Lines of commented out code per file Lines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how much source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator. Depends-on : - Returns : map[loc, int] Back to top trans.rascal.comments.commentLOC Short name : commentLOC Friendly name : Number of lines containing comments per file Number of lines containing comments per file is a basic metric used for downstream processing. This metric does not consider the difference between natural language comments and commented out code. Depends-on : - Returns : map[loc, int] Back to top trans.rascal.comments.commentLinesPerLanguage Short name : commentLinesPerLanguage Friendly name : Number of lines containing comments per language (excluding headers) Number of lines containing comments per language (excluding headers). The balance between comments and code indicates understandability. Too many comments are often not maintained and may lead to confusion, not enough means the code lacks documentation explaining its intent. This is a basic fact collection metric which is used further downstream. Depends-on : trans.rascal.comments.commentLOC trans.rascal.comments.headerLOC trans.rascal.comments.commentedOutCode Returns : map[str, int] Back to top trans.rascal.comments.commentedOutCodePerLanguage Short name : commentedOutCodePerLanguage Friendly name : Lines of commented out code per language Lines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how much source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator. Depends-on : trans.rascal.comments.commentedOutCode Returns : map[str, int] Back to top trans.rascal.comments.headerLOC Short name : headerLOC Friendly name : Header size per file Header size per file is a basic metric counting the size of the comment at the start of each file. It is used for further processing downstream. Depends-on : - Returns : map[loc, int] Back to top trans.rascal.comments.matchingLicenses Short name : matchingLicenses Friendly name : Used licenses (from selected list of known licenses) We match against a list of known licenses to find out which are used in the current project Depends-on : - Returns : set[str] Back to top trans.rascal.comments.headerPercentage Short name : headerPercentage Friendly name : Percentage of files with headers. Percentage of files with headers is an indicator for the amount of files which have been tagged with a copyright statement (or not). If the number is low this indicates a problem with the copyright of the program. Source files without a copyright statement are not open-source, they are owned, in principle, by the author and may not be copied without permission. Note that the existence of a header does not guarantee the presence of an open-source license, but its absence certainly is telling. Depends-on : trans.rascal.comments.headerLOC Returns : real Back to top trans.rascal.LOC.genericLOC Short name : countLoc Friendly name : Language independent physical lines of code Physical lines of code simply counts the number of newline characters (OS independent) in a source code file. The metric can be used to compare the volume between two systems. Depends-on : - Returns : map[loc, int] Back to top trans.rascal.LOC.genericLOCoverFiles Short name : giniLOCOverFiles Friendly name : Spread of code over files We find out how evenly the code is spread over files. The number should be quite stable over time. A jump in this metric indicates a large change in the code base. If the code is focused in only a few very large files then this may be a contra-indicator for quality. Depends-on : - Returns : real Back to top trans.rascal.LOC.locPerLanguage Short name : locPerLanguage Friendly name : Physical lines of code per language Physical lines of code simply counts the number of newline characters (OS independent) in a source code file. We accumulate this number per programming language. The metric can be used to compare the volume between two systems and to assess in which programming language the bulk of the code is written. Depends-on : trans.rascal.LOC.genericLOC Returns : map[str, int] Back to top trans.rascal.clones.cloneLOCPerLanguage Short name : cloneLOCPerLanguage Friendly name : Lines of code in Type I clones larger than 6 lines, per language Lines of code in Type I clones larger than 6 lines, per language. A Type I clone is a literal clone. A large number of literal clones is considered to be bad. This metric is not easily compared between systems because it is not size normalized yet. We use it for further processing downstream. You can analyze the trend over time using this metric. Depends-on : - Returns : map[str, int] Transient Metric Providers for Java Code These metrics are related to the Java source code of analyzed projects. Back to top style.filesWithErrorProneness Short name : filesWithErrorProneness Friendly name : Files with style violations which make the code error prone. This is basic metric which can not be easily compared between projects. Percentage of files with error proneness Depends-on : style.errorProneness Returns : int Back to top style.understandability Short name : understandability Friendly name : Inefficient code Percentage of the projects files with coding style violations which indicate the code may be hard to read and understand, but not necessarily more error prone. Depends-on : style.styleViolations Returns : Table Back to top style.inefficiencies Short name : inefficiencies Friendly name : Inefficient code Percentage of the projects files with coding style violations which indicate common inefficient ways of doing things in Java. Depends-on : style.styleViolations Returns : Table Back to top style.filesWithUnderstandabilityIssues Short name : filesWithUnderstandabilityIssues Friendly name : Files with style violations which make the code harder to understand Percentage of files with understandability issues. This is a basic metric which can not be easily compared between projects. Depends-on : style.understandability Returns : int Back to top style.errorProneness Short name : errorProneness Friendly name : Error proneness Percentage of the projects files with coding style violations which indicate error prone code. This is a basic metric which collects per file all the style violations, recording the line number and the kind of style violation. Each kind of violation is grouped into a category. The resulting table is hard to interpret manually and can not be compared between projects. Other metrics further downstream do aggregate this information. Depends-on : style.styleViolations Returns : Table Back to top style.spreadOfStyleViolations Short name : spreadOfStyleViolations Friendly name : Spread of style violations over files Between 0 and 1 how evenly spread are the style violations. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.styleViolations Returns : real Back to top style.filesWithInefficiencies Short name : filesWithInefficiencies Friendly name : Files with style violations which indicate inefficiencies. This is a basic metric which can not be easily compared between projects. Percentage of files with inefficiencies Depends-on : style.inefficiencies Returns : int Back to top style.filesWithStyleViolations Short name : filesWithStyleViolations Friendly name : Counts the number of files with any kind of style violation. This metric can not be easily compared between projects. Percentage of files with style violations Depends-on : style.styleViolations Returns : int Back to top style.spreadOfUnderstandabilityIssues Short name : spreadOfUnderstandabilityIssues Friendly name : Spread of understandability issues over files Between 0 and 1 how evenly spread are the understandability issues. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.understandability Returns : real Back to top style.spreadOfInefficiencies Short name : spreadOfInefficiencies Friendly name : Spread of inefficiencies over files Between 0 and 1 how evenly spread are the style violations which indicate inefficiencies. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.inefficiencies Returns : real Back to top style.styleViolations Short name : styleViolations Friendly name : All style violations This is a basic metric which collects per file all the style violations, recording the line number and the kind of style violation. Each kind of violation is grouped into a category. The resulting table is hard to interpret manually and can not be compared between projects. Other metrics further downstream do aggregate this information. Depends-on : - Returns : Table Back to top style.spreadOfErrorProneness Short name : spreadOfErrorProneness Friendly name : Spread of error proneness style violations over files Between 0 and 1 how evenly spread are the style violations which indicate error proneness. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.errorProneness Returns : real Back to top rascal.testability.java.TestOverPublicMethods Short name : percentageOfTestedPublicMethods Friendly name : Number of JUnit tests averaged over the total number of public methods Number of JUnit tests averaged over the total number of public methods. Ideally all public methods are tested. With this number we compute how far from the ideal situation the project is. Depends-on : - Returns : real Back to top rascal.testability.java.NumberOfTestMethods Short name : numberOfTestMethods Friendly name : Number of JUnit test methods. This is an intermediate absolute metric used to compute others. The bare metric is hard to compare between projects. Number of JUnit test methods Depends-on : - Returns : int Back to top rascal.testability.java.TestCoverage Short name : estimateTestCoverage Friendly name : Static Estimation of test coverage This is a static over-estimation of test coverage: which code is executed in the system when all JUnit test cases are executed? We approximate this by using the static call graphs and assuming every method which can be called, will be called. This leads to an over-approximation, as compared to a dynamic code coverage analysis, but the static analysis does follow the trend and a low code coverage here is an good indicator for a lack in testing effort for the project. Depends-on : - Returns : real Back to top trans.rascal.OO.java.MIF-Java Short name : MIF_Java Friendly name : Method inheritance factor (Java) Method inheritance factor (Java) Depends-on : - Returns : map[loc, real] Back to top trans.rascal.OO.java.Ca-Java-Quartiles Short name : Ca_Java_Q Friendly name : Afferent coupling quartiles (Java) Afferent coupling quartiles (Java) Depends-on : trans.rascal.OO.java.Ca-Java Returns : map[str, real] Back to top trans.rascal.OO.java.DAC-Java Short name : DAC_Java Friendly name : Data abstraction coupling (Java) Data abstraction coupling (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.OO.java.CF-Java Short name : CF_Java Friendly name : Coupling factor (Java) Coupling factor (Java) Depends-on : - Returns : real Back to top trans.rascal.OO.java.I-Java Short name : I_Java Friendly name : Instability (Java) Instability (Java) Depends-on : trans.rascal.OO.java.Ce-Java trans.rascal.OO.java.Ca-Java Returns : map[loc, real] Back to top trans.rascal.OO.java.DAC-Java-Quartiles Short name : DAC_Java_Q Friendly name : Data abstraction coupling quartiles (Java) Data abstraction coupling quartiles (Java) Depends-on : trans.rascal.OO.java.DAC-Java Returns : map[str, real] Back to top trans.rascal.OO.java.MPC-Java-Quartiles Short name : MPC_Java_Q Friendly name : Message passing coupling quartiles (Java) Message passing coupling quartiles (Java) Depends-on : trans.rascal.OO.java.MPC-Java Returns : map[str, real] Back to top trans.rascal.OO.java.NOM-Java Short name : NOM_Java Friendly name : Number of methods (Java) Number of methods (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.OO.java.LCOM-Java Short name : LCOM_Java Friendly name : Lack of cohesion in methods (Java) Lack of cohesion in methods (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.OO.java.CBO-Java Short name : CBO_Java Friendly name : Coupling between objects (Java) Coupling between objects (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.OO.java.Ce-Java Short name : Ce_Java Friendly name : Efferent coupling (Java) Efferent coupling (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.OO.java.PF-Java Short name : PF_Java Friendly name : Polymorphism factor (Java) Polymorphism factor (Java) Depends-on : - Returns : real Back to top trans.rascal.OO.java.RFC-Java-Quartiles Short name : RFC_Java_Q Friendly name : Response for class quartiles (Java) Response for class quartiles (Java) Depends-on : trans.rascal.OO.java.RFC-Java Returns : map[str, real] Back to top trans.rascal.OO.java.I-Java-Quartiles Short name : I_Java_Q Friendly name : Instability quartiles (Java) Instability quartiles (Java) Depends-on : trans.rascal.OO.java.I-Java Returns : map[str, real] Back to top trans.rascal.OO.java.RFC-Java Short name : RFC_Java Friendly name : Response for class (Java) Response for class (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.OO.java.LCC-Java Short name : LCC_Java Friendly name : Loose class cohesion (Java) Loose class cohesion (Java) Depends-on : - Returns : map[loc, real] Back to top trans.rascal.OO.java.MIF-Java-Quartiles Short name : MIF_Java_Q Friendly name : Method inheritance factor quartiles (Java) Method inheritance factor quartiles (Java) Depends-on : trans.rascal.OO.java.MIF-Java Returns : map[str, real] Back to top trans.rascal.OO.java.DIT-Java Short name : DIT_Java Friendly name : Depth of inheritance tree (Java) Depth of inheritance tree (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.OO.java.MHF-Java Short name : MHF_Java Friendly name : Method hiding factor (Java) Method hiding factor (Java) Depends-on : - Returns : real Back to top trans.rascal.OO.java.TCC-Java Short name : TCC_Java Friendly name : Tight class cohesion (Java) Tight class cohesion (Java) Depends-on : - Returns : map[loc, real] Back to top trans.rascal.OO.java.AHF-Java Short name : AHF_Java Friendly name : Attribute hiding factor (Java) Attribute hiding factor (Java) Depends-on : - Returns : real Back to top trans.rascal.OO.java.LCOM-Java-Quartiles Short name : LCOM_Java_Q Friendly name : Lack of cohesion in methods quartiles (Java) Lack of cohesion in methods quartiles (Java) Depends-on : trans.rascal.OO.java.LCOM-Java Returns : map[str, real] Back to top trans.rascal.OO.java.Ca-Java Short name : Ca_Java Friendly name : Afferent coupling (Java) Afferent coupling (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.OO.java.A-Java Short name : A_Java Friendly name : Abstractness (Java) Abstractness (Java) Depends-on : - Returns : real Back to top trans.rascal.OO.java.DIT-Java-Quartiles Short name : DIT_Java_Q Friendly name : Depth of inheritance tree quartiles (Java) Depth of inheritance tree quartiles (Java) Depends-on : trans.rascal.OO.java.DIT-Java Returns : map[str, real] Back to top trans.rascal.OO.java.TCC-Java-Quartiles Short name : TCC_Java_Q Friendly name : Tight class cohesion quartiles (Java) Tight class cohesion quartiles (Java) Depends-on : trans.rascal.OO.java.TCC-Java Returns : map[str, real] Back to top trans.rascal.OO.java.LCOM4-Java-Quartiles Short name : LCOM4_Java_Q Friendly name : Lack of cohesion in methods 4 quartiles (Java) Lack of cohesion in methods 4 quartiles (Java) Depends-on : trans.rascal.OO.java.LCOM4-Java Returns : map[str, real] Back to top trans.rascal.OO.java.LCOM4-Java Short name : LCOM4_Java Friendly name : Lack of cohesion in methods 4 (Java) Lack of cohesion in methods 4 (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.OO.java.SR-Java Short name : SR_Java Friendly name : Specialization ratio (Java) Specialization ratio (Java) Depends-on : - Returns : real Back to top trans.rascal.OO.java.AIF-Java-Quartiles Short name : AIF_Java_Q Friendly name : Attribute inheritance factor quartiles (Java) Attribute inheritance factor quartiles (Java) Depends-on : trans.rascal.OO.java.AIF-Java Returns : map[str, real] Back to top trans.rascal.OO.java.NOC-Java-Quartiles Short name : NOC_Java_Q Friendly name : Number of children quartiles (Java) Number of children quartiles (Java) Depends-on : trans.rascal.OO.java.NOC-Java Returns : map[str, real] Back to top trans.rascal.OO.java.NOC-Java Short name : NOC_Java Friendly name : Number of children (Java) Number of children (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.OO.java.AIF-Java Short name : AIF_Java Friendly name : Attribute inheritance factor (Java) Attribute inheritance factor (Java) Depends-on : - Returns : map[loc, real] Back to top trans.rascal.OO.java.RR-Java Short name : RR_Java Friendly name : Reuse ratio (Java) Reuse ratio (Java) Depends-on : - Returns : real Back to top trans.rascal.OO.java.LCC-Java-Quartiles Short name : LCC_Java_Q Friendly name : Loose class cohesion quartiles (Java) Loose class cohesion quartiles (Java) Depends-on : trans.rascal.OO.java.LCC-Java Returns : map[str, real] Back to top trans.rascal.OO.java.NOA-Java Short name : NOA_Java Friendly name : Number of attributes (Java) Number of attributes (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.OO.java.Ce-Java-Quartiles Short name : Ce_Java_Q Friendly name : Efferent coupling quartiles (Java) Efferent coupling quartiles (Java) Depends-on : trans.rascal.OO.java.Ce-Java Returns : map[str, real] Back to top trans.rascal.OO.java.NOM-Java-Quartiles Short name : NOM_Java_Q Friendly name : Number of methods quartiles (Java) Number of methods quartiles (Java) Depends-on : trans.rascal.OO.java.NOM-Java Returns : map[str, real] Back to top trans.rascal.OO.java.NOA-Java-Quartiles Short name : NOA_Java_Q Friendly name : Number of attributes quartiles (Java) Number of attributes quartiles (Java) Depends-on : trans.rascal.OO.java.NOA-Java Returns : map[str, real] Back to top trans.rascal.OO.java.CBO-Java-Quartiles Short name : CBO_Java_Q Friendly name : Coupling between objects quartiles (Java) Coupling between objects quartiles (Java) Depends-on : trans.rascal.OO.java.CBO-Java Returns : map[str, real] Back to top trans.rascal.OO.java.MPC-Java Short name : MPC_Java Friendly name : Message passing coupling (Java) Message passing coupling (Java) Depends-on : - Returns : map[loc, int] Back to top trans.rascal.LOC.java.LOCoverJavaClass Short name : giniLOCOverClassJava Friendly name : Distribution of physical lines of code over Java classes, interfaces and enums The distribution of physical lines of code over Java classes, interfaces and enums explains how complexity is distributed over the design elements of a system. Depends-on : - Returns : real Back to top trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles Short name : countUsesOfAdvancedLanguageFeaturesQ Friendly name : Usage of advanced Java features quartiles Quartiles of counts of advanced Java features (wildcards, union types and anonymous classes). The numbers indicate the thresholds that delimit the first 25%, 50% and 75% of the data as well as the maximum and minumum values. Depends-on : trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJava Returns : map[str, real] Back to top trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJava Short name : countUsesOfAdvancedLanguageFeatures Friendly name : Usage of advanced Java features Usage of advanced Java features (wildcards, union types and anonymous classes), reported per file and line number of the occurrence. This metric is for downstream processing by other metrics. Depends-on : - Returns : map[loc file, int count] Back to top trans.rascal.CC.java.CCHistogramJava Short name : CCHistogramJava Friendly name : Number of Java methods per CC risk factor Number of Java methods per CC risk factor, counts the number of methods which are in a low, medium or high risk factor. The histogram can be compared between projects to indicate which is probably easier to maintain on a method-by-method basis. Depends-on : trans.rascal.CC.java.CCJava Returns : map[str, int] Back to top trans.rascal.CC.java.CCOverJavaMethods Short name : giniCCOverMethodsJava Friendly name : CC over Java methods Calculates how cyclomatic complexity is spread over the methods of a system. If high CC is localized, then this may be easily fixed but if many methods have high complexity, then the project may be at risk. This metric is good to compare between projects. Depends-on : trans.rascal.CC.java.CCJava Returns : real Back to top trans.rascal.CC.java.CCJava Short name : getCC Friendly name : McCabe's Cyclomatic Complexity Metric (Java) Cyclomatic complexity is a measure of the number of unique control flow paths in the methods of a class. This indicates how many different test cases you would need to test the method. A high number indicates also a lot of work to understand the method. This metric is a basic metric for further processing downstream. It is not easily compared between projects. Depends-on : - Returns : map[loc, int] Back to top trans.rascal.CC.java.WMCJava Short name : getWMC Friendly name : Weighted Method Count (Java) Cyclomatic complexity is a measure of the number of unique control flow paths in the methods of a class. This indicates how many different test cases you would need to test the method. A high number indicates also a lot of work to understand the method. The weighted method count for a class is the sum of the cyclomatic complexity measures of all methods in the class. This metric is a basic metric for further processing downstream. It is not easily compared between projects. Depends-on : trans.rascal.CC.java.CCJava Returns : map[loc class, int wmcCount] Back to top Transient Metric Providers for OSGi Dependencies These metrics are related to OSGi dependencies declared in MANIFEST.MF files. Back to top trans.rascal.dependency.numberRequiredPackagesInSourceCode Short name : numberRequiredPackagesInSourceCode Friendly name : Number required packages in source code Retrieves the number of required packages found in the project source code. Depends-on : - Returns : int Back to top trans.rascal.dependency.osgi.allOSGiPackageDependencies Short name : allOSGiPackageDependencies Friendly name : All OSGi package dependencies Retrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). Depends-on : - Returns : set[loc] Back to top trans.rascal.dependency.osgi.unversionedOSGiRequiredBundles Short name : unversionedOSGiRequiredBundles Friendly name : Unversioned OSGi required bundles Retrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header). If returned value != {} there is a smell in the Manifest. Depends-on : - Returns : set[loc] Back to top trans.rascal.dependency.osgi.unusedOSGiImportedPackages Short name : unusedOSGiImportedPackages Friendly name : Unused OSGi imported packages Retrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell). Depends-on : - Returns : set[loc] Back to top trans.rascal.dependency.osgi.numberOSGiSplitImportedPackages Short name : numberOSGiSplitImportedPackages Friendly name : Number OSGi split imported packages Retrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest. Depends-on : - Returns : int Back to top trans.rascal.dependency.osgi.ratioUnusedOSGiImportedPackages Short name : ratioUnusedOSGiImportedPackages Friendly name : Ratio of unused OSGi imported packages Retrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages. Depends-on : trans.rascal.dependency.osgi.unusedOSGiImportedPackages Returns : real Back to top trans.rascal.dependency.osgi.allOSGiBundleDependencies Short name : allOSGiBundleDependencies Friendly name : All OSGi bundle dependencies Retrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies). Depends-on : - Returns : set[loc] Back to top trans.rascal.dependency.osgi.unversionedOSGiExportedPackages Short name : unversionedOSGiExportedPackages Friendly name : Unversioned OSGi exported packages Retrieves the set of unversioned OSGi exported packages (declared in the Export-Package header). If returned value != {} there is a smell in the Manifest. Depends-on : - Returns : set[loc] Back to top trans.rascal.dependency.osgi.numberOSGiSplitExportedPackages Short name : numberOSGiSplitExportedPackages Friendly name : Number OSGi split exported packages Retrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest. Depends-on : - Returns : int Back to top trans.rascal.dependency.osgi.allOSGiDynamicImportedPackages Short name : allOSGiDynamicImportedPackages Friendly name : All OSGi dynamically imported packages Retrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file. Depends-on : - Returns : set[loc] Back to top trans.rascal.dependency.osgi.numberOSGiBundleDependencies Short name : numberOSGiBundleDependencies Friendly name : Number all OSGi bundle dependencies Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies). Depends-on : - Returns : int Back to top trans.rascal.dependency.osgi.ratioUnversionedOSGiImportedPackages Short name : ratioUnversionedOSGiImportedPackages Friendly name : Ratio unversioned OSGi imported packages Retrieves the ratio of unversioned OSGi imported packages. Depends-on : trans.rascal.dependency.osgi.unversionedOSGiImportedPackages Returns : real Back to top trans.rascal.dependency.osgi.unversionedOSGiImportedPackages Short name : unversionedOSGiImportedPackages Friendly name : Unversioned OSGi imported packages Retrieves the set of unversioned OSGi imported packages (declared in the Import-Package header). If returned value != {} there is a smell in the Manifest. Depends-on : - Returns : set[loc] Back to top trans.rascal.dependency.osgi.numberOSGiPackageDependencies Short name : numberOSGiPackageDependencies Friendly name : Number of all OSGi package dependencies Retrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). Depends-on : - Returns : int Back to top trans.rascal.dependency.osgi.ratioUnversionedOSGiRequiredBundles Short name : ratioUnversionedOSGiRequiredBundles Friendly name : Ratio unversioned OSGi required bundles Retrieves the ratio of unversioned OSGi required bundles. Depends-on : trans.rascal.dependency.osgi.unversionedOSGiRequiredBundles Returns : real Back to top trans.rascal.dependency.osgi.usedOSGiUnimportedPackages Short name : usedOSGiUnimportedPackages Friendly name : Used OSGi unimported packages Retrieves the set of used but unimported packages. This metric does not consider packages implicitly imported through the Bundle-Require header. If set != {} then developers may be depending on the execution environment (smell). Depends-on : - Returns : set[loc] Back to top trans.rascal.dependency.osgi.ratioUnversionedOSGiExportedPackages Short name : ratioUnversionedOSGiExportedPackages Friendly name : Ratio of unversioned OSGi exported packages Retrieves the ratio of unversioned OSGi exported packages. Depends-on : trans.rascal.dependency.osgi.unversionedOSGiExportedPackages Returns : real Back to top trans.rascal.dependency.osgi.ratioUsedOSGiImportedPackages Short name : ratioUsedOSGiImportedPackages Friendly name : Ratio of used OSGi imported packages Retrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code. Depends-on : - Returns : real Back to top Transient Metric Providers for Maven dependencies These metrics are related to Maven dependencies declared in pom.xml files. Back to top trans.rascal.dependency.numberRequiredPackagesInSourceCode Short name : numberRequiredPackagesInSourceCode Friendly name : Number required packages in source code Retrieves the number of required packages found in the project source code. Depends-on : - Returns : int Back to top trans.rascal.dependency.maven.ratioOptionalMavenDependencies Short name : ratioOptionalMavenDependencies Friendly name : Ratio optional Maven dependencies Retrieves the ratio of optional Maven dependencies. Depends-on : trans.rascal.dependency.maven.allOptionalMavenDependencies Returns : real Back to top trans.rascal.dependency.maven.numberUniqueMavenDependencies Short name : numberUniqueMavenDependencies Friendly name : Number unique Maven dependencies Retrieves the number of unique Maven dependencies. Depends-on : - Returns : int Back to top trans.rascal.dependency.maven.allOptionalMavenDependencies Short name : allOptionalMavenDependencies Friendly name : All optional Maven dependencies Retrieves all the optional Maven dependencies. Depends-on : - Returns : set[loc] Back to top trans.rascal.dependency.maven.isUsingTycho Short name : isUsingTycho Friendly name : Is using Tycho Checks if the current project is a Tycho project. Depends-on : - Returns : bool Back to top trans.rascal.dependency.maven.numberMavenDependencies Short name : numberMavenDependencies Friendly name : Number Maven dependencies Retrieves the number of Maven dependencies. Depends-on : - Returns : int Back to top trans.rascal.dependency.maven.allMavenDependencies Short name : allMavenDependencies Friendly name : All Maven dependencies Retrieves all the Maven dependencies. Depends-on : - Returns : set[loc] Back to top Transient Metric Providers for Docker Dependencies This metric is related to Docker dependencies declared in Dockerfiles. Back to top org.eclipse.scava.metricprovider.trans.configuration.docker.dependencies Short name : trans.configuration.docker.dependencies Friendly name : Dependencies declared in Dockerfiles Retrieves the names of the dependencies that are declared in the Dockerfiles of a project and additional information such as their version and type. Depends-on : None Returns : DockerDependency which contains: Variable Type dependencyName String dependencyVersion String type String subType String Back to top Transient Metric Providers for Puppet Dependencies This metric is related to Puppet dependencies declared in Puppet manifests. Back to top org.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies Short name : trans.configuration.puppet.dependencies Friendly name : Dependencies declared in Puppet manifests Retrieves the names of the dependencies that are declared in the Puppet manifests of a project and additional information such as their version and type. Depends-on : None Returns : PuppetDependency which contains: Variable Type dependencyName String dependencyVersion String type String Back to top Transient Metric Providers for Docker Smells This metric is related to Docker smells detected in Dockerfiles. Back to top org.eclipse.scava.metricprovider.trans.configuration.docker.smells Short name : trans.configuration.docker.smells Friendly name : Smells detected in Dockerfiles Detects the smells in the Dockerfiles of a project and additional information such as their reason, the file and the line that each smells is detected. Depends-on : None Returns : Smell which contains: Variable Type smellName String reason String code String fileName String line String Back to top Transient Metric Providers for Puppet Smells These metrics are related to Puppet smells detected in Puppet manifests. Back to top org.eclipse.scava.metricprovider.trans.configuration.puppet.designsmells Short name : trans.configuration.puppet.designsmells Friendly name : Design smells detected in Puppet manifests Detects the design smells in the Puppet manifests of a project and additional information such as their reason and the file that each smells is detected. Depends-on : None Returns : Smell which contains: Variable Type smellName String reason String fileName String Back to top org.eclipse.scava.metricprovider.trans.configuration.puppet.implementationsmells Short name : trans.configuration.puppet.implementationsmells Friendly name : Implementation smells detected in Puppet manifests Detects the implementation smells in the Puppet manifests of a project and additional information such as their reason, the file and the line that each smells is detected. Depends-on : None Returns : Smell which contains: Variable Type smellName String reason String fileName String line String Back to top Transient Metric Providers for Docker Antipatterns This metric is related to Docker antipatterns detected in Dockerfiles. Back to top org.eclipse.scava.metricprovider.trans.configuration.docker.antipatterns Short name : trans.configuration.docker.antipatterns Friendly name : Antipatterns detected in Dockerfiles Detects the antipatterns in the Dockerfiles of a project and additional information such as their reason, the file and the line that each antipattern is detected and the commit and date that this antipattern is related. Depends-on : None Returns : DockerAntipattern which contains: Variable Type smellName String reason String code String fileName String line String commit String date String Back to top Transient Metric Providers for Puppet Antipatterns These metrics are related to Puppet antipatterns detected in Puppet manifests. Back to top org.eclipse.scava.metricprovider.trans.configuration.puppet.designantipatterns Short name : trans.configuration.puppet.designantipatterns Friendly name : Design antipatterns detected in Puppet manifests Detects the design antipatterns in the Puppet manifests of a project and additional information such as their reason, the file that each antipattern is detected and the commit and date that this antipattern is related. Depends-on : None Returns : DesignAntipattern which contains: Variable Type smellName String reason String fileName String commit String date String Back to top org.eclipse.scava.metricprovider.trans.configuration.puppet.implementationantipatterns Short name : trans.configuration.puppet.implementationantipatterns Friendly name : Implementation antipatterns detected in Puppet manifests Detects the implementation antipatterns in the Puppet manifests of a project and additional information such as their reason, the file and the line that each antipattern is detected and the commit and date that this antipattern is related. Depends-on : None Returns : Smell which contains: Variable Type smellName String reason String fileName String line String commit String date String org.eclipse.scava.metricprovider.trans.configuration.projects.relations Back to top Transient Metric Providers for Indexing These metrics facilitate data indexing unto the platform. Back to top Transient Metric Providers for Projects Relations This metric is related to the relations between projects that are analysed at the platform. Back to top org.eclipse.scava.metricprovider.trans.configuration.projects.relations Short name : trans.configuration.projects.relations Friendly name : Relations between projects Detects the relations between projects that are already analysed at the platform by determining if a project is used as dependency by another project. Depends-on : None Returns : ProjectRelation which contains: Variable Type relationName String dependencyType String Back to top Transient Metric Providers for New Versions These metrics are related to the new version of the dependencies of the projects that are analysed at the platform. Back to top org.eclipse.scava.metricprovider.trans.newversion.docker Short name : trans.newversion.docker Friendly name : New versions of Docker dependencies Detects the new versions of dependencies of Docker based projects. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.docker.dependencies Returns : NewDockerVersion which contains: Variable Type packageName String oldVersion String newVersion String Back to top org.eclipse.scava.metricprovider.trans.newversion.puppet Short name : trans.newversion.puppet Friendly name : New versions of Puppet dependencies Detects the new versions of dependencies of Puppet based projects. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies Returns : NewPuppetVersion which contains: Variable Type packageName String oldVersion String newVersion String Back to top org.eclipse.scava.metricprovider.trans.newversion.osgi Short name : trans.newversion.osgi Friendly name : New versions of OSGi dependencies Detects the new versions of dependencies of OSGi based projects. Depends-on : None Returns : NewOsgiVersion which contains: Variable Type packageName String oldVersion String newVersion String Back to top org.eclipse.scava.metricprovider.trans.newversion.maven Short name : trans.newversion.maven Friendly name : New versions of Maven dependencies Detects the new versions of dependencies of Maven based projects. Depends-on : None Returns : NewMavenVersion which contains: Variable Type packageName String oldVersion String newVersion String Back to top org.eclipse.scava.metricprovider.trans.indexing.preparation Short name : index preparation transmetric Friendly name : index preparation This identifies the metric(s) that have been chosen to be executed by the user in preparation for indexing (note: This is required to enable the indexing capabilities of the platform to be dynamic. Depends-on : None Returns : IndexPrepTransMetric which contains: Variable Type executedMetricProviders List<ExecutedMetricProviders> Additional Information : ExecutedMetricProviders : List<String> metricIdentifiers Back to top org.eclipse.scava.metricprovider.indexing.bugs Short name : bug indexing metric Friendly name : bug tracking system indexer This metric prepares and indexes documents relating to bug tracking systems. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : BugsIndexingMetric Back to top org.eclipse.scava.metricprovider.indexing.commits Short name : metricprovider.indexing.commits Friendly name : Commits indexer This metric prepares and indexes documents relating to commits. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : CommitsIndexingMetric Back to top org.eclipse.scava.metricprovider.indexing.communicationchannels Short name : communication channels indexing metric Friendly name : communication channels indexer This metric prepares and indexes documents relating to communication channels. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : CommunicationChannelsIndexingMetric Back to top org.eclipse.scava.metricprovider.indexing.documentation Short name : metricprovider.indexing.documentation Friendly name : Documentation indexer This metric prepares and indexes documents relating to documentation. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.documentation Returns : DocumentationIndexingMetric Back to top Transient Metric Providers for API These transient metrics are related to the analysis and evolution of API Back to top org.eclipse.scava.metricprovider.trans.migrationissuesmaracas Short name : trans.migrationissuesmaracas Friendly name : Migration Issues Detection using Maracas This metric convert the changes found by Maracas into Regex useful for other metrics. Depends-on : trans.rascal.api.changedMethods Returns : MigrationIssueMaracasTransMetric which contains: Variable Type maracasMeasurements List<MaracasMeasurement> Additional Information : MaracasMeasurement : List<String> regex String change int lastUpdateDate Back to top Factoids Factoids are plugins used to present data that has been mined and analysed using one or more historic and/or transient metric providers. Back to top Factoids for Bug Trackers These factoids are related to bug tracking systems. Back to top org.eclipse.scava.factoid.bugs.channelusage Short name : factoid.bugs.channelusage Friendly name : Bug Tracker Usage data This plugin generates the factoid regarding usage data for bug trackers. For example, the total number of new bugs, comments or patches per year. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.newbugs , org.eclipse.scava.metricprovider.historic.bugs.comments , org.eclipse.scava.metricprovider.historic.bugs.patches Additional Information : Star rating information : 4 star number of bugs or comments > working days in a year (250) . 3 star 2 x number of bugs or comments > working days in a year (250) . 2 star 4 x number of bugs or comments > working days in a year (250) . 1 star otherwise Back to top org.eclipse.scava.factoid.bugs.emotion Short name : factoid.bugs.emotion Friendly name : Bug Tracker Emotions This plugin generates the factoid regarding emotions for bug trackers. For example, the percentage of positive, negative or surprise emotions expressed. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Anger, fear and sadness are considered negative while joy and love are considered positive. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.newbugs , org.eclipse.scava.metricprovider.historic.bugs.comments , org.eclipse.scava.metricprovider.historic.bugs.patches Additional Information : Star rating information : 4 star positive emotion percentage > 80 OR negative emotion percentage < 35 . 3 star positive emotion percentage > 65 OR negative emotion percentage < 50 . 2 star positive emotion percentage > 50 OR negative emotion percentage < 65 . 1 star otherwise Back to top org.eclipse.scava.factoid.bugs.hourly Short name : factoid.bugs.hourly Friendly name : Bug Tracker hourly data This plugin generates the factoid regarding hourly statistics for bug trackers. For example, the percentage of bugs, comments etc. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies Additional Information : Star rating information : 4 star maximum percentage of hourly comments > 2 x uniform percentage of comments per hour (100/24) . 3 star maximum percentage of hourly comments > 4 x uniform percentage of comments per hour (100/24) . 2 star maximum percentage of hourly comments > 6 x uniform percentage of comments per hour (100/24) . 1 star otherwise Back to top org.eclipse.scava.factoid.bugs.responsetime Short name : factoid.bugs.responsetime Friendly name : Bug Tracker Response Time This plugin generates the factoid regarding response time for bug trackers. This could be a cummulative average, yearly average etc. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.responsetime Additional Information : Star rating information : 4 star Zero(0) < yearly average response time < eight hours milliseconds (8 x 60 x 60 x 1000) . 3 star Zero(0) < yearly average response time < day milliseconds (3 x eight hour milliseconds) . 2 star Zero(0) < yearly average response time < week milliseconds (7 x week milliseconds) . 1 star otherwise Back to top org.eclipse.scava.factoid.bugs.sentiment Short name : factoid.bugs.sentiment Friendly name : Bug Tracker Sentiment This plugin generates the factoid regarding sentiment for bug trackers. For example, the average sentiment in all bug trackers associated to a project. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment) Depends-on : org.eclipse.scava.metricprovider.historic.bugs.sentiment Additional Information : Star rating information : 4 star average sentiment > 0.5 OR thread end sentiment - thread begining sentiment > 0.25 && thread begining sentiment > 0.15 . 3 star average sentiment > 0.25 OR thread end sentiment - thread begining sentiment > 0.125 && thread begining sentiment > 0.0 . 2 star average sentiment > 0 OR thread end sentiment - thread begining sentiment > 0 . 1 star otherwise Back to top org.eclipse.scava.factoid.bugs.severity Short name : factoid.bugs.severity Friendly name : Bug Tracker Severity This plugin generates the factoid regarding severity for bug trackers. For example, the number of bugs per severity level, the average sentiment for each severity etc. There are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. Also, blocker , critical and major are regarded as serious bugs. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.severity , org.eclipse.scava.metricprovider.historic.bugs.severitybugstatus , org.eclipse.scava.metricprovider.historic.bugs.severityresponsetime , org.eclipse.scava.metricprovider.historic.bugs.severitysentiment Additional Information : Star rating information : 1 star percentage of serious bugs > 50 . 2 star percentage of serious bugs > 25 . 3 star percentage of serious bugs > 12.5 . 4 star otherwise (i.e., fewer percentage of serious bugs). Back to top org.eclipse.scava.factoid.bugs.size Short name : factoid.bugs.size Friendly name : Bug Tracker Size This plugin generates the factoid regarding bug size for bug trackers. For example, the cumulative number of bug comments or patches. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.newbugs , org.eclipse.scava.metricprovider.historic.bugs.comments , org.eclipse.scava.metricprovider.historic.bugs.patches Additional Information : Star rating information : 4 star number of bugs or parches > 1000 OR number of comments > 10000 . 3 star 2 x number of bugs or parches > 1000 OR 2 x number of comments > 10000 . 2 star 4 x number of bugs or parches > 1000 OR 4 x number of comments > 10000 . 1 star otherwise Back to top org.eclipse.scava.factoid.bugs.status Short name : factoid.bugs.status Friendly name : Bug Tracker Status This plugin generates the factoid regarding bug status for bug trackers. For example, the number of fixed bugs, duplicate bugs etc. There are 7 bug status labels (resolved, nonResolved, fixed, worksForMe, wontFix, invalid and duplicate). Depends-on : org.eclipse.scava.metricprovider.historic.bugs.status Additional Information : Star rating information : 4 star perventage of resolved bug > 75 . 3 star perventage of resolved bug > 50 . 2 star perventage of resolved bug > 25 . 1 star otherwise (i.e., very few resolved bugs) Back to top org.eclipse.scava.factoid.bugs.threadlength Short name : factoid.bugs.threadlength Friendly name : Bug Tracker Thread Length This plugin generates the factoid regarding bug thread length for bug trackers. For example, the average length of discussion associated to bugs. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.bugs Additional Information : Star rating information : 4 star Zero(0) < average comments < 5 . 3 star Zero(0) < average comments < 10 . 2 star Zero(0) < average comments < 20 . 1 star otherwise Back to top org.eclipse.scava.factoid.bugs.users Short name : factoid.bugs.users Friendly name : Bug Tracker Users This plugin generates the factoid regarding users for bug trackers. For example, the average number of users associated to a project in a bug tracking system. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.users , org.eclipse.scava.metricprovider.historic.bugs.bugs Additional Information : Star rating information : 4 star daily new users in last month > 8 x 0.25 OR daily active users in last month > 8 x 2.5 OR daily new users in last year > 4 x 0.25 OR daily active users in last year > 4 x 2.5*. 3 star daily new users in last month > 4 x 0.25 OR daily active users in last month > 4 x 2.5 OR daily new users in last year > 2 x 0.25 OR daily active users in last year > 2 x 2.5*. 2 star daily new users in last month > 2 x 0.25 OR daily active users in last month > 2 x 2.5 OR daily new users in last year > 0.25 OR daily active users in last year > 2.5 . 1 star otherwise Back to top org.eclipse.scava.factoid.bugs.weekly Short name : factoid.bugs.weekly Friendly name : Bug Tracker Weekly This plugin generates the factoid regarding weekly user engagements for bug trackers. For example, the average number of bug comments per week. This can be used to present the most and least busy week in terms of engagement for a particular project. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.dailyrequestsreplies Additional Information : Star rating information : 4 star maximum percentage of weekly comments < 2 x uniform percentage of comments per week (100/7) . 3 star maximum percentage of weekly comments < 3 x uniform percentage of comments per week (100/7) . 2 star maximum percentage of weekly comments < 4 x uniform percentage of comments per week (100/7) . 1 star otherwise Back to top Factoids for Newsgroups and Forums These factoids are related to communication channels. Back to top org.eclipse.scava.factoid.newsgroups.channelusage Short name : factoid.newsgroups.channelusage Friendly name : Newsgroup Channel Usage This plugin generates the factoid regarding usage data for newsgroups. For example, the total number of new articles or threads per year. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.articles , org.eclipse.scava.metricprovider.historic.newsgroups.newthreads Additional Information : Star rating information : 4 star number of articles OR threads > working days in a year (250 ). 3 star 2 x number of articles OR threads > working days in a year (250) . 2 star 4 x number of articles OR threads > working days in a year (250) . 1 star otherwise Back to top org.eclipse.scava.factoid.newsgroups.emotion Short name : factoid.newsgroups.emotion Friendly name : Newsgroup Channel Emotion This plugin generates the factoid regarding emotions for newsgroups, such as the percentage of positive, negative or surprise emotions expressed. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Anger, fear and sadness are considered negative while joy and love are considered positive. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.emotions Additional Information : Star rating information : 4 star positive emotion percentage > 80 OR negative emotion percentage < 35 . 3 star positive emotion percentage > 65 OR negative emotion percentage < 50 . 2 star positive emotion percentage > 50 OR negative emotion percentage < 65 . 1 star otherwise Back to top org.eclipse.scava.factoid.newsgroups.hourly Short name : factoid.newsgroups.hourly Friendly name : Newsgroup Channel hourly data This plugin generates the factoid regarding hourly data for newsgroups, such as the percentage of articles, threads etc. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies Additional Information : Star rating information : 4 star maximum percentage of hourly articles > 2 x uniform percentage of articles per hour (100/24) . 3 star maximum percentage of hourly articles > 4 x uniform percentage of articles per hour (100/24) . 2 star maximum percentage of hourly articles > 6 x uniform percentage of articles per hour (100/24) . 1 star otherwise Back to top org.eclipse.scava.factoid.newsgroups.responsetime Short name : factoid.newsgroups.responsetime Friendly name : Newsgroup Channel Response Time This plugin generates the factoid regarding response time for newsgroups. This could be a cummulative average, yearly average etc. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.responsetime Additional Information : Star rating information : 4 star Zero(0) < yearly average response time < eight hours in milliseconds (8 x 60 x 60 x 1000) . 3 star Zero(0) < yearly average response time < day in milliseconds (3 x eight hours in milliseconds) . 2 star Zero(0) < yearly average response time < week in milliseconds (7 x day in milliseconds) . 1 star otherwise Back to top org.eclipse.scava.factoid.newsgroups.sentiment Short name : factoid.newsgroups.sentiment Friendly name : Newsgroup Channel Sentiment This plugin generates the factoid regarding sentiments for newsgroups. For example, the average sentiment in all newsgroup channel associated to a project. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment) Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.sentiment Additional Information : Star rating information : 4 star average sentiment > 0.5 OR thread end sentiment - thread begining sentiment > 0.25 && thread begining sentiment > 0.15 . 3 star average sentiment > 0.25 OR thread end sentiment - thread begining sentiment > 0.125 && thread begining sentiment > 0.0 . 2 star average sentiment > 0 OR thread end sentiment - thread begining sentiment > 0 . 1 star otherwise Back to top org.eclipse.scava.factoid.newsgroups.severity Short name : factoid.newsgroups.severity Friendly name : Newsgroup Channel Severity This plugin generates the factoid regarding severity for newsgroups. For example, the number of articles per severity level, the average sentiment for each severity etc. There are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Note: blocker , critical and major are regarded as serious bugs. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.severityresponsetime , org.eclipse.scava.metricprovider.historic.newsgroups.severity , org.eclipse.scava.metricprovider.historic.newsgroups.severitysentiment Additional Information : Star rating information : 1 star percentage of serious bugs > 50 . 2 star percentage of serious bugs > 25 . 3 star percentage of serious bugs > 12.5 . 4 star otherwise (i.e., fewer percentage of serious bugs). Back to top org.eclipse.scava.factoid.newsgroups.size Short name : factoid.newsgroups.size Friendly name : Newsgroup Channel Size This plugin generates the factoid regarding thread or article size for newsgroups. For example, the cummulative number of threads. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.articles , org.eclipse.scava.metricprovider.historic.newsgroups.newthreads Additional Information : Star rating information : 4 star number of threads > 1000 OR number of articles > 10000 . 3 star 2 x number of threads > 1000 OR 2 x number of articles > 10000 . 2 star 4 x number of threads > 1000 OR 4 x number of articles > 10000 . 1 star otherwise Back to top org.eclipse.scava.factoid.newsgroups.status Short name : factoid.newsgroups.status Friendly name : Newsgroup Channel Status This plugin generates the factoid regarding thread or article status for newsgroups. For example, the number of requests and replies, unanswered threads etc. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.unansweredthreads , org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies , org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies.average Additional Information : Star rating information : 4 star perventage of replies > 75 . 3 star perventage of replies > 50 . 2 star perventage of replies > 25 . 1 star otherwise (i.e., very few replies) Back to top org.eclipse.scava.factoid.newsgroups.threadlength Short name : factoid.newsgroups.threadlength Friendly name : Newsgroup Channel Thread Length This plugin generates the factoid regarding thread length for newsgroups. For example, the average length of discussion per day, month etc. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.threads Additional Information : Star rating information : 4 star Zero(0) < average comments < 5 . 3 star Zero(0) < average comments < 10 . 2 star Zero(0) < average comments < 20 . 1 star otherwise Back to top org.eclipse.scava.factoid.newsgroups.users Short name : factoid.newsgroups.users Friendly name : Newsgroup Channel Users This plugin generates the factoid regarding users for newsgroups. For example, the average number of users associated to a project in a newsgroup channel. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.users , org.eclipse.scava.metricprovider.historic.newsgroups.threads Additional Information : Star rating information : 4 star daily new users in last month > 8 x 0.25 OR daily active users in last month > 8 x 2.5 OR daily new users in last year > 4 x 0.25 OR daily active users in last year > 4 x 2.5 . 3 star daily new users in last month > 4 x 0.25 OR daily active users in last month > 4 x 2.5 OR daily new users in last year > 2 x 0.25 OR daily active users in last year > 2 x 2.5 . 2 star daily new users in last month > 2 x 0.25 OR daily active users in last month > 2 x 2.5 OR daily new users in last year > 0.25 OR daily active users in last year > 2.5 . 1 star otherwise Back to top org.eclipse.scava.factoid.newsgroups.weekly Short name : factoid.newsgroups.weekly Friendly name : Newsgroup Channel Weekly This plugin generates the factoid regarding weekly user engagement for newsgroups. For example, the average number of comments per week. This can be used to present the most and least busy week in terms of engagement for a particular project. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.dailyrequestsreplies Additional Information : Star rating information : 4 star maximum percentage of weekly articles < 2 x uniform percentage of articles per week (100/7) . 3 star maximum percentage of weekly articles < 3 x uniform percentage of articles per week (100/7) . 2 star maximum percentage of weekly articles < 4 x uniform percentage of articles per week (100/7) . 1 star otherwise Back to top","title":"Metrics Reference Guide"},{"location":"user-guide/metrics/#metrics-reference-guide","text":"This guide describes the historic and transient metric providers, as well as factoids, provided by the Scava platform. Historic Metric Providers for: Bug Trackers Newsgroups and Forums Commits and Committers Documentation Generic Source Code Java Code OSGi Dependencies Maven Dependencies Docker Dependencies Puppet Dependencies Docker Smells Puppet Smells Transient Metric Providers for: Bug Trackers Newsgroups and forums Documentation Natural Language Processing Commits and Committers Generic Source Code Java Code OSGi Dependencies Maven Dependencies Docker Dependencies Puppet Dependencies Docker Smells Puppet Smells Docker Antipatterns Puppet Antipatterns Projects Relations New Versions Indexing API Factoids for: Bug Trackers Newsgroups and Forums","title":"Metrics Reference Guide"},{"location":"user-guide/metrics/#historic-metric-providers","text":"Historic metrics maintain a record of various heuristics associated with a specific open source project over its lifetime. They typically depend on the results from one or more transient metrics and are typically displayed in the Scava dashboards.","title":"Historic Metric Providers"},{"location":"user-guide/metrics/#historic-metric-providers-for-bug-trackers","text":"The following Historic Metric Providers are associated with Issue trackers Back to top","title":"Historic Metric Providers for Bug Trackers"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsbugs","text":"Short name : historic.bugs.bugs Friendly name : Number of bugs per day per bug tracker This metric computes the number of bugs per day for each bug tracker seperately. It also computes additional information such as average comments per bug, average comments per user, average requests and/or replies per user and bug. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata , org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsBugsHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackerData> numberOfBugs int averageCommentsPerBug float averageRequestsPerBug float averageRepliesPerBug float averageCommentsPerUser float averageRequestsPerUser float averageRepliesPerUser float Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfBugs Visualisation Output Information : BugsHistoricMetricProvider : id bugs.bugs id bugs.comments-bugaverage id bugs.comments-useraverage id bugs.requests-bugaverage id bugs.requests-useraverage id bugs.replies-bugaverage id bugs.replies-useraverage id bugs.requestsreplies-useraverage id bugs.requestsreplies-bugaverage Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.bugs"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugscomments","text":"Short name : historic.bugs.comments Friendly name : Number of bug comments per day per bug tracker This metric computes the number of bug comments submitted by the community (users) per day for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.comments Returns : BugsCommentsHistoricMetric which contains: Variable Type Bugs List<DailyBugData> numberOfComments int cumulativeNumberOfComments int Additional Information : DailyBugData : String bugTrackerID int numberOfComments int cumulativeNumberOfComments Visualisation Output Information : CommentsHistoricMetricProvider : id bugs.comments id bugs.cumulativeComments Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.comments"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsemotions","text":"Short name : historic.bugs.emotions Friendly name : Number of emotions per day per bug tracker This metric computes the emotional dimensions present in bug comments submitted by the community (users) per day for each bug tracker. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise). Depends-on : org.eclipse.scava.metricprovider.trans.bugs.emotions Returns : BugsEmotionsHistoricMetric which contains: Variable Type bugData List<BugData> Dimensions List<Dimensions> Additional Information : BugData : String bugTrackerID int numberOfComments int cumulativeNumberOfComments Dimensions : String bugTrackerId String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfComments int cumulativeNumberOfComments float percentage float cumulativePercentage Visualisation Output Information : EmotionsHistoricMetricProvider : id bugs.emotions.cumulativeComments id bugs.emotions.cumulativeCommentPercentages id bugs.emotions.comments id bugs.emotions.commentPercentages Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.emotions"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsmigrationissues","text":"Short name : historic.bugs.migrationissues Friendly name : Migration Issues Detection in Bug Trackers per day per bug tracker This metric stores how many migration issues have been found per day for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.migrationissues Returns : BugTrackerMigrationIssueHistoricMetric which contains: Variable Type dailyBugTrackerMigrationData List<DailyBugTrackerMigrationData> Additional Information : DailyBugTrackerMigrationData : String bugTrackerId List<String> bugsId; int numberOfBugs Visualisation Output Information : BugTrackerMigrationIssueHistoricMetricProvider : id bugs.dailymigrationissues Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.migrationissues"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsmigrationissuesmaracas","text":"Short name : historic.bugs.migrationissuesmaracas Friendly name : Migration Issues Detection along with Maracas in Bug Trackers per day per bug tracker This metric stores how many migration issues have been found containing changes detected with MARACAS per day for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.migrationissuesmaracas Returns : BugTrackerMigrationIssueMaracasHistoricMetric which contains: Variable Type dailyBugTrackerMigrationMaracasData List<DailyBugTrackerMigrationMaracasData> bugTrackerMigrationMaracasData List<BugTrackerMigrationMaracasData> Additional Information : DailyBugTrackerMigrationMaracasData : String bugTrackerId List<String> bugsId; int numberOfIssues BugTrackerMigrationMaracasData : String bugTrackerId String bugId; List<String> changesAndMatchingPercentage Visualisation Output Information : BugTrackerMigrationIssueMaracasHistoricMetricProvider : id bugs.dailymigrationissuesmaracas id bugs.migrationissuesmaracas.changes Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.migrationissuesmaracas"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsnewbugs","text":"Short name : historic.bugs.newbugs Friendly name : Number of new bugs per day per bug tracker This metric computes the number of new bugs reported by the community (users) per day for each bug tracker. A small number of bug reports can indicate either a bug-free, robust project or a project with a small/inactive user community. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.newbugs Returns : BugsNewBugsHistoricMetric which contains: Variable Type dailyBugData List<DailyBugData> numberOfBugs int cumulativeNumberOfBugs int Additional Information : DailyBugData : String bugTrackerId int numberOfBugs int cumulativeNumberOfBugs Visualisation Output Information : NewUsersHistoricMetricProvider : id bugs.cumulativeNewUsers id bugs.newUsers Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.newbugs"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsnewusers","text":"Short name : historic.bugs.newusers Friendly name : Number of new users per day per bug tracker This metric computes the number of new users per day for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsNewUsersHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackerData> numberOfNewUsers int cumulativeNumberOfNewUsers int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfNewUsers int cumulativeNumberOfNewUsers Visualisation Output Information : NewUsersHistoricMetricProvider : id bugs.cumulativeNewUsers id bugs.newUsers Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.newusers"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsopentime","text":"Short name : historic.bugs.opentime Friendly name : Average duration to close an open bug This metric computes the average duration between creating and closing bugs. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : OpenTimeHistoricMetricProvider which contains: Variable Type avgBugOpenTime String avgBugOpenTimeInDays double bugsConsidered int Visualisation Output Information : OpenTimeHistoricMetricProvider : id bugs.bugOpenTime id bugs.bugOpenTime-bugs Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.opentime"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugspatches","text":"Short name : historic.bugs.patches Friendly name : Number of bug patches per day This class computes the number of bug patches per day, for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.patches Returns : PatchesHistoricMetricProvider which contains: Variable Type numberOfPatches int cumulativeNumberOfPatches int bugs List<DailyBugData> Additional Information : DailyBugData : String bugTrackerId int numberOfPatches int cumulativeNumberOfPatches Visualisation Output Information : PatchesHistoricMetricProvider : id bugs.cumulativePatches id bugs.patches Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.patches"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsrequestsreplies","text":"Short name : historic.bugs.requestsreplies Friendly name : Number of request and replies in bug comments per bug tracker This metric computes the number of requests and replies realting to comments posted to bugs by the community (users) per day for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsRequestsRepliesHistoricMetric which contains: Variable Type Bugs List<DailyBugTrackerData> numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies Visualisation Output Information : RequestsRepliesHistoricMetricProvider : id bugs.replies id bugs.cumulativereplies id bugs.requests id bugs.cumulativerequests id bugs.requestsreplies id bugs.cumulativerequestsreplies Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.requestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsrequestsrepliesaverage","text":"Short name : historic.bugs.requestsreplies.average Friendly name : Average number of requests and replies in bug comments per bug tracker This metric computes the average number of bug comments considered as request and reply for each bug tracker per day. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsRequestsRepliesHistoricMetric which contains: Variable Type bugs List<DailyBugTrackerData> numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies Visualisation Output Information : RequestsRepliesHistoricMetricProvider : id bugs.requests-averageperday id bugs.requestsreplies-averageperday id bugs.comments-averageperday id bugs.replies-averageperday Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.requestsreplies.average"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsresponsetime","text":"Short name : historic.bugs.responsetime Friendly name : Average response time to open bugs per bug tracker This metric computes the average time in which the community (users) responds to open bugs per day for each bug tracker seperately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsResponseTimeHistoricMetric which contains: Variable Type bugTrackerId String avgResponseTimeFormatted String cumulativeAvgResponseTimeFormatted String avgResponseTime float cumulativeAvgResponseTime float bugsConsidered int cumulativeBugsConsidered int Visualisation Output Information : ResponseTimeHistoricMetricProvider : id bugs.averageResponseTime id bugs.cumulativeAverageResponseTime id bugs.cumulativeAverageResponseTime-bugs id bugs.averageResponseTime-bugs Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.responsetime"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugssentiment","text":"Short name : historic.bugs.sentiment Friendly name : Overall sentiment per bug tracker This metric computes the overall sentiment per bug tracker up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score for each bug contributes equally, regardless of it's size. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSentimentHistoricMetric which contains: Variable Type overallAverageSentiment float overallSentimentAtThreadBeggining float overallSentimentAtThreadEnd float Visualisation Output Information : SentimentHistoricMetricProvider : id bugs.averageSentiment id bugs.sentimentAtThreadEnd id bugs.sentimentAtThreadBeggining id bugs.sentiment The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.sentiment"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseverity","text":"Short name : historic.bugs.severity Friendly name : Number of bugs per severity level per bug tracker This metric computes the number of severity levels for bugs submitted by the community (users) every day for each bug tracker. Specifically, it calculates the number and percentage of bugs that have been categorised into 1 of 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification Returns : BugsSeveritiesHistoricMetric which contains: Variable Type bugData List<BugData> severityLevels List<ServerityLevel> Additional Information : BugData : String bugTrackerId int numberOfBugs SeverityLevel : String bugTrackerId String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs int percentage Visualisation Output Information : SeverityHistoricMetricProvider : id bugs.severity id bugs.severity.percentages Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.severity"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseveritybugstatus","text":"Short name : historic.bugs.severitybugstatus Friendly name : Number of each bug status per bug severity level This metric computes the total number and percentage of each bug status per severity level, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate) and 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs int numberOfWontFixBugs int numberOfWorksForMeBugs int numberOfNonResolvedClosedBugs int numberOfInvalidBugs int numberOfFixedBugs int numberOfDuplicateBugs float percentageOfResolvedClosedBugs float percentageOfWontFixBugs float percentageOfWorksForMeBugs float percentageOfNonResolvedClosedBugs float percentageOfInvalidBugs float percentageOfFixedBugs float percentageOfDuplicateBugs Visualisation Output : SeverityBugStatusHistoricMetricProvider : id bugs.severity.duplicateBugs id bugs.severity.duplicateBugs.percentages id bugs.severity.fixedBugs id bugs.severity.fixedBugs.percentages id bugs.severity.invalidBugs id bugs.severity.invalidBugs.percentages id bugs.severity.nonResolvedClosedBugs id bugs.severity.nonResolvedClosedBugs.percentages id bugs.severity.resolvedClosedBugs id bugs.severity.resolvedClosedBugs.percentages id bugs.severity.wontFixBugs id bugs.severity.wontFixBugs.percentages id bugs.severity.worksForMeBugs id bugs.severity.worksForMeBugs.percentages Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.severitybugstatus"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseverityresponsetime","text":"Short name : historic.bugs.severityresponsetime Friendly name : Average response time to bugs per severity level per day This metric computes the average time in which the community (users) responds to open bugs per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) String avgResponseTimeFormatted int numberOfBugs long avgResponseTime Visualisation Output Information : SeverityResponseTimeHistoricMetricProvider : id bugs.severity.averageResponseTime Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.severityresponsetime"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseveritysentiment","text":"Short name : historic.bugs.severitysentiment Friendly name : Average sentiment per bugs severity level per day This metric computes for each bug severity level, the average sentiment, sentiment at the begining and end of bug comments posted by the community (users) every day for each bug tracker. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). There are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs float averageSentiment float sentimentAtThreadBeggining float sentimentAtThreadEnd Visualisation Output Information : SeveritySentimentHistoricMetricProvider : id bugs.severity.sentiment id bugs.severity.averageSentiment id bugs.severity.sentimentAtThreadBeggining id bugs.severity.sentimentAtThreadEnd The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.severitysentiment"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsstatus","text":"Short name : historic.bugs.status Friendly name : Number of bugs per bug status per day This metric computes the total number of bugs that corresponds to each bug status, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate). Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsStatusHistoricMetric which contains: Variable Type numberOfBugs long numberOfResolvedClosedBugs int numberOfWontFixBugs int numberOfWorksForMeBugs int numberOfNonResolvedClosedBugs int numberOfInvalidBugs int numberOfFixedBugs int numberOfDuplicateBugs int Visualisation Output Information : StatusHistoricMetricProvider : id bugs.duplicateBugs id bugs.fixedBugs id bugs.invalidBugs id bugs.nonResolvedClosedBugs id bugs.wontFixBugs id bugs.worksForMeBugs id bugs.resolvedClosedBugs Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.status"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugstopics","text":"Short name : historic.bugs.topics Friendly name : Labels of topic clusters in bug comments per bug tracker This metric computes the labels of topic clusters extracted from bug comments submitted by the community (users), per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.topics Returns : BugsTopicsHistoricMetric which contains: Variable Type bugTopics List<BugTopic> Additional Information : SeverityLevel : String bugTrackerId List<String> labels float numberOfDocuments List<String> commentsId Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.topics"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsunansweredbugs","text":"Short name : historic.bugs.unansweredbugs Friendly name : Number of unanswered bugs per day This metric computes the number of unanswered bugs per day. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsUnansweredBugsHistoricMetric which contains: Variable Type numberOfUnansweredBugs int Visualisation Output Information : UnansweredThreadsHistoricMetricProvider : id bugs.unansweredBugs Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.unansweredbugs"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsusers","text":"Short name : historic.bugs.users Friendly name : Number of users, active and inactive per day per bug tracker This metric computes the number of users, number of active and inactive users per day for each bug tracker separately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsUsersHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackingData> numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers int Additional Information : DailyBugTrackingData : String bugTrackerId int numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers Visualisation Output Information : UsersHistoricMetricProvider : id bugs.users id bugs.activeusers id bugs.inactiveusers Back to top","title":"org.eclipse.scava.metricprovider.historic.bugs.users"},{"location":"user-guide/metrics/#historic-metric-providers-for-newsgroups-and-forums","text":"The following Historic Metric Providers are associated with newsgroups. Back to top","title":"Historic Metric Providers for Newsgroups and Forums"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsarticles","text":"Short name : historic.newsgroups.articles Friendly name : Number of articles per day per news group This metric computes the number of articles submitted by the community (users) per day for each newsgroup separately Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.articles Returns : NewsgroupsArticlesHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfArticles int cummulativeNumberOfArticles Visualisation Output Information : ArticlesHistoricMetricProvider : id newsgroups.articles id newsgroups.cumulativeArticles Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.articles"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsemotions","text":"Short name : historic.newsgroups.emotions Friendly name : Number of emotions per day per newsgroup This metric computes the emotional dimensions present in newsgroup comments submitted by the community (users) per day for each newsgroup. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise). Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.emotions Returns : NewsgroupsEmotionsHistoricMetric which contains: Variable Type newsgroupsData List<Newsgroups> emotionDimension List<Emotion> Additional Information : NewsgroupsData : String newsgroupName int numberOfArticles int cummulativeNumberOfArticles EmotionDimension : String newsgroupName String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfArticles int cumulativeNumberOfArticles float percentage float cumulativePercentage Visualisation Output Information : EmotionsHistoricMetricProvider : id newsgroups.emotions.articlePercentages id newsgroups.emotions.cumulativeArticles id newsgroups.emotions.cumulativeArticlePercentages id newsgroups.emotions.articles Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.emotions"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsmigrationissues","text":"Short name : historic.newsgroups.migrationissues Friendly name : Migration Issues Detection in articles per day per newsgroup This metric detects migration issues in articles per day for each newgroup. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.migrationissues Returns : NewsgroupsMigrationIssueHistoricMetric which contains: Variable Type dailyNewsgroupsMigrationData List<DailyNewsgroupsMigrationData> Additional Information : DailyBugTrackerMigrationData : String newsgroupName List<Integer> threadsId int numberOfIssues Visualisation Output Information : NewsgroupsMigrationIssueHistoricMetricProvider : id newsgroups.dailymigrationissues Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.migrationissues"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsmigrationissuesmaracas","text":"Short name : historic.newsgroups.migrationissuesmaracas Friendly name : Migration Issues Detection along with Maracas in articles per day per newsgroup This metric stores how many migration issues have been found containing changes detected with MARACAS per day for each newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.migrationissuesmaracas Returns : NewsgroupMigrationIssueMaracasHistoricMetric which contains: Variable Type dailyNewsgroupMigrationMaracasData List<DailyNewsgroupMigrationMaracasData> newsgroupMigrationMaracasData List<NewsgroupMigrationMaracasData> Additional Information : DailyNewsgroupMigrationMaracasData : String newsgroupName List<Integer> threadsId; int numberOfIssues NewsgroupMigrationMaracasData : String newsgroupName int threadId; List<String> changesAndMatchingPercentage Visualisation Output Information : NewsgroupMigrationIssueMaracasHistoricMetricProvider : id newsgroups.dailymigrationissuesmaracas id newsgroups.migrationissuesmaracas.changes Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.migrationissuesmaracas"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsnewthreads","text":"Short name : historic.newsgroups.newthreads Friendly name : Number of new threads per day per newsgroup This metric computes the number of new threads submitted by the community (users) per day for each newsgroup separately Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : NewsgroupsNewThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfNewThreads int cummulativeNumberOfNewThreads Visualisation Output Information : NewThreadsHistoricMetricProvider : id newsgroups.newThreads id newsgroups.cumulativeNewThreads Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.newthreads"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsnewusers","text":"Short name : historic.newsgroups.newusers Friendly name : Number of new users per day per newsgroup This metric computes the number of new users per day for each newsgroup seperately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsNewUsersHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfNewUsers int cummulativeNumberOfNewUsers Visualisation Output Information : NewUsersHistoricMetricProvider: id newsgroups.cumulativeNewUsers id newsgroups.newUsers Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.newusers"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsrequestsreplies","text":"Short name : historic.newsgroups.requestsreplies Friendly name : Number of requests and replies in articles per day This metric computes the number of requests and replies in newsgroup articles submitted by the community (users) per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsRequestsRepliesHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies Visualisation Output Information : RequestsRepliesHistoricMetricProvider : id newsgroups.requests id newsgroups.cumulativerequests id newsgroups.replies id newsgroups.cumulativereplies id newsgroups.requestsreplies id newsgroups.cumulativerequestsreplies Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsrequestsrepliesaverage","text":"Short name : historic.newsgroups.requestsreplies.average Friendly name : Average number of articles, requests and replies per day This metric computes the average number of newsgroup articles, including the number of requests and replies within the newsgroup articles per day. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsRequestsRepliesAverageHistoricMetric which contains: Variable Type averageArticlesPerDay float averageRequestsPerDay float averageRepliesPerDay float Visualisation Output Information : RequestsRepliesAverageHistoricMetricProvider : id newsgroups.requestsreplies-averageperday id newsgroups.requests-averageperday id newsgroups.replies-averageperday id newsgroups.comments-averageperday Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies.average"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsresponsetime","text":"Short name : historic.newsgroups.responsetime Friendly name : Average response time to threads per day per newsgroup This metric computes the average time in which the community responds to open threads per day for each newsgroup separately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsResponseTimeHistoricMetric which contains: Variable Type newsgroupName String avgResponseTime long avgResponseTimeFormatted String threadsConsidered int cumulativeAvgResponseTimeFormatted String cumulativeThreadsConsidered int Visualisation Output Information : ResponseTimeHistoricMetricProvider : id newsgroups.averageResponseTime id newsgroups.cumulativeAverageResponseTime id newsgroups.cumulativeAverageResponseTime-threads","title":"org.eclipse.scava.metricprovider.historic.newsgroups.responsetime"},{"location":"user-guide/metrics/#id-newsgroupsaverageresponsetime-threads","text":"Back to top","title":"id  newsgroups.averageResponseTime-threads"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupssentiment","text":"Short name : historic.newsgroups.sentiment Friendly name : Overall sentiment of newsgroup articles This metric computes the overall sentiment per newsgroup repository up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score of each thread contributes equally, irrespective of its size. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.sentiment Returns : NewsgroupsSentimentHistoricMetric which contains: Variable Type overallAverageSentiment float overallSentimentAtThreadBegining float overallSentimentAtThreadEnd float Visualisation Output Information : SentimentHistoricMetricProvider : id newsgroups.averageSentiment id newsgroups.sentimentAtThreadEnd id newsgroups.sentimentAtThreadBeggining id newsgroups.sentiment The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.sentiment"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsseverity","text":"Short name : historic.newsgroups.severity Friendly name : Number of each severity level in newsgroup threads per day This metric computes the number of each severity levels in threads submitted every day, per newsgroup. There are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification Returns : NewsgroupsSeveritiesHistoricMetric which contains: Variable Type newsgroupData List<Newsgroups> severityLevel List<SeverityLevel> Additional Information : NewsgroupData : String newsgroupName int numberThreads SeverityLevel : String newsgroupName String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads float percentage Visualisation Output Information : SeverityHistoricMetricProvider : id newsgroups.severity id newsgroups.severity.percentages Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.severity"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsseverityresponsetime","text":"Short name : historic.newsgroups.severityresponsetime Friendly name : Average response time to threads per severity level per day This metric computes the average time in which the community (users) responds to open threads per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Average response time to threads per severity level This metric computes the average response time for newsgroup threads submitted every day, based on their severity levels. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsSeverityResponseTimeHistoricMetric which contains: Variable Type severityLevel List<SeverityLevel> Additional Information : SeverityLevel : String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads long avgResponseTime String avgResponseTimeFormatted Visualisation Output Information : SeverityResponseTimeHistoricMetricProvider : id newsgroups.severity.averageResponseTime Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.severityresponsetime"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsseveritysentiment","text":"Short name : historic.newsgroups.severitysentiment Friendly name : Average sentiment in threads per severity level per day This metric computes the average sentiment, the sentiment at the beginning of threads and the sentiment at the end of threads; for each severity level in newsgroup threads submitted every day. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). Note: there are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.newsgroups.sentiment Returns : NewsgroupsSeveritySentimentHistoricMetric which contains: Variable Type severityLevel List<SeverityLevel> Additional Information : SeverityLevel : String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads float avgSentiment float avgSentimentThreadBeginning float avgSentimentThreadEnd Visualisation Output Information : SeveritySentimentHistoricMetricProvider : id newsgroups.severity.averageSentiment id newsgroups.severity.sentiment id newsgroups.severity.sentimentAtThreadEnd id newsgroups.severity.sentimentAtThreadBeggining The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.severitysentiment"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsthreads","text":"Short name : historic.newsgroups.threads Friendly name : Number of threads per day per newsgroup This metric computes the number of threads per day for each newsgroup separately. The metric also computes average values for articles per thread, requests per thread, replies per thread, articles per user, requests per user and replies per user. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfThreads float averageArticlesPerThread float averageRequestsPerThread float averageRepliesPerThread float averageArticlesPerUser float averageRequestsPerUser float averageRepliesPerUser Visualisation Output Information : ThreadsHistoricMetricProvider : id newsgroups.threads id newsgroups.articles-threadaverage id newsgroups.articles-useraverage id newsgroups.requests-threadaverage id newsgroups.requests-useraverage id newsgroups.replies-threadaverage id newsgroups.replies-useraverage id newsgroups.requestsreplies-threadaverage id newsgroups.requestsreplies-useraverage Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.threads"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupstopics","text":"Short name : historic.newsgroups.topics Friendly name : Labels of newsgroup topics per newsgroup This metric computes the labels of topics clusters in articles submitted by the community (users), for each newsgroup seperately. Depends-on : org.eclipse.scava.metricprovider.trans.topics Returns : NewsgroupTopicsHistoricMetric which contains: Variable Type newsgroupTopic List<NewsgrpTopic> Additional Information : NewsgroupTopic : String newsgroupName List<String> labels int numberOfDocuments List<Integer> articlesId Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.topics"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsunansweredthreads","text":"Short name : historic.newsgroups.unansweredthreads Friendly name : Number of unanswered threads per day per newsgroup This metric computes the number of unanswered threads per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsUnansweredThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfUnansweredThreads Visualisation Output Information : UnansweredThreadsHistoricMetricProvider : id newsgroups.unansweredThreads Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.unansweredthreads"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsusers","text":"Short name : historic.newsgroups.users Friendly name : Number of users, active and inactive per day per newsgroup This metric computes the number of users, including active and inactive users per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsUsersHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers Visualisation Output Information : UsersHistoricMetricProvider : id newsgroups.users id newsgroups.activeusers id newsgroups.inactiveusers id newsgroups.activeinactiveusers Back to top","title":"org.eclipse.scava.metricprovider.historic.newsgroups.users"},{"location":"user-guide/metrics/#historic-metric-providers-for-commits-and-committers","text":"The following Historic Metric Providers are related to the commits and committers of a project. Back to top","title":"Historic Metric Providers for Commits and Committers"},{"location":"user-guide/metrics/#transrascalactivecommitterscommittersoverfilehistoric","text":"Short name : giniCommittersOverFile.historic Friendly name : Historic giniCommittersOverFile Historic version of : trans.rascal.activecommitters.committersoverfile Calculates the gini coefficient of committers per file Depends-on : trans.rascal.activecommitters.committersoverfile Returns : real Back to top","title":"trans.rascal.activecommitters.committersoverfile.historic"},{"location":"user-guide/metrics/#transrascalactivecommitterspercentageofweekendcommitshistoric","text":"Short name : percentageOfWeekendCommits.historic Friendly name : Historic percentageOfWeekendCommits Historic version of : trans.rascal.activecommitters.percentageOfWeekendCommits Percentage of commits made during the weekend Depends-on : trans.rascal.activecommitters.percentageOfWeekendCommits Returns : int Back to top","title":"trans.rascal.activecommitters.percentageOfWeekendCommits.historic"},{"location":"user-guide/metrics/#transrascalactivecommitterscommitsperdeveloperhistoric","text":"Short name : commitsPerDeveloper.historic Friendly name : Historic commitsPerDeveloper Historic version of : trans.rascal.activecommitters.commitsPerDeveloper The number of commits per developer indicates not only the volume of the contribution of an individual but also the style in which he or she commits, when combined with other metrics such as churn. Few and big commits are different from many small commits. This metric is used downstream by other metrics as well. Depends-on : trans.rascal.activecommitters.commitsPerDeveloper Returns : map[str, int] Back to top","title":"trans.rascal.activecommitters.commitsPerDeveloper.historic"},{"location":"user-guide/metrics/#transrascalactivecommittersnumberofactivecommitterslongtermhistoric","text":"Short name : numberOfActiveCommittersLongTerm.historic Friendly name : Historic numberOfActiveCommittersLongTerm Historic version of : trans.rascal.activecommitters.numberOfActiveCommittersLongTerm Number of long time active committers over time (active in last year). This measures a smooth window of one year, where every day we report the number of developers active in the previous 365 days. Depends-on : trans.rascal.activecommitters.numberOfActiveCommittersLongTerm Returns : int Back to top","title":"trans.rascal.activecommitters.numberOfActiveCommittersLongTerm.historic"},{"location":"user-guide/metrics/#transrascalactivecommittersnumberofactivecommittershistoric","text":"Short name : numberOfActiveCommitters.historic Friendly name : Historic numberOfActiveCommitters Historic version of : trans.rascal.activecommitters.numberOfActiveCommitters Number of active committers over time (active in last two weeks). This measures a smooth window of two weeks, where every day we report the number of developers in the previous 14 days. Depends-on : trans.rascal.activecommitters.numberOfActiveCommitters Returns : int Back to top","title":"trans.rascal.activecommitters.numberOfActiveCommitters.historic"},{"location":"user-guide/metrics/#rascalgenericchurncommitstodayhistoric","text":"Short name : commitsToday.historic Friendly name : Historic commitsToday Historic version of : rascal.generic.churn.commitsToday Counts the number of commits made today. Depends-on : rascal.generic.churn.commitsToday Returns : int Back to top","title":"rascal.generic.churn.commitsToday.historic"},{"location":"user-guide/metrics/#rascalgenericchurnchurntodayhistoric","text":"Short name : commitsToday.historic Friendly name : Historic commitsToday Historic version of : rascal.generic.churn.churnToday Counts the churn for today: the total number of lines of code added and deleted. This metric is used further downstream to analyze trends. Depends-on : rascal.generic.churn.churnToday Returns : int Back to top","title":"rascal.generic.churn.churnToday.historic"},{"location":"user-guide/metrics/#rascalgenericchurnchurnpercommitintwoweekshistoric","text":"Short name : churnPerCommitInTwoWeeks.historic Friendly name : Historic churnPerCommitInTwoWeeks Historic version of : rascal.generic.churn.churnPerCommitInTwoWeeks The ratio between the churn and the number of commits indicates how large each commit is on average. We compute this as a sliding average over two weeks which smoothens exceptions and makes it possible to see a trend historically. Commits should not be to big all the time, because that would indicate either that programmers are not focusing on well-defined tasks or that the system architecture does not allow for separation of concerns. Depends-on : rascal.generic.churn.churnPerCommitInTwoWeeks Returns : int Back to top","title":"rascal.generic.churn.churnPerCommitInTwoWeeks.historic"},{"location":"user-guide/metrics/#rascalgenericchurnfilespercommithistoric","text":"Short name : numberOfFilesPerCommit.historic Friendly name : Historic numberOfFilesPerCommit Historic version of : rascal.generic.churn.filesPerCommit Counts the number of files per commit to find out about the separation of concerns in the architecture or in the tasks the programmers perform. This metric is used further downstream. Depends-on : rascal.generic.churn.filesPerCommit Returns : map[loc, int] Back to top","title":"rascal.generic.churn.filesPerCommit.historic"},{"location":"user-guide/metrics/#rascalgenericchurnchurnpercommithistoric","text":"Short name : churnPerCommit.historic Friendly name : Historic churnPerCommit Historic version of : rascal.generic.churn.churnPerCommit Count churn. Churn is the number lines added or deleted. We measure this per commit because the commit is a basic unit of work for a programmer. This metric computes a table per commit for today and is not used for comparison between projects. It is used further downstream to analyze activity. Depends-on : rascal.generic.churn.churnPerCommit Returns : map[loc, int] Back to top","title":"rascal.generic.churn.churnPerCommit.historic"},{"location":"user-guide/metrics/#rascalgenericchurnchurnpercommitterhistoric","text":"Short name : churnPerCommitter.historic Friendly name : Historic churnPerCommitter Historic version of : rascal.generic.churn.churnPerCommitter Count churn per committer: the number of lines of code added and deleted. It zooms in on the single committer producing a table which can be used for downstream processing. Depends-on : rascal.generic.churn.churnPerCommitter Returns : map[str author, int churn] Back to top","title":"rascal.generic.churn.churnPerCommitter.historic"},{"location":"user-guide/metrics/#rascalgenericchurncommitsintwoweekshistoric","text":"Short name : commitsInTwoWeeks.historic Friendly name : Historic commitsInTwoWeeks Historic version of : rascal.generic.churn.commitsInTwoWeeks Churn in the last two weeks: aggregates the number of commits over a 14-day sliding window. Depends-on : rascal.generic.churn.commitsInTwoWeeks Returns : int Back to top","title":"rascal.generic.churn.commitsInTwoWeeks.historic"},{"location":"user-guide/metrics/#rascalgenericchurnchurnintwoweekshistoric","text":"Short name : churnInTwoWeeks.historic Friendly name : Historic churnInTwoWeeks Historic version of : rascal.generic.churn.churnInTwoWeeks Churn in the last two weeks: aggregates the lines of code added and deleted over a 14-day sliding window. Depends-on : rascal.generic.churn.churnInTwoWeeks Returns : int Back to top","title":"rascal.generic.churn.churnInTwoWeeks.historic"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoriccommitsmessagestopics","text":"Short name : historic.commits.messages.topics Friendly name : Labels of topics in commits messages analyzed in the last 30 days This metric computes the labels of topic clusters in commits messages pushed by users in the last 30 days Depends-on : org.eclipse.scava.metricprovider.trans.commits.message.topics Returns : CommitsMessagesTopicsHistoricMetric which contains: Variable Type commitMessageTopics List<CommitMessageTopic> Additional Information : CommitMessageTopic : String repository String labels int numberOfMessages List<String> commitsMessageId Visualisation Output Information : CommitsMessagesTopicsHistoricMetricProvider : id commits.topics.messages Back to top","title":"org.eclipse.scava.metricprovider.historic.commits.messages.topics"},{"location":"user-guide/metrics/#historic-metric-providers-for-documentation","text":"The following Historic Metric Providers are associated with documentation analyses. Back to top","title":"Historic Metric Providers for Documentation"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricdocumentationreadability","text":"Short name : historic.documentation.readability Friendly name : Documentation readability Historic Metric Historic metric that stores the evolution of the documentation readability. The higher the readability score, the harder to understand the text. Depends-on : org.eclipse.scava.metricprovider.trans.documentation.readability Returns : DocumentationReadabilityHistoricMetric which contains: Variable Type documentationReadability List<DocumentationHistoricReadability> documentationEntriesReadability List<DocumentationEntryHistoricReadability> Additional Information : DocumentationHistoricReadability : String documentationId int numberOfDocumentationEntries double averageDocumentationReadability DocumentationEntryHistoricReadability : String documentationId String entryId double readability Visualisation Output Information : readability : id documentation.readability.entries id documentation.readability Back to top","title":"org.eclipse.scava.metricprovider.historic.documentation.readability"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricdocumentationsentiment","text":"Short name : historic.documentation.sentiment Friendly name : Documentation sentiment polarity Historic Metric Historic metric that stores the evolution of the documentation sentiment polarity. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment) Depends-on : org.eclipse.scava.metricprovider.trans.documentation.sentiment Returns : DocumentationSentimentHistoricMetric which contains: Variable Type documentationSentiment List<DocumentationHistoricSentiment> documentationEntriesSentiment List<DocumentationEntryHistoricSentiment> Additional Information : DocumentationHistoricSentiment : String documentationId int numberOfDocumentationEntries double averageDocumentationSentiment DocumentationEntryHistoricSentiment : String documentationId String entryId String polarity Visualisation Output Information : sentiment : id documentation.sentiment.entries id documentation.sentiment Back to top","title":"org.eclipse.scava.metricprovider.historic.documentation.sentiment"},{"location":"user-guide/metrics/#historic-metric-providers-for-generic-source-code","text":"These metrics are related to the source code of analyzed projects, regardless of the language(s) they are written in. Back to top","title":"Historic Metric Providers for Generic Source Code"},{"location":"user-guide/metrics/#transrascalclonesclonelocperlanguagehistoric","text":"Short name : cloneLOCPerLanguage.historic Friendly name : Historic cloneLOCPerLanguage Historic version of : trans.rascal.clones.cloneLOCPerLanguage Lines of code in Type I clones larger than 6 lines, per language. A Type I clone is a literal clone. A large number of literal clones is considered to be bad. This metric is not easily compared between systems because it is not size normalized yet. We use it for further processing downstream. You can analyze the trend over time using this metric. Depends-on : trans.rascal.clones.cloneLOCPerLanguage Returns : map[str, int] Back to top","title":"trans.rascal.clones.cloneLOCPerLanguage.historic"},{"location":"user-guide/metrics/#transrascalreadabilityfilereadabilityquartileshistoric","text":"Short name : fileReadabilityQ.historic Friendly name : Historic fileReadabilityQ Historic version of : trans.rascal.readability.fileReadabilityQuartiles We measure file readability by counting exceptions to common usage of whitespace in source code, such as spaces after commas. The quartiles represent how many of the files have how many of these deviations. A few deviations per file is ok, but many files with many deviations indicates a lack of attention to readability. Depends-on : trans.rascal.readability.fileReadabilityQuartiles Returns : map[str, real] Back to top","title":"trans.rascal.readability.fileReadabilityQuartiles.historic"},{"location":"user-guide/metrics/#transrascalcommentscommentlinesperlanguagehistoric","text":"Short name : commentLinesPerLanguage.historic Friendly name : Historic commentLinesPerLanguage Historic version of : trans.rascal.comments.commentLinesPerLanguage Number of lines containing comments per language (excluding headers). The balance between comments and code indicates understandability. Too many comments are often not maintained and may lead to confusion, not enough means the code lacks documentation explaining its intent. This is a basic fact collection metric which is used further downstream. Depends-on : trans.rascal.comments.commentLinesPerLanguage Returns : map[str, int] Back to top","title":"trans.rascal.comments.commentLinesPerLanguage.historic"},{"location":"user-guide/metrics/#transrascalcommentscommentedoutcodeperlanguagehistoric","text":"Short name : commentedOutCodePerLanguage.historic Friendly name : Historic commentedOutCodePerLanguage Historic version of : trans.rascal.comments.commentedOutCodePerLanguage Lines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how much source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator. Depends-on : trans.rascal.comments.commentedOutCodePerLanguage Returns : map[str, int] Back to top","title":"trans.rascal.comments.commentedOutCodePerLanguage.historic"},{"location":"user-guide/metrics/#transrascalcommentsheaderpercentagehistoric","text":"Short name : headerPercentage.historic Friendly name : Historic headerPercentage Historic version of : trans.rascal.comments.headerPercentage Percentage of files with headers is an indicator for the amount of files which have been tagged with a copyright statement (or not). If the number is low this indicates a problem with the copyright of the program. Source files without a copyright statement are not open-source, they are owned, in principle, by the author and may not be copied without permission. Note that the existence of a header does not guarantee the presence of an open-source license, but its absence certainly is telling. Depends-on : trans.rascal.comments.headerPercentage Returns : real Back to top","title":"trans.rascal.comments.headerPercentage.historic"},{"location":"user-guide/metrics/#transrascallocgenericlocoverfileshistoric","text":"Short name : giniLOCOverFiles.historic Friendly name : Historic giniLOCOverFiles Historic version of : trans.rascal.LOC.genericLOCoverFiles We find out how evenly the code is spread over files. The number should be quite stable over time. A jump in this metric indicates a large change in the code base. If the code is focused in only a few very large files then this may be a contra-indicator for quality. Depends-on : trans.rascal.LOC.genericLOCoverFiles Returns : real Back to top","title":"trans.rascal.LOC.genericLOCoverFiles.historic"},{"location":"user-guide/metrics/#transrascalloclocperlanguagehistoric","text":"Short name : locPerLanguage.historic Friendly name : Historic locPerLanguage Historic version of : trans.rascal.LOC.locPerLanguage Physical lines of code simply counts the number of newline characters (OS independent) in a source code file. We accumulate this number per programming language. The metric can be used to compare the volume between two systems and to assess in which programming language the bulk of the code is written. Depends-on : trans.rascal.LOC.locPerLanguage Returns : map[str, int] Back to top","title":"trans.rascal.LOC.locPerLanguage.historic"},{"location":"user-guide/metrics/#historic-metric-providers-for-java-code","text":"These metrics are related to the Java source code of analyzed projects. Back to top","title":"Historic Metric Providers for Java code"},{"location":"user-guide/metrics/#stylefileswitherrorpronenesshistoric","text":"Short name : filesWithErrorProneness.historic Friendly name : Historic filesWithErrorProneness Historic version of : style.filesWithErrorProneness Percentage of files with error proneness Depends-on : style.filesWithErrorProneness Returns : int Back to top","title":"style.filesWithErrorProneness.historic"},{"location":"user-guide/metrics/#stylefileswithunderstandabilityissueshistoric","text":"Short name : filesWithUnderstandabilityIssues.historic Friendly name : Historic filesWithUnderstandabilityIssues Historic version of : style.filesWithUnderstandabilityIssues Percentage of files with understandability issues. This is a basic metric which can not be easily compared between projects. Depends-on : style.filesWithUnderstandabilityIssues Returns : int Back to top","title":"style.filesWithUnderstandabilityIssues.historic"},{"location":"user-guide/metrics/#stylespreadofstyleviolationshistoric","text":"Short name : spreadOfStyleViolations.historic Friendly name : Historic spreadOfStyleViolations Historic version of : style.spreadOfStyleViolations Between 0 and 1 how evenly spread are the style violations. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.spreadOfStyleViolations Returns : real Back to top","title":"style.spreadOfStyleViolations.historic"},{"location":"user-guide/metrics/#stylefileswithinefficiencieshistoric","text":"Short name : filesWithInefficiencies.historic Friendly name : Historic filesWithInefficiencies Historic version of : style.filesWithInefficiencies Percentage of files with inefficiencies Depends-on : style.filesWithInefficiencies Returns : int Back to top","title":"style.filesWithInefficiencies.historic"},{"location":"user-guide/metrics/#stylefileswithstyleviolationshistoric","text":"Short name : filesWithStyleViolations.historic Friendly name : Historic filesWithStyleViolations Historic version of : style.filesWithStyleViolations Percentage of files with style violations Depends-on : style.filesWithStyleViolations Returns : int Back to top","title":"style.filesWithStyleViolations.historic"},{"location":"user-guide/metrics/#stylespreadofunderstandabilityissueshistoric","text":"Short name : spreadOfUnderstandabilityIssues.historic Friendly name : Historic spreadOfUnderstandabilityIssues Historic version of : style.spreadOfUnderstandabilityIssues Between 0 and 1 how evenly spread are the understandability issues. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.spreadOfUnderstandabilityIssues Returns : real Back to top","title":"style.spreadOfUnderstandabilityIssues.historic"},{"location":"user-guide/metrics/#stylespreadofinefficiencieshistoric","text":"Short name : spreadOfInefficiencies.historic Friendly name : Historic spreadOfInefficiencies Historic version of : style.spreadOfInefficiencies Between 0 and 1 how evenly spread are the style violations which indicate inefficiencies. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.spreadOfInefficiencies Returns : real Back to top","title":"style.spreadOfInefficiencies.historic"},{"location":"user-guide/metrics/#stylespreadoferrorpronenesshistoric","text":"Short name : spreadOfErrorProneness.historic Friendly name : Historic spreadOfErrorProneness Historic version of : style.spreadOfErrorProneness Between 0 and 1 how evenly spread are the style violations which indicate error proneness. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.spreadOfErrorProneness Returns : real Back to top","title":"style.spreadOfErrorProneness.historic"},{"location":"user-guide/metrics/#rascaltestabilityjavatestoverpublicmethodshistoric","text":"Short name : percentageOfTestedPublicMethods.historic Friendly name : Historic percentageOfTestedPublicMethods Historic version of : rascal.testability.java.TestOverPublicMethods Number of JUnit tests averaged over the total number of public methods. Ideally all public methods are tested. With this number we compute how far from the ideal situation the project is. Depends-on : rascal.testability.java.TestOverPublicMethods Returns : real Back to top","title":"rascal.testability.java.TestOverPublicMethods.historic"},{"location":"user-guide/metrics/#rascaltestabilityjavanumberoftestmethodshistoric","text":"Short name : numberOfTestMethods.historic Friendly name : Historic numberOfTestMethods Historic version of : rascal.testability.java.NumberOfTestMethods Number of JUnit test methods Depends-on : rascal.testability.java.NumberOfTestMethods Returns : int Back to top","title":"rascal.testability.java.NumberOfTestMethods.historic"},{"location":"user-guide/metrics/#rascaltestabilityjavatestcoveragehistoric","text":"Short name : estimateTestCoverage.historic Friendly name : Historic estimateTestCoverage Historic version of : rascal.testability.java.TestCoverage This is a static over-estimation of test coverage: which code is executed in the system when all JUnit test cases are executed? We approximate this by using the static call graphs and assuming every method which can be called, will be called. This leads to an over-approximation, as compared to a dynamic code coverage analysis, but the static analysis does follow the trend and a low code coverage here is an good indicator for a lack in testing effort for the project. Depends-on : rascal.testability.java.TestCoverage Returns : real Back to top","title":"rascal.testability.java.TestCoverage.historic"},{"location":"user-guide/metrics/#transrascaloojavaca-java-quartileshistoric","text":"Short name : Ca_Java_Q.historic Friendly name : Historic Ca_Java_Q Historic version of : trans.rascal.OO.java.Ca-Java-Quartiles Afferent coupling quartiles (Java) Depends-on : trans.rascal.OO.java.Ca-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.Ca-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavacf-javahistoric","text":"Short name : CF_Java.historic Friendly name : Historic CF_Java Historic version of : trans.rascal.OO.java.CF-Java Coupling factor (Java) Depends-on : trans.rascal.OO.java.CF-Java Returns : real Back to top","title":"trans.rascal.OO.java.CF-Java.historic"},{"location":"user-guide/metrics/#transrascaloojavadac-java-quartileshistoric","text":"Short name : DAC_Java_Q.historic Friendly name : Historic DAC_Java_Q Historic version of : trans.rascal.OO.java.DAC-Java-Quartiles Data abstraction coupling quartiles (Java) Depends-on : trans.rascal.OO.java.DAC-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.DAC-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavampc-java-quartileshistoric","text":"Short name : MPC_Java_Q.historic Friendly name : Historic MPC_Java_Q Historic version of : trans.rascal.OO.java.MPC-Java-Quartiles Message passing coupling quartiles (Java) Depends-on : trans.rascal.OO.java.MPC-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.MPC-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavapf-javahistoric","text":"Short name : PF_Java.historic Friendly name : Historic PF_Java Historic version of : trans.rascal.OO.java.PF-Java Polymorphism factor (Java) Depends-on : trans.rascal.OO.java.PF-Java Returns : real Back to top","title":"trans.rascal.OO.java.PF-Java.historic"},{"location":"user-guide/metrics/#transrascaloojavarfc-java-quartileshistoric","text":"Short name : RFC_Java_Q.historic Friendly name : Historic RFC_Java_Q Historic version of : trans.rascal.OO.java.RFC-Java-Quartiles Response for class quartiles (Java) Depends-on : trans.rascal.OO.java.RFC-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.RFC-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavai-java-quartileshistoric","text":"Short name : I_Java_Q.historic Friendly name : Historic I_Java_Q Historic version of : trans.rascal.OO.java.I-Java-Quartiles Instability quartiles (Java) Depends-on : trans.rascal.OO.java.I-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.I-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavamif-java-quartileshistoric","text":"Short name : MIF_Java_Q.historic Friendly name : Historic MIF_Java_Q Historic version of : trans.rascal.OO.java.MIF-Java-Quartiles Method inheritance factor quartiles (Java) Depends-on : trans.rascal.OO.java.MIF-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.MIF-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavamhf-javahistoric","text":"Short name : MHF_Java.historic Friendly name : Historic MHF_Java Historic version of : trans.rascal.OO.java.MHF-Java Method hiding factor (Java) Depends-on : trans.rascal.OO.java.MHF-Java Returns : real Back to top","title":"trans.rascal.OO.java.MHF-Java.historic"},{"location":"user-guide/metrics/#transrascaloojavaahf-javahistoric","text":"Short name : AHF_Java.historic Friendly name : Historic AHF_Java Historic version of : trans.rascal.OO.java.AHF-Java Attribute hiding factor (Java) Depends-on : trans.rascal.OO.java.AHF-Java Returns : real Back to top","title":"trans.rascal.OO.java.AHF-Java.historic"},{"location":"user-guide/metrics/#transrascaloojavalcom-java-quartileshistoric","text":"Short name : LCOM_Java_Q.historic Friendly name : Historic LCOM_Java_Q Historic version of : trans.rascal.OO.java.LCOM-Java-Quartiles Lack of cohesion in methods quartiles (Java) Depends-on : trans.rascal.OO.java.LCOM-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.LCOM-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavaa-javahistoric","text":"Short name : A_Java.historic Friendly name : Historic A_Java Historic version of : trans.rascal.OO.java.A-Java Abstractness (Java) Depends-on : trans.rascal.OO.java.A-Java Returns : real Back to top","title":"trans.rascal.OO.java.A-Java.historic"},{"location":"user-guide/metrics/#transrascaloojavadit-java-quartileshistoric","text":"Short name : DIT_Java_Q.historic Friendly name : Historic DIT_Java_Q Historic version of : trans.rascal.OO.java.DIT-Java-Quartiles Depth of inheritance tree quartiles (Java) Depends-on : trans.rascal.OO.java.DIT-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.DIT-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavatcc-java-quartileshistoric","text":"Short name : TCC_Java_Q.historic Friendly name : Historic TCC_Java_Q Historic version of : trans.rascal.OO.java.TCC-Java-Quartiles Tight class cohesion quartiles (Java) Depends-on : trans.rascal.OO.java.TCC-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.TCC-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavalcom4-java-quartileshistoric","text":"Short name : LCOM4_Java_Q.historic Friendly name : Historic LCOM4_Java_Q Historic version of : trans.rascal.OO.java.LCOM4-Java-Quartiles Lack of cohesion in methods 4 quartiles (Java) Depends-on : trans.rascal.OO.java.LCOM4-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.LCOM4-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavasr-javahistoric","text":"Short name : SR_Java.historic Friendly name : Historic SR_Java Historic version of : trans.rascal.OO.java.SR-Java Specialization ratio (Java) Depends-on : trans.rascal.OO.java.SR-Java Returns : real Back to top","title":"trans.rascal.OO.java.SR-Java.historic"},{"location":"user-guide/metrics/#transrascaloojavaaif-java-quartileshistoric","text":"Short name : AIF_Java_Q.historic Friendly name : Historic AIF_Java_Q Historic version of : trans.rascal.OO.java.AIF-Java-Quartiles Attribute inheritance factor quartiles (Java) Depends-on : trans.rascal.OO.java.AIF-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.AIF-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavanoc-java-quartileshistoric","text":"Short name : NOC_Java_Q.historic Friendly name : Historic NOC_Java_Q Historic version of : trans.rascal.OO.java.NOC-Java-Quartiles Number of children quartiles (Java) Depends-on : trans.rascal.OO.java.NOC-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.NOC-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavarr-javahistoric","text":"Short name : RR_Java.historic Friendly name : Historic RR_Java Historic version of : trans.rascal.OO.java.RR-Java Reuse ratio (Java) Depends-on : trans.rascal.OO.java.RR-Java Returns : real Back to top","title":"trans.rascal.OO.java.RR-Java.historic"},{"location":"user-guide/metrics/#transrascaloojavalcc-java-quartileshistoric","text":"Short name : LCC_Java_Q.historic Friendly name : Historic LCC_Java_Q Historic version of : trans.rascal.OO.java.LCC-Java-Quartiles Loose class cohesion quartiles (Java) Depends-on : trans.rascal.OO.java.LCC-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.LCC-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavace-java-quartileshistoric","text":"Short name : Ce_Java_Q.historic Friendly name : Historic Ce_Java_Q Historic version of : trans.rascal.OO.java.Ce-Java-Quartiles Efferent coupling quartiles (Java) Depends-on : trans.rascal.OO.java.Ce-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.Ce-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavanom-java-quartileshistoric","text":"Short name : NOM_Java_Q.historic Friendly name : Historic NOM_Java_Q Historic version of : trans.rascal.OO.java.NOM-Java-Quartiles Number of methods quartiles (Java) Depends-on : trans.rascal.OO.java.NOM-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.NOM-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavanoa-java-quartileshistoric","text":"Short name : NOA_Java_Q.historic Friendly name : Historic NOA_Java_Q Historic version of : trans.rascal.OO.java.NOA-Java-Quartiles Number of attributes quartiles (Java) Depends-on : trans.rascal.OO.java.NOA-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.NOA-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaloojavacbo-java-quartileshistoric","text":"Short name : CBO_Java_Q.historic Friendly name : Historic CBO_Java_Q Historic version of : trans.rascal.OO.java.CBO-Java-Quartiles Coupling between objects quartiles (Java) Depends-on : trans.rascal.OO.java.CBO-Java-Quartiles Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.CBO-Java-Quartiles.historic"},{"location":"user-guide/metrics/#transrascaladvancedfeaturesjavaadvancedlanguagefeaturesjavaquartileshistoric","text":"Short name : countUsesOfAdvancedLanguageFeaturesQ.historic Friendly name : Historic countUsesOfAdvancedLanguageFeaturesQ Historic version of : trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles Quartiles of counts of advanced Java features (wildcards, union types and anonymous classes). The numbers indicate the thresholds that delimit the first 25%, 50% and 75% of the data as well as the maximum and minumum values. Depends-on : trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles Returns : map[str, real] Back to top","title":"trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles.historic"},{"location":"user-guide/metrics/#transrascalccjavacchistogramjavahistoric","text":"Short name : CCHistogramJava.historic Friendly name : Historic CCHistogramJava Historic version of : trans.rascal.CC.java.CCHistogramJava Number of Java methods per CC risk factor, counts the number of methods which are in a low, medium or high risk factor. The histogram can be compared between projects to indicate which is probably easier to maintain on a method-by-method basis. Depends-on : trans.rascal.CC.java.CCHistogramJava Returns : map[str, int] Back to top","title":"trans.rascal.CC.java.CCHistogramJava.historic"},{"location":"user-guide/metrics/#transrascalccjavaccoverjavamethodshistoric","text":"Short name : giniCCOverMethodsJava.historic Friendly name : Historic giniCCOverMethodsJava Historic version of : trans.rascal.CC.java.CCOverJavaMethods Calculates how cyclomatic complexity is spread over the methods of a system. If high CC is localized, then this may be easily fixed but if many methods have high complexity, then the project may be at risk. This metric is good to compare between projects. Depends-on : trans.rascal.CC.java.CCOverJavaMethods Returns : real","title":"trans.rascal.CC.java.CCOverJavaMethods.historic"},{"location":"user-guide/metrics/#historic-metric-providers-for-osgi-dependencies","text":"These metrics are related to OSGi dependencies declared in MANIFEST.MF files. Back to top","title":"Historic Metric Providers for OSGi Dependencies"},{"location":"user-guide/metrics/#transrascaldependencyosginumberosgibundledependencieshistoric","text":"Short name : numberOSGiBundleDependencies.historic Friendly name : Historic numberOSGiBundleDependencies Historic version of : trans.rascal.dependency.osgi.numberOSGiBundleDependencies Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies). Depends-on : trans.rascal.dependency.osgi.numberOSGiBundleDependencies Returns : int","title":"trans.rascal.dependency.osgi.numberOSGiBundleDependencies.historic"},{"location":"user-guide/metrics/#historic-metric-providers-for-maven-dependencies","text":"These metrics are related to Maven dependencies declared in pom.xml files. Back to top","title":"Historic Metric Providers for Maven Dependencies"},{"location":"user-guide/metrics/#transrascaldependencymavennumbermavendependencieshistoric","text":"Short name : numberMavenDependencies.historic Friendly name : Historic numberMavenDependencies Historic version of : trans.rascal.dependency.maven.numberMavenDependencies Retrieves the number of Maven dependencies. Depends-on : trans.rascal.dependency.maven.numberMavenDependencies Returns : int Back to top","title":"trans.rascal.dependency.maven.numberMavenDependencies.historic"},{"location":"user-guide/metrics/#historic-metric-providers-for-docker-dependencies","text":"The following Historic Metric Provider is associated with Docker Dependencies Back to top","title":"Historic Metric Providers for Docker Dependencies"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricconfigurationdockerdependencies","text":"Short name : historic.configuration.docker.dependencies Friendly name : Number of dependencies defined in Dockerfiles per day This metric computes the number of the dependencies that are defined in the Dockerfiles of a project per day. It also computes additional information such as the number of each version of the dependencies (image/package). Depends-on : org.eclipse.scava.metricprovider.trans.configuration.docker.dependencies Returns : DockerDependenciesHistoricMetric which contains: Variable Type numberOfDockerDependencies int numberOfDockerPackageDependencies int numberOfDockerImageDependencies int Visualisation Output Information : DockerDependenciesHistoricMetric : id docker.dependencies id docker.packageDependencies id docker.imageDependencies Back to top","title":"org.eclipse.scava.metricprovider.historic.configuration.docker.dependencies"},{"location":"user-guide/metrics/#historic-metric-providers-for-puppet-dependencies","text":"The following Historic Metric Provider is associated with Puppet Dependencies Back to top","title":"Historic Metric Providers for Puppet Dependencies"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricconfigurationpuppetdependencies","text":"Short name : historic.configuration.puppet.dependencies Friendly name : Number of dependencies defined in Puppet manifests per day This metric computes the number of the dependencies that are defined in the Puppet manifests of a project per day. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies Returns : PuppetDependenciesHistoricMetric which contains: Variable Type numberOfPuppetDependencies int Visualisation Output Information : DockerDependenciesHistoricMetric : id puppet.dependencies Back to top","title":"org.eclipse.scava.metricprovider.historic.configuration.puppet.dependencies"},{"location":"user-guide/metrics/#historic-metric-providers-for-docker-smells","text":"The following Historic Metric Provider is associated with Docker Smells Back to top","title":"Historic Metric Providers for Docker Smells"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricconfigurationdockersmells","text":"Short name : historic.configuration.docker.smells Friendly name : Number of smells detected in Dockerfiles per day This metric computes the number of the smells that are detected in the Dockerfiles of a project per day. It also computes additional information such as the number of each type of the smell. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.docker.smells Returns : DockerSmellsHistoricMetric which contains: Variable Type cumulativeNumberOfDockerSmells int numberOfImproperUpgradeSmells int numberOfUnknownPackageVersionSmells int numberOfUntaggedImageSmells int numberOfImproperSudoSmells int numberOfImproperCopySmells int numberOfImproperFromSmells int numberOfImproperCmdSmells int numberOfMeaninglessSmells int numberOfInvalidPortsSmells int numberOfImproperShellSmells int numberOfImproperEntrypointSmells int numberOfDeprecatedInstructionSmells int Visualisation Output Information : DockerDependenciesHistoricMetric : id docker.smells id docker.smells.improperUpgradeSmells id docker.smells.unknownPackageVersionSmells id docker.smells.untaggedImageSmells id docker.smells.improperSudoSmells id docker.smells.improperCopySmells id docker.smells.improperFromSmells id docker.smells.improperCmdSmells id docker.smells.meaninglessSmells id docker.smells.invalidPortsSmells id docker.smells.improperShellSmells id docker.smells.improperEntrypointSmells id docker.smells.deprecatedInstructionSmells Back to top","title":"org.eclipse.scava.metricprovider.historic.configuration.docker.smells"},{"location":"user-guide/metrics/#historic-metric-providers-for-puppet-smells","text":"The following Historic Metric Providers are associated with Puppet Smells Back to top","title":"Historic Metric Providers for Puppet Smells"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricconfigurationpuppetdesignsmells","text":"Short name : historic.configuration.puppet.designsmells Friendly name : Number of design smells detected in Puppet manifests per day This metric computes the number of the design smells that are detected in the Puppet manifests of a project per day. It also computes additional information such as the number of each type of the smell. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.puppet.designsmells Returns : PuppetDesignSmellsHistoricMetric which contains: Variable Type cumulativeNumberOfDesignSmells int numberOfMultifacetedSmells int numberOfUnnecessarySmells int numberOfImperativeSmells int numberOfMissAbSmells int numberOfInsufficientSmells int numberOfUnstructuredSmells int numberOfTightSmells int numberOfBrokenSmells int numberOfMissingDepSmells int numberOfHairballSmells int numberOfDeficientSmells int numberOfWeakenSmells int Visualisation Output Information : DockerDependenciesHistoricMetric : id puppet.design.smells id puppet.design.multifacetedSmells id puppet.design.unnecessarySmells id puppet.design.imperativeSmells id puppet.design.missAbSmells id puppet.design.insufficientSmells id puppet.design.unstructuredSmells id puppet.design.tightSmells id puppet.design.brokenSmells id puppet.design.missingDepSmells id puppet.design.hairballSmells id puppet.design.deficientSmells id puppet.design.weakenSmells Back to top","title":"org.eclipse.scava.metricprovider.historic.configuration.puppet.designsmells"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricconfigurationpuppetimplementationsmells","text":"Short name : historic.configuration.puppet.implementationsmells Friendly name : Number of implementation smells detected in Puppet manifests per day This metric computes the number of the implementation smells that are detected in the Puppet manifests of a project per day. It also computes additional information such as the number of each type of the smell. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.puppet.implementationsmells Returns : PuppetImplementationSmellsHistoricMetric which contains: Variable Type cumulativeNumberOfImplementationSmells int numberOfMissingDefaultCaseSmells int numberOfInconsistentNamingSmells int numberOfDuplicateEntitySmells int numberOfMisplacedAttributeSmells int numberOfImproperAlignment int numberOfInvalidPropertySmells int numberOfImproperQuoteSmells int numberOfLongStatementsSmells int numberOfUnguardedVariableSmells int numberOfMissingDocSmells int numberOfDeprecatedStatementsSmells int numberOfIncompleteTasksSmells int numberOfComplexExpressionSmells int numberOfMissingElseSmells int Visualisation Output Information : DockerDependenciesHistoricMetric : id puppet.implementation.smells id puppet.implementation.missingDefaultCaseSmells id puppet.implementation.inconsistentNamingSmells id puppet.implementation.duplicateEntitySmells id puppet.implementation.misplacedAttributeSmells id puppet.implementation.improperAlignmentSmells id puppet.implementation.invalidPropertySmells id puppet.implementation.improperQuoteSmells id puppet.implementation.longStatementsSmells id puppet.implementation.unguardedVariableSmells id puppet.implementation.missingDocSmells id puppet.implementation.deprecatedStatementsSmells id puppet.implementation.incompleteTasksSmells id puppet.implementation.complexExpressionSmells id puppet.implementation.missingElseSmells Back to top","title":"org.eclipse.scava.metricprovider.historic.configuration.puppet.implementationsmells"},{"location":"user-guide/metrics/#transient-metric-providers","text":"Transient metrics are used to calculate heuristics that are associated with a particular period in time, i.e. a single day. Transient Metrics are stored temporarily within the knowledge base and their output is passed as parameters in the calculation of other transient and historic metrics. Depending on the complexity, a transient metric can depend on the output from other tools, other transient metircs or have no dependencies at all. Back to top","title":"Transient Metric Providers"},{"location":"user-guide/metrics/#transient-metric-providers-for-bug-trackers","text":"The following Transient Metric Providers are associated with Issue trackers. Back to top","title":"Transient Metric Providers for Bug Trackers"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsactiveusers","text":"Short name : trans.bugs.activeusers Friendly name : Number of users with new bug comment in the last 15 days This metric computes the number of users that submitted new bug comments in the last 15 days, for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsActiveUsersTransMetric which contains: Variable Type bugs List<BugsData> users List<User> Additional Information : BugData : String bugTrackerId int activeUsers int inactiveUsers int previousUsers int users int days User : String bugTrackerId String userId String lastActivityDate int comments int requests int replies Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.activeusers"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsbugmetadata","text":"Short name : trans.bugs.bugmetadata Friendly name : Bug header metadata This metric computes various metadata in bug header, i.e. priority, status, operation system and resolution. Other values computed by this metric includes average sentiment, content class and requests/replies. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification , org.eclipse.scava.metricprovider.trans.sentimentclassification , org.eclipse.scava.metricprovider.trans.detectingcode Returns : BugsBugMetadataTransMetric which contains: Variable Type BugData List<BugData> CommentData List<CommentData> Additional Information : BugData : String bugTrackerId String bugId String status List<String> resolution String operatingSystem String priority String creationTime String lastClosedTime String startSentiment String endSentiment float averageSentiment int commentSum int sentimentSum String firstCommentId String lastCommentId CommentData : String bugTrackerId String bugId String commentId String creationTime String creator String contentClass String requestReplyPrediction Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.bugmetadata"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugscomments","text":"Short name : trans.bugs.comments Friendly name : Number of bug comments This metric computes the number of bug comments, per bug tracker. Depends-on : None Returns : BugsCommentsTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> Additional Information : BugTrackerData: String bugTrackerId int numberOfComments int cumulativeNumberOfComments Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.comments"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugscontentclasses","text":"Short name : trans.bugs.contentclasses Friendly name : Content classes in bug comments This metric computes the frequency and percentage of content Classes in bug comments, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsContentClassesTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> contentClasses List<ContentClass> Additional Information : BugTrackerData : String bugTrackerId int numberOfComments ContentClass : String bugTrackerId String classLabel int numberOfComments float percentage Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.contentclasses"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsdailyrequestsreplies","text":"Short name : trans.bugs.dailyrequestsreplies Friendly name : Number of bug comments, requests and replies per day This metric computes the number of bug comments, including those regarded as requests and replies each day, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsDailyRequestsRepliesTransMetric which contains: Variable Type dayComments List<DayComments> Additional Information : DayComments : String name String bugTrackerId int numberOfComments int numberOfRequests int numberOfReplies float percentageOfComments float percentageOfRequests float percentageOfReplies Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.dailyrequestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsemotions","text":"Short name : trans.bugs.emotions Friendly name : Emotions in bug comments This metric computes the emotional dimensions in bug comments, per bug tracker. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.emotionclassification Returns : BugsEmotionsTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> dimensions List<EmotionDimension> Additional Information : BugTrackerData : String bugTrackerId int numberOfComments int cumulativeNumberOfComments EmotionDimension : String bugTrackerId String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfComments int cumulativeNumberOfComments float percentage float cumulativePercentage Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.emotions"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugshourlyrequestsreplies","text":"Short name : trans.bugs.hourlyrequestsreplies Friendly name : Number of bug comments, requests and replies per hour This metric computes the number of bug comments, including those regarded as requests and replies, every hour of the day, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsHourlyRequestsRepliesTransMetric which contains: Variable Type hourComments List<HourComments> Additional Information : HourComments : String bugTrackerId String hour int numberOfComments int numberOfRequests int numberOfReplies float percentageOfComments float percentageOfRequests float percentageOfReplies Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsmigrationissues","text":"Short name : trans.bugs.migrationissues Friendly name : Migration Issues Detection in Bug Trackers This metric detects migration issues in Bug Tracking Systems. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.topics Returns : BugTrackerMigrationIssueTransMetric which contains: Variable Type bugTrackerMigrationIssues List<BugTrackerMigrationIssue> Additional Information : BugTrackerMigrationIssue : String bugTrackerId String bugId String summary Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.migrationissues"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsmigrationissuesmaracas","text":"Short name : trans.bugs.migrationissuesmaracas Friendly name : Migration Issues Detection Maracas in Bug Trackers This metric detects migration issues in Bug Tracking Systems along with data from Maracas. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.bugs.migrationissues , org.eclipse.scava.metricprovider.trans.migrationissuesmaracas , org.eclipse.scava.metricprovider.trans.plaintextprocessing Returns : BugTrackerMigrationIssueMaracasTransMetric which contains: Variable Type bugTrackerMigrationIssuesMaracas List<BugTrackerMigrationIssueMaracas> Additional Information : BugTrackerMigrationIssueMaracas : String bugTrackerId String bugId List<String> changes List<Double> matchingPercentage Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.migrationissuesmaracas"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsnewbugs","text":"Short name : trans.bugs.newbugs Friendly name : Number of new bugs This metric computes the number of new bugs over time, per bug tracker. Depends-on : None Returns : BugsNewBugsTransMetric which contains: Variable Type bugTrackerData type Additional Information : BugTrackerData : String bugTrackerId int numberOfBugs int cumulativeNumberOfBugs Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.newbugs"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugspatches","text":"Short name : trans.bugs.patches Friendly name : Number of patches per bug This metric computes the number of patches submitted by the community (users) for each bug. Depends-on : None Returns : BugsPatchesTransMetric which contains: Variable Type bugTrackerData type Additional Information : BugTrackerData : String bugTrackerId int numberOfPatches int cumulativeNumberOfPatches Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.patches"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsreferences","text":"Short name : trans.bugs.references Friendly name : Bugs References This metrics search for references of commits or bugs within comments comming from bugs comments. Depends-on : None Returns : BugsReferenceTransMetric which contains: Variable Type bugs List<BugReferringTo> Additional Information : BugReferringTo : String bugTrackerId String bugId String commentId List<String> bugsReferred (URLs) List<String> commitsReferred (URLs) Note : When this metric is used on GitHub, it should be noted that some references of bugs will be in fact pull requests. The reason is that GitHub considers pull requests equally as issues. Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.references"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsrequestsreplies","text":"Short name : trans.bugs.requestreplies Friendly name : Bug statistics (answered?, response time) This metric computes for each bug, whether it was answered. If so, it computes the time taken to respond. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsRequestsRepliesTransMetric which contains: Variable Type bugs List<BugStatistics> Additional Information : BugStatistics : String bugTrackerId String bugId boolean answered long responseDurationSec String responseDate Back to top","title":"org.eclipse.scava.metricprovider.trans.bugs.requestsreplies"},{"location":"user-guide/metrics/#transient-metric-providers-for-newsgroups-and-forums","text":"The following Transient Metric Providers are associated with communication channels in general, either newsgroups or forums. Despite the name of the metrics are newsgroups, all the metrics are valid for communication channels. Back to top","title":"Transient Metric Providers for Newsgroups and Forums"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsactiveusers","text":"Short name : trans.newsgroups.activeusers Friendly name : Number of users with new comment in the last 15 days This metric computes the number of users that submitted news comments in the last 15 days, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsActiveUsersTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> user List<User> Additional Information : NewsgroupData : String newsgroupName int activeUsers int inactiveUsers int previousUsers int users int days User : String newsgroupName String userId String lastActiveDate int articles int requests int replies Back to top","title":"org.eclipse.scava.metricprovider.trans.newsgroups.activeusers"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsarticles","text":"Short name : trans.newsgroups.articles Friendly name : Number of articles per newsgroup This metric computes the number of articles, per newsgroup. Depends-on : None Returns : NewsgroupsArticlesTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> Additional Information : NewsgroupData : String newsgroupName int numberOfArticles int cumulativeNumberOfArticles Back to top","title":"org.eclipse.scava.metricprovider.trans.newsgroups.articles"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupscontentclasses","text":"Short name : trans.newsgroups.contentclasses Friendly name : Content classes in newsgroup articles This metric computes the content classes in newgroup articles, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : NewsgroupsContentClassesTransMetric which contains: Variable Type newsGroupData List<NewsgroupData> contentClass List< ContentClass> Additional Information : NewsGroupData: String newsgroupName int numberOfArticles ContentClass: String newsgroupName String classLabel int numberOfArticles float percentage Back to top","title":"org.eclipse.scava.metricprovider.trans.newsgroups.contentclasses"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsdailyrequestsreplies","text":"Short name : trans.newsgroups.dailyrequestsreplies Friendly name : Number of articles, requests and replies per day This metric computes the number of articles, including those regarded as requests and replies for each day of the week, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsDailyRequestsRepliesTransMetric which contains: Variable Type dailyArticles List<DailyArticles> Additional Information : DailyArticles : String name int numberOfArticles int numberOfRequests int numberOfReplies float percentageOfArticles float percentageOfRequests float percentageOfReplies Back to top","title":"org.eclipse.scava.metricprovider.trans.newsgroups.dailyrequestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsemotions","text":"Short name : trans.newsgroups.emotions Friendly name : Emotions in newsgroup articles This metric computes the emotional dimensions in newsgroup articles, per newsgroup. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.emotionclassification Returns : NewsgroupsEmotionsTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> emotionDimension List<EmotionDimension> Additional Information : NewsgroupData : String newsgroupName int numberOfArticles int cumulativeNumberOfArticles EmotionDimension : String newsgroupName String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfArticles int cumulativeNumberOfArticles float percentage float cumulativePercentage Back to top","title":"org.eclipse.scava.metricprovider.trans.newsgroups.emotions"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupshourlyrequestsreplies","text":"Short name : trans.newsgroups.hourlyrequestsreplies Friendly name : Number of articles, requests and replies per hour This metric computes the number of articles, including those regarded as requests and replies for each hour of the day, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsHourlyRequestsRepliesTransMetric which contains: Variable Type hourArticles List<HourArticles> Additional Information : HourArticles : String hour int numberOfArticles int numberOfRequests int numberOfReplies float percentageOfArticles float percentageOfRequests float percentageOfReplies Back to top","title":"org.eclipse.scava.metricprovider.trans.newsgroups.hourlyrequestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsmigrationissues","text":"Short name : trans.newsgroups.migrationissues Friendly name : Migration Issues Detection in Communication Channels This metric detects migration issues in Communication Channels articles. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.topics , org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : NewsgroupsMigrationIssueTransMetric which contains: Variable Type newsgroupsMigrationIssues List<NewsgroupsMigrationIssue> Additional Information : NewsgroupsMigrationIssue : String newsgroupName int threadId long articleId Back to top","title":"org.eclipse.scava.metricprovider.trans.newsgroups.migrationissues"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsmigrationissuesmaracas","text":"Short name : trans.newsgroups.migrationissuesmaracas Friendly name : Migration Issues Detection Maracas in Newsgroups This metric detects migration issues in Newsgroups along with data from Maracas. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.newsgroups.migrationissues , org.eclipse.scava.metricprovider.trans.migrationissuesmaracas , org.eclipse.scava.metricprovider.trans.plaintextprocessing Returns : BugTrackerMigrationIssueMaracasTransMetric which contains: Variable Type newsgroupsMigrationIssuesMaracas List<NewsgroupMigrationIssueMaracas> Additional Information : NewsgroupMigrationIssueMaracas : String newsgrupName int threadId List<String> changes List<Double> matchingPercentage Back to top","title":"org.eclipse.scava.metricprovider.trans.newsgroups.migrationissuesmaracas"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupssentiment","text":"Short name : trans.newsgroups.sentiment Friendly name : Average sentiment in newsgroup threads The metric computes the average sentiment, including sentiment at the beginning and end of each thread, per newsgroup. Sentiment polarity value could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment) Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.sentimentclassification Returns : NewsgroupsSentimentTransMetric which contains: Variable Type threadStatistics List<ThreadStatistics> Additional Information : ThreadStatistics : String newsgroupName int threadId float averageSentiment String startSentiment String endSentiment Back to top","title":"org.eclipse.scava.metricprovider.trans.newsgroups.sentiment"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsthreads","text":"Short name : trans.newsgroups.threads Friendly name : Assigns newsgroup articles to threads This metric holds information for assigning newsgroup articles to threads. The threading algorithm is executed from scratch every time. Depends-on : None , Returns : NewsgroupsThreadsTransMetric which contains: Variable Type articleData List<ArticleData> threadData List<ThreadData> newsgroupData List<NewsgroupData> currentDate List<CurrentDate> Additional Information : ArticleData : String newsgroupName int articleNumber String articlesId String date String from String subject String contentClass String references ThreadData : int threadId NewsgroupData : String newsgroupName int threads int previousThreads CurrentDate : String date Back to top","title":"org.eclipse.scava.metricprovider.trans.newsgroups.threads"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsthreadsrequestsreplies","text":"Short name : trans.newsgroups.threadsrequestsreplies Friendly name : Thread statistics (answered?, response time) The metric computes for each thread whether it is answered. If so, it computes the response time. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsThreadsRequestsRepliesTransMetric which contains: Variable Type threadStatistics List<ThreadStatistics> Additional Information : ThreadStatistics : String newsgroupName int threadId boolean firstRequest boolean answered long responseDurationSec String responseDate Back to top","title":"org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies"},{"location":"user-guide/metrics/#transient-metric-providers-for-documentation","text":"The following Transient Metric Providers are associated with documentation analyses. Back to top","title":"Transient Metric Providers for Documentation"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransdocumentation","text":"Short name : trans.documentation Friendly name : Documentation processing This metric process the files returned from the documentation readers and extracts the body (in format HTML or text) Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : DocumentationTransMetric which contains: Variable Type documentationEntries List<DocumentationEntry> documentation List<Documentation> Additional Information : DocumentationEntry : String documentationId String entryId String body String originalFormatName String originalFormatMime boolean htmlFormatted Documentation : String documentationId List<String> entriesId List<String> removedEntriesId String lastUpdateDate String lastRevisionAnalyzed String nextUpdateDate Back to top","title":"org.eclipse.scava.metricprovider.trans.documentation"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransdocumentationclassification","text":"Short name : trans.documentation.classification Friendly name : Documentation classification This metric determines which type of documentation is present. The possible types are: API , Development , Installation , Started , User . Depends-on : org.eclipse.scava.metricprovider.trans.documentation , org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : DocumentationClassificationTransMetric which contains: Variable Type documentationEntriesClassification List<DocumentationEntryClassification> Additional Information : DocumentationEntryClassification : String documentationId String entryId List<String> types Back to top","title":"org.eclipse.scava.metricprovider.trans.documentation.classification"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransdocumentationdetectingcode","text":"Short name : trans.documentation.detectingcode Friendly name : Documentation detection of code This metric process the plain text from documentation and detects the portions corresponding to code and natural language Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.documentation.plaintext Returns : DocumentationDetectingCodeTransMetric which contains: Variable Type documentationEntriesDetectingCode List<DocumentationEntryDetectingCode> Additional Information : DocumentationEntryDetectingCode : String documentationId String entryId String naturalLanguage String code Back to top","title":"org.eclipse.scava.metricprovider.trans.documentation.detectingcode"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransdocumentationplaintext","text":"Short name : trans.documentation.plaintext Friendly name : Documentation plain text processor This metric process the body of each documentation entry and extracts the plain text Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.documentation Returns : DocumentationPlainTextTransMetric which contains: Variable Type documentationEntriesPlainText List<DocumentationEntryPlainText> Additional Information : DocumentationEntryPlainText : String documentationId String entryId List<String> plainText Back to top","title":"org.eclipse.scava.metricprovider.trans.documentation.plaintext"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransdocumentationreadability","text":"Short name : trans.documentation.readability Friendly name : Documentation calculation of readability This metric calculates the readability of each documentation entry. The higher the score, the more difficult to understand the text. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.documentation , org.eclipse.scava.metricprovider.trans.documentation.detectingcode Returns : DocumentationReadabilityTransMetric which contains: Variable Type documentationEntriesReadability List<DocumentationEntryReadability> Additional Information : DocumentationEntryReadability : String documentationId String entryId double readability Back to top","title":"org.eclipse.scava.metricprovider.trans.documentation.readability"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransdocumentationsentiment","text":"Short name : trans.documentation.sentiment Friendly name : Documentation Sentiment Analysis This metric calculates the sentiment polarity of each documentation entry. Sentiment polarity value could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment) Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.documentation , org.eclipse.scava.metricprovider.trans.documentation.detectingcode Returns : DocumentationSentimentTransMetric which contains: Variable Type documentationEntriesSentiment List<DocumentationEntrySentiment> Additional Information : DocumentationEntrySentiment : String documentationId String entryId String polarity Back to top","title":"org.eclipse.scava.metricprovider.trans.documentation.sentiment"},{"location":"user-guide/metrics/#transient-metric-providers-for-natural-language-processing","text":"The following Transient Metric Providers are associated with Natural Language Processing tools. Back to top","title":"Transient Metric Providers for Natural Language Processing"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransdetectingcode","text":"Short name : trans.detectingcode Friendly name : Distinguishes between code and natural language This metric determines the parts of a bug comment or a newsgroup article that contains code or natural language. Depends-on : org.eclipse.scava.metricprovider.trans.plaintextprocessing Returns : DetectingCodeTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerCommentDetectingCode> newsgroupArticles List<NewsgroupArticleDetectingCode> forumPosts List<ForumPostsDetectingCode> Additional Information : BugTrackerCommentDetectingCode : String bugTrackerId String bugId String commentId String naturalLanguage String code NewsgroupArticleDetectingCode : String newsgroupName String articleNumber String naturalLanguage String code ForumPostsDetectingCode : String forumId String topicId String postId String naturalLanguage String code Back to top","title":"org.eclipse.scava.metricprovider.trans.detectingcode"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransemotionclassification","text":"Short name : trans.emotionclassification Friendly name : Emotion classifier This metric computes the emotions present in each bug comment, newsgroup article or forum post. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : EmotionClassificationTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerCommentEmotionClassification> newsgroupArticles List<NewsgroupArticleEmotionClassification> forumPosts List<ForumPostsEmotionClassification> Additional Information : BugTrackerCommentEmotionClassification : String bugTrackerId String bugId String commentId String emotions NewsgroupArticleEmotionClassification : String newsgroupName String articleNumber String emotions ForumPostsEmotionClassification : String forumId String topicId String postId String emotions Back to top","title":"org.eclipse.scava.metricprovider.trans.emotionclassification"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransplaintextprocessing","text":"Short name : trans.plaintextprocessing Friendly name : Plain text processing This metric preprocess each bug comment, newsgroup article or forum post into a split plain text format. Depends-on : None Returns : PlainTextProcessingTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerCommentPlainTextProcessing> newsgroupArticles List<NewsgroupArticlePlainTextProcessing> forumPosts List<ForumPostsPlainTextProcessing> Additional Information : BugTrackerCommentPlainTextProcessing : String bugTrackerId String bugId String commentId String plainText boolean hadReplies NewsgroupArticlePlainTextProcessing : String newsgroupName String articleNumber String plainText boolean hadReplies ForumPostsPlainTextProcessing : String forumId String topicId String postId String plainText boolean hadReplies Back to top","title":"org.eclipse.scava.metricprovider.trans.plaintextprocessing"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransrequestreplyclassification","text":"Short name : trans.requestreplyclassification Friendly name : Request/Reply classification This metric computes if a bug comment, newsgroup article or forum post is a request of a reply. Depends-on : org.eclipse.scava.metricprovider.trans.plaintextprocessing , org.eclipse.scava.metricprovider.trans.detectingcode Returns : RequestReplyClassificationTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerComments> newsgroupArticles List<NewsgroupArticles> forumPosts List<ForumPosts> Additional Information : BugTrackerComments : String bugTrackerId String bugId String commentId String classificationResult String date NewsgroupArticles : String newsgroupName String articleNumber String classificationResult String date ForumPosts : String forumId String topicId String postId String classificationResult String date Back to top","title":"org.eclipse.scava.metricprovider.trans.requestreplyclassification"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertranssentimentclassification","text":"Short name : trans.sentimentclassification Friendly name : Sentiment classification This metric computes the sentiment of each bug comment, newsgroup article or forum post. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or 1 (positive sentiment). Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : SentimentClassificationTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerCommentsSentimentClassification> newsgroupArticles List<NewsgroupArticlesSentimentClassification> forumPosts List<ForumPostSentimentClassification> Additional Information : BugTrackerCommentsSentimentClassification : String bugTrackerId String bugId String commentId String polarity ( negative (-1) , neutral (0) or positive (1) ) NewsgroupArticlesSentimentClassification : String newsgroupName String articleNumber String polarity ( negative (-1) , neutral (0) or positive (1) ) ForumPostSentimentClassification : String forumId String topicId String postId String polarity ( negative (-1) , neutral (0) or positive (1) ) Back to top","title":"org.eclipse.scava.metricprovider.trans.sentimentclassification"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransseverityclassification","text":"Short name : trans.severityclassification Friendly name : Severity classification This metric computes the severity of each bug comment, newsgroup article or forum post. Severity could be blocker, critical, major, minor, enhancement, normal). For bug comments, there is an additional severity level called unknown . A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode , org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : SeverityClassificationTransMetric which contains: Variable Type bugTrackerBugs List<BugTrackerBugsData> newsgroupArticles List<NewsgroupArticleData> newsgroupThreads List<NewsgroupThreadData> forumPosts ForumPostData> Additional Information : BugTrackerBugsData : String bugTrackerId String bugId String severity int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams NewsgroupArticleData : String NewsgroupName long articleNumber int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams NewsgroupThreadData : String newsgroupName int threadId String severity BugTrackerBugsData : String forumId String topicId String severity int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams Back to top","title":"org.eclipse.scava.metricprovider.trans.severityclassification"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertranstopics","text":"Short name : trans.topics Friendly name : Topic clustering This metric computes topic clusters for each bug comment, newsgroup article or forum post in the last 30 days. Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : TopicsTransMetric which contains: Variable Type bugTrackerComments List<BugTrackerCommentsData> bugTrackerTopics List<BugTrackerTopic> newsgroupArticles List<NewsgroupArticlesData> newsgroupTopics List<NewsgroupTopic> Additional Information : BugTrackerCommentsData : String bugTrackerId String bugId String commentId String subject String text String date NewsgroupArticlesData : String newsgroupName long articleNumber String subject String text String date BugTrackerTopic : String bugTrackerId List<String> labels int numberOfDocuments List<String> commentsId NewsgroupTopic : String newsgroupName List<String> labels int numberOfDocuments List<Long> articlesId","title":"org.eclipse.scava.metricprovider.trans.topics"},{"location":"user-guide/metrics/#transient-metric-providers-for-commits-and-committers","text":"These metrics are related to the commits and committers of a project. Back to top","title":"Transient Metric Providers for Commits and Committers"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertranscommitsmessageplaintext","text":"Short name : trans.commits.message.plaintext Friendly name : Commits message plain text This metric preprocess each commit message to get a split plain text version. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : CommitsMessagePlainTextTransMetric which contains: Variable Type commitsMessagesPlainText List<CommitMessagePlainText> Additional Information : CommitMessagePlainText : String repository (URL) String revision (Commit SHA) List<String> plainText Back to top","title":"org.eclipse.scava.metricprovider.trans.commits.message.plaintext"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertranscommitsmessagereferences","text":"Short name : trans.commits.messagereferences Friendly name : Commits Messages References This metrics search for references of commits or bugs within the messages of commits. In order to detect bugs references, it is necessary to use at the same time one Bug Tracker, as the retrieval of references are based on patterns defined by bug trackers. If multiple or zero Bug Trackers are defined in the project, the metric will only search for commits (alphanumeric strings of 40 characters). Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : CommitsMessageReferenceTransMetric which contains: Variable Type commitsMessagesReferringTo List<CommitMessageReferringTo> Additional Information : BugReferringTo : String repository (URL) String revision (Commit SHA) List<String> bugsReferred (URLs) List<String> commitsReferred (URLs) Note : When this metric is used on GitHub, it should be noted that some references of bugs will be in fact pull requests. The reason is that GitHub considers pull requests equally as issues. Back to top","title":"org.eclipse.scava.metricprovider.trans.commits.messagereferences"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertranscommitsmessagetopics","text":"Short name : trans.commits.message.topics Friendly name : Commits Messages Topic Clustering This metric computes topic clusters for each commit message. Depends-on : org.eclipse.scava.metricprovider.trans.commits.message.plaintext Returns : CommitsMessageTopicsTransMetric which contains: Variable Type commitsMessages List<CommitMessage> commitsTopics List<CommitsTopic> Additional Information : CommitMessage : String repository (URL) String revision (Commit SHA) String subject String message Date date CommitsTopic : String repository (URL) List<String> labels int numberOfMessages List<String> commitsMessageId Back to top","title":"org.eclipse.scava.metricprovider.trans.commits.message.topics"},{"location":"user-guide/metrics/#transrascalactivecommittersactivecommitters","text":"Short name : activeCommitters Friendly name : Committers of last two weeks A list of committers who have been active the last two weeks. This metric is meant for downstream processing. Depends-on : trans.rascal.activecommitters.committersToday Returns : rel[datetime,set[str]] Back to top","title":"trans.rascal.activecommitters.activeCommitters"},{"location":"user-guide/metrics/#transrascalactivecommitterscommittersoverfile","text":"Short name : giniCommittersOverFile Friendly name : Committers over file Calculates the gini coefficient of committers per file Depends-on : trans.rascal.activecommitters.countCommittersPerFile Returns : real Back to top","title":"trans.rascal.activecommitters.committersoverfile"},{"location":"user-guide/metrics/#transrascalactivecommitterscountcommittersperfile","text":"Short name : countCommittersPerFile Friendly name : Number of committers per file Count the number of committers that have touched a file. Depends-on : trans.rascal.activecommitters.committersPerFile Returns : map[loc file, int numberOfCommitters] Back to top","title":"trans.rascal.activecommitters.countCommittersPerFile"},{"location":"user-guide/metrics/#transrascalactivecommittersfirstlastcommitdatesperdeveloper","text":"Short name : firstLastCommitDates Friendly name : First and last commit dates per developer Collects per developer the first and last dates on which he or she contributed code. This basic metric is used downstream for other metrics, but it is also used to drill down on the membership of specific individuals of the development team. Depends-on : trans.rascal.activecommitters.committersToday Returns : map[str, tuple[datetime,datetime]] Back to top","title":"trans.rascal.activecommitters.firstLastCommitDatesPerDeveloper"},{"location":"user-guide/metrics/#transrascalactivecommittersdevelopmentteam","text":"Short name : developmentTeam Friendly name : Development team Lists the names of people who have been contributing code at least once in the history of the project. Depends-on : trans.rascal.activecommitters.committersToday Returns : set[str] Back to top","title":"trans.rascal.activecommitters.developmentTeam"},{"location":"user-guide/metrics/#transrascalactivecommitterspercentageofweekendcommits","text":"Short name : percentageOfWeekendCommits Friendly name : Percentage of weekend commits Percentage of commits made during the weekend Depends-on : trans.rascal.activecommitters.commitsPerWeekDay Returns : int Back to top","title":"trans.rascal.activecommitters.percentageOfWeekendCommits"},{"location":"user-guide/metrics/#transrascalactivecommittersmaximumactivecommittersever","text":"Short name : maximumActiveCommittersEver Friendly name : Maximum active committers ever What is the maximum number of committers who have been active together in any two week period? Depends-on : trans.rascal.activecommitters.numberOfActiveCommitters.historic Returns : int Back to top","title":"trans.rascal.activecommitters.maximumActiveCommittersEver"},{"location":"user-guide/metrics/#transrascalactivecommittersdevelopmentteamemails","text":"Short name : developmentTeamEmails Friendly name : Development team Lists the names of people who have been contributing code at least once in the history of the project. Depends-on : trans.rascal.activecommitters.committersEmailsToday Returns : set[str] Back to top","title":"trans.rascal.activecommitters.developmentTeamEmails"},{"location":"user-guide/metrics/#transrascalactivecommittersdevelopmentdomainnames","text":"Short name : developmentDomainNames Friendly name : Development team domain names Lists the domain names of email addresses of developers if such information is present. Depends-on : trans.rascal.activecommitters.developmentTeamEmails Returns : set[str] Back to top","title":"trans.rascal.activecommitters.developmentDomainNames"},{"location":"user-guide/metrics/#transrascalactivecommitterscommittersperfile","text":"Short name : committersPerFile Friendly name : Committers per file Register which committers have contributed to which files Depends-on : - Returns : rel[loc,str] Back to top","title":"trans.rascal.activecommitters.committersPerFile"},{"location":"user-guide/metrics/#transrascalactivecommitterslongertermactivecommitters","text":"Short name : longerTermActiveCommitters Friendly name : Committers of last year Committers who have been active the last 12 months. This metric is meant for downstream processing. Depends-on : trans.rascal.activecommitters.committersToday Returns : rel[datetime,set[str]] Back to top","title":"trans.rascal.activecommitters.longerTermActiveCommitters"},{"location":"user-guide/metrics/#transrascalactivecommitterscommitsperdeveloper","text":"Short name : commitsPerDeveloper Friendly name : Number of commits per developer The number of commits per developer indicates not only the volume of the contribution of an individual but also the style in which he or she commits, when combined with other metrics such as churn. Few and big commits are different from many small commits. This metric is used downstream by other metrics as well. Depends-on : - Returns : map[str, int] Back to top","title":"trans.rascal.activecommitters.commitsPerDeveloper"},{"location":"user-guide/metrics/#transrascalactivecommitterscommittersage","text":"Short name : ageOfCommitters Friendly name : Developer experience in project Measures in days the amount of time between the first and last contribution of each developer. Depends-on : trans.rascal.activecommitters.firstLastCommitDatesPerDeveloper Returns : rel[str,int] Back to top","title":"trans.rascal.activecommitters.committersAge"},{"location":"user-guide/metrics/#transrascalactivecommitterscommitterstoday","text":"Short name : committersToday Friendly name : Active committers Who have been active today? Depends-on : - Returns : set[str] Back to top","title":"trans.rascal.activecommitters.committersToday"},{"location":"user-guide/metrics/#transrascalactivecommittersprojectage","text":"Short name : projectAge Friendly name : Age of the project (nr of days between first and last commit) Age of the project (nr of days between first and last commit) Depends-on : trans.rascal.activecommitters.firstLastCommitDatesPerDeveloper Returns : int Back to top","title":"trans.rascal.activecommitters.projectAge"},{"location":"user-guide/metrics/#transrascalactivecommitterscommitsperweekday","text":"Short name : commitsPerWeekDay Friendly name : Commits per week day On which day of the week do commits take place? Depends-on : - Returns : map[str, int] Back to top","title":"trans.rascal.activecommitters.commitsPerWeekDay"},{"location":"user-guide/metrics/#transrascalactivecommitterscommittersemailstoday","text":"Short name : committersEmailsToday Friendly name : Active committers Who have been active today? Depends-on : - Returns : set[str] Back to top","title":"trans.rascal.activecommitters.committersEmailsToday"},{"location":"user-guide/metrics/#transrascalactivecommitterssizeofdevelopmentteam","text":"Short name : sizeOfDevelopmentTeam Friendly name : Size of development team How many people have ever contributed code to this project? Depends-on : trans.rascal.activecommitters.developmentTeam Returns : int Back to top","title":"trans.rascal.activecommitters.sizeOfDevelopmentTeam"},{"location":"user-guide/metrics/#transrascalactivecommittersnumberofactivecommitterslongterm","text":"Short name : numberOfActiveCommittersLongTerm Friendly name : Number of active committers long term Number of long time active committers over time (active in last year). This measures a smooth window of one year, where every day we report the number of developers active in the previous 365 days. Depends-on : trans.rascal.activecommitters.longerTermActiveCommitters Returns : int Back to top","title":"trans.rascal.activecommitters.numberOfActiveCommittersLongTerm"},{"location":"user-guide/metrics/#transrascalactivecommittersnumberofactivecommitters","text":"Short name : numberOfActiveCommitters Friendly name : Number of active committers Number of active committers over time (active in last two weeks). This measures a smooth window of two weeks, where every day we report the number of developers in the previous 14 days. Depends-on : trans.rascal.activecommitters.activeCommitters Returns : int Back to top","title":"trans.rascal.activecommitters.numberOfActiveCommitters"},{"location":"user-guide/metrics/#rascalgenericchurncommitstoday","text":"Short name : commitsToday Friendly name : Number of commits today Counts the number of commits made today. Depends-on : - Returns : int Back to top","title":"rascal.generic.churn.commitsToday"},{"location":"user-guide/metrics/#rascalgenericchurnchurntoday","text":"Short name : commitsToday Friendly name : Churn of today Counts the churn for today: the total number of lines of code added and deleted. This metric is used further downstream to analyze trends. Depends-on : - Returns : int Back to top","title":"rascal.generic.churn.churnToday"},{"location":"user-guide/metrics/#rascalgenericchurnchurnpercommitintwoweeks","text":"Short name : churnPerCommitInTwoWeeks Friendly name : Churn per commit in two weeks The ratio between the churn and the number of commits indicates how large each commit is on average. We compute this as a sliding average over two weeks which smoothens exceptions and makes it possible to see a trend historically. Commits should not be to big all the time, because that would indicate either that programmers are not focusing on well-defined tasks or that the system architecture does not allow for separation of concerns. Depends-on : rascal.generic.churn.churnInTwoWeeks rascal.generic.churn.commitsInTwoWeeks Returns : int Back to top","title":"rascal.generic.churn.churnPerCommitInTwoWeeks"},{"location":"user-guide/metrics/#rascalgenericchurnchurnactivity","text":"Short name : churnActivity Friendly name : Churn over the last two weeks Churn in the last two weeks: collects the lines of code added and deleted over a 14-day sliding window. Depends-on : rascal.generic.churn.churnToday Returns : rel[datetime,int] Back to top","title":"rascal.generic.churn.churnActivity"},{"location":"user-guide/metrics/#rascalgenericchurncommitactivity","text":"Short name : commitActivity Friendly name : Commits in last two weeks Number of commits in the last two weeks: collects commit activity over a 14-day sliding window. Depends-on : rascal.generic.churn.commitsToday Returns : rel[datetime,int] Back to top","title":"rascal.generic.churn.commitActivity"},{"location":"user-guide/metrics/#rascalgenericchurncorecommitterschurn","text":"Short name : coreCommittersChurn Friendly name : Churn per core committer Find out about the committers what their total number of added and deleted lines for this system. Depends-on : rascal.generic.churn.churnPerCommitter Returns : map[str, int] Back to top","title":"rascal.generic.churn.coreCommittersChurn"},{"location":"user-guide/metrics/#rascalgenericchurnfilespercommit","text":"Short name : numberOfFilesPerCommit Friendly name : Number of files per commit Counts the number of files per commit to find out about the separation of concerns in the architecture or in the tasks the programmers perform. This metric is used further downstream. Depends-on : - Returns : map[loc, int] Back to top","title":"rascal.generic.churn.filesPerCommit"},{"location":"user-guide/metrics/#rascalgenericchurnchurnpercommit","text":"Short name : churnPerCommit Friendly name : Counts number of lines added and deleted per commit. Count churn. Churn is the number lines added or deleted. We measure this per commit because the commit is a basic unit of work for a programmer. This metric computes a table per commit for today and is not used for comparison between projects. It is used further downstream to analyze activity. Depends-on : - Returns : map[loc, int] Back to top","title":"rascal.generic.churn.churnPerCommit"},{"location":"user-guide/metrics/#rascalgenericchurnchurnpercommitter","text":"Short name : churnPerCommitter Friendly name : Churn per committer Count churn per committer: the number of lines of code added and deleted. It zooms in on the single committer producing a table which can be used for downstream processing. Depends-on : - Returns : map[str author, int churn] Back to top","title":"rascal.generic.churn.churnPerCommitter"},{"location":"user-guide/metrics/#rascalgenericchurnchurnperfile","text":"Short name : churnPerFile Friendly name : Churn per file Churn per file counts the number of files added and deleted for a single file. This is a basic metric to indicate hotspots in the design of the system which is changed often. This metric is used further downstream. Depends-on : - Returns : map[loc file, int churn] Back to top","title":"rascal.generic.churn.churnPerFile"},{"location":"user-guide/metrics/#rascalgenericchurncommitsintwoweeks","text":"Short name : commitsInTwoWeeks Friendly name : Number of commits in the last two weeks Churn in the last two weeks: aggregates the number of commits over a 14-day sliding window. Depends-on : rascal.generic.churn.commitActivity Returns : int Back to top","title":"rascal.generic.churn.commitsInTwoWeeks"},{"location":"user-guide/metrics/#rascalgenericchurnchurnintwoweeks","text":"Short name : churnInTwoWeeks Friendly name : Sum of churn in the last two weeks Churn in the last two weeks: aggregates the lines of code added and deleted over a 14-day sliding window. Depends-on : rascal.generic.churn.churnActivity Returns : int Back to top","title":"rascal.generic.churn.churnInTwoWeeks"},{"location":"user-guide/metrics/#transient-metric-providers-for-generic-source-code","text":"These metrics are related to the source code of analyzed projects, regardless of the language(s) they are written in. Back to top","title":"Transient Metric Providers for Generic Source Code"},{"location":"user-guide/metrics/#transrascalreadabilityfilereadability","text":"Short name : fileReadability Friendly name : File readability Code readability per file, measured by use of whitespace measures deviations from common usage of whitespace in source code, such as spaces after commas. This is a basic collection metric which is used further downstream. Depends-on : - Returns : map[loc, real] Back to top","title":"trans.rascal.readability.fileReadability"},{"location":"user-guide/metrics/#transrascalreadabilityfilereadabilityquartiles","text":"Short name : fileReadabilityQ Friendly name : File readability quartiles We measure file readability by counting exceptions to common usage of whitespace in source code, such as spaces after commas. The quartiles represent how many of the files have how many of these deviations. A few deviations per file is ok, but many files with many deviations indicates a lack of attention to readability. Depends-on : trans.rascal.readability.fileReadability Returns : map[str, real] Back to top","title":"trans.rascal.readability.fileReadabilityQuartiles"},{"location":"user-guide/metrics/#transrascalcommentsheadercounts","text":"Short name : headerCounts Friendly name : Number of appearances of estimated unique headers In principle it is expected for the files in a project to share the same license. The license text in the header of each file may differ slightly due to different copyright years and or lists of contributors. The heuristic allows for slight differences. The metric produces the number of different types of header files found. A high number is a contra-indicator, meaning either a confusing licensing scheme or the source code of many different projects is included in the code base of the analyzed system. Depends-on : - Returns : list[int] Back to top","title":"trans.rascal.comments.headerCounts"},{"location":"user-guide/metrics/#transrascalcommentscommentedoutcode","text":"Short name : commentedOutCode Friendly name : Lines of commented out code per file Lines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how much source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator. Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.comments.commentedOutCode"},{"location":"user-guide/metrics/#transrascalcommentscommentloc","text":"Short name : commentLOC Friendly name : Number of lines containing comments per file Number of lines containing comments per file is a basic metric used for downstream processing. This metric does not consider the difference between natural language comments and commented out code. Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.comments.commentLOC"},{"location":"user-guide/metrics/#transrascalcommentscommentlinesperlanguage","text":"Short name : commentLinesPerLanguage Friendly name : Number of lines containing comments per language (excluding headers) Number of lines containing comments per language (excluding headers). The balance between comments and code indicates understandability. Too many comments are often not maintained and may lead to confusion, not enough means the code lacks documentation explaining its intent. This is a basic fact collection metric which is used further downstream. Depends-on : trans.rascal.comments.commentLOC trans.rascal.comments.headerLOC trans.rascal.comments.commentedOutCode Returns : map[str, int] Back to top","title":"trans.rascal.comments.commentLinesPerLanguage"},{"location":"user-guide/metrics/#transrascalcommentscommentedoutcodeperlanguage","text":"Short name : commentedOutCodePerLanguage Friendly name : Lines of commented out code per language Lines of commented out code per file uses heuristics (frequency of certain substrings typically used in code and not in natural language) to find out how much source code comments are actually commented out code. Commented out code is, in large quantities is a quality contra-indicator. Depends-on : trans.rascal.comments.commentedOutCode Returns : map[str, int] Back to top","title":"trans.rascal.comments.commentedOutCodePerLanguage"},{"location":"user-guide/metrics/#transrascalcommentsheaderloc","text":"Short name : headerLOC Friendly name : Header size per file Header size per file is a basic metric counting the size of the comment at the start of each file. It is used for further processing downstream. Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.comments.headerLOC"},{"location":"user-guide/metrics/#transrascalcommentsmatchinglicenses","text":"Short name : matchingLicenses Friendly name : Used licenses (from selected list of known licenses) We match against a list of known licenses to find out which are used in the current project Depends-on : - Returns : set[str] Back to top","title":"trans.rascal.comments.matchingLicenses"},{"location":"user-guide/metrics/#transrascalcommentsheaderpercentage","text":"Short name : headerPercentage Friendly name : Percentage of files with headers. Percentage of files with headers is an indicator for the amount of files which have been tagged with a copyright statement (or not). If the number is low this indicates a problem with the copyright of the program. Source files without a copyright statement are not open-source, they are owned, in principle, by the author and may not be copied without permission. Note that the existence of a header does not guarantee the presence of an open-source license, but its absence certainly is telling. Depends-on : trans.rascal.comments.headerLOC Returns : real Back to top","title":"trans.rascal.comments.headerPercentage"},{"location":"user-guide/metrics/#transrascallocgenericloc","text":"Short name : countLoc Friendly name : Language independent physical lines of code Physical lines of code simply counts the number of newline characters (OS independent) in a source code file. The metric can be used to compare the volume between two systems. Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.LOC.genericLOC"},{"location":"user-guide/metrics/#transrascallocgenericlocoverfiles","text":"Short name : giniLOCOverFiles Friendly name : Spread of code over files We find out how evenly the code is spread over files. The number should be quite stable over time. A jump in this metric indicates a large change in the code base. If the code is focused in only a few very large files then this may be a contra-indicator for quality. Depends-on : - Returns : real Back to top","title":"trans.rascal.LOC.genericLOCoverFiles"},{"location":"user-guide/metrics/#transrascalloclocperlanguage","text":"Short name : locPerLanguage Friendly name : Physical lines of code per language Physical lines of code simply counts the number of newline characters (OS independent) in a source code file. We accumulate this number per programming language. The metric can be used to compare the volume between two systems and to assess in which programming language the bulk of the code is written. Depends-on : trans.rascal.LOC.genericLOC Returns : map[str, int] Back to top","title":"trans.rascal.LOC.locPerLanguage"},{"location":"user-guide/metrics/#transrascalclonesclonelocperlanguage","text":"Short name : cloneLOCPerLanguage Friendly name : Lines of code in Type I clones larger than 6 lines, per language Lines of code in Type I clones larger than 6 lines, per language. A Type I clone is a literal clone. A large number of literal clones is considered to be bad. This metric is not easily compared between systems because it is not size normalized yet. We use it for further processing downstream. You can analyze the trend over time using this metric. Depends-on : - Returns : map[str, int]","title":"trans.rascal.clones.cloneLOCPerLanguage"},{"location":"user-guide/metrics/#transient-metric-providers-for-java-code","text":"These metrics are related to the Java source code of analyzed projects. Back to top","title":"Transient Metric Providers for Java Code"},{"location":"user-guide/metrics/#stylefileswitherrorproneness","text":"Short name : filesWithErrorProneness Friendly name : Files with style violations which make the code error prone. This is basic metric which can not be easily compared between projects. Percentage of files with error proneness Depends-on : style.errorProneness Returns : int Back to top","title":"style.filesWithErrorProneness"},{"location":"user-guide/metrics/#styleunderstandability","text":"Short name : understandability Friendly name : Inefficient code Percentage of the projects files with coding style violations which indicate the code may be hard to read and understand, but not necessarily more error prone. Depends-on : style.styleViolations Returns : Table Back to top","title":"style.understandability"},{"location":"user-guide/metrics/#styleinefficiencies","text":"Short name : inefficiencies Friendly name : Inefficient code Percentage of the projects files with coding style violations which indicate common inefficient ways of doing things in Java. Depends-on : style.styleViolations Returns : Table Back to top","title":"style.inefficiencies"},{"location":"user-guide/metrics/#stylefileswithunderstandabilityissues","text":"Short name : filesWithUnderstandabilityIssues Friendly name : Files with style violations which make the code harder to understand Percentage of files with understandability issues. This is a basic metric which can not be easily compared between projects. Depends-on : style.understandability Returns : int Back to top","title":"style.filesWithUnderstandabilityIssues"},{"location":"user-guide/metrics/#styleerrorproneness","text":"Short name : errorProneness Friendly name : Error proneness Percentage of the projects files with coding style violations which indicate error prone code. This is a basic metric which collects per file all the style violations, recording the line number and the kind of style violation. Each kind of violation is grouped into a category. The resulting table is hard to interpret manually and can not be compared between projects. Other metrics further downstream do aggregate this information. Depends-on : style.styleViolations Returns : Table Back to top","title":"style.errorProneness"},{"location":"user-guide/metrics/#stylespreadofstyleviolations","text":"Short name : spreadOfStyleViolations Friendly name : Spread of style violations over files Between 0 and 1 how evenly spread are the style violations. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.styleViolations Returns : real Back to top","title":"style.spreadOfStyleViolations"},{"location":"user-guide/metrics/#stylefileswithinefficiencies","text":"Short name : filesWithInefficiencies Friendly name : Files with style violations which indicate inefficiencies. This is a basic metric which can not be easily compared between projects. Percentage of files with inefficiencies Depends-on : style.inefficiencies Returns : int Back to top","title":"style.filesWithInefficiencies"},{"location":"user-guide/metrics/#stylefileswithstyleviolations","text":"Short name : filesWithStyleViolations Friendly name : Counts the number of files with any kind of style violation. This metric can not be easily compared between projects. Percentage of files with style violations Depends-on : style.styleViolations Returns : int Back to top","title":"style.filesWithStyleViolations"},{"location":"user-guide/metrics/#stylespreadofunderstandabilityissues","text":"Short name : spreadOfUnderstandabilityIssues Friendly name : Spread of understandability issues over files Between 0 and 1 how evenly spread are the understandability issues. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.understandability Returns : real Back to top","title":"style.spreadOfUnderstandabilityIssues"},{"location":"user-guide/metrics/#stylespreadofinefficiencies","text":"Short name : spreadOfInefficiencies Friendly name : Spread of inefficiencies over files Between 0 and 1 how evenly spread are the style violations which indicate inefficiencies. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.inefficiencies Returns : real Back to top","title":"style.spreadOfInefficiencies"},{"location":"user-guide/metrics/#stylestyleviolations","text":"Short name : styleViolations Friendly name : All style violations This is a basic metric which collects per file all the style violations, recording the line number and the kind of style violation. Each kind of violation is grouped into a category. The resulting table is hard to interpret manually and can not be compared between projects. Other metrics further downstream do aggregate this information. Depends-on : - Returns : Table Back to top","title":"style.styleViolations"},{"location":"user-guide/metrics/#stylespreadoferrorproneness","text":"Short name : spreadOfErrorProneness Friendly name : Spread of error proneness style violations over files Between 0 and 1 how evenly spread are the style violations which indicate error proneness. This metric makes sense if there are more than 5 files in a project and can be compared between projects as well. If problems are widespread this may be a quality contra-indicator, while a localized problem could be easily fixed. Depends-on : style.errorProneness Returns : real Back to top","title":"style.spreadOfErrorProneness"},{"location":"user-guide/metrics/#rascaltestabilityjavatestoverpublicmethods","text":"Short name : percentageOfTestedPublicMethods Friendly name : Number of JUnit tests averaged over the total number of public methods Number of JUnit tests averaged over the total number of public methods. Ideally all public methods are tested. With this number we compute how far from the ideal situation the project is. Depends-on : - Returns : real Back to top","title":"rascal.testability.java.TestOverPublicMethods"},{"location":"user-guide/metrics/#rascaltestabilityjavanumberoftestmethods","text":"Short name : numberOfTestMethods Friendly name : Number of JUnit test methods. This is an intermediate absolute metric used to compute others. The bare metric is hard to compare between projects. Number of JUnit test methods Depends-on : - Returns : int Back to top","title":"rascal.testability.java.NumberOfTestMethods"},{"location":"user-guide/metrics/#rascaltestabilityjavatestcoverage","text":"Short name : estimateTestCoverage Friendly name : Static Estimation of test coverage This is a static over-estimation of test coverage: which code is executed in the system when all JUnit test cases are executed? We approximate this by using the static call graphs and assuming every method which can be called, will be called. This leads to an over-approximation, as compared to a dynamic code coverage analysis, but the static analysis does follow the trend and a low code coverage here is an good indicator for a lack in testing effort for the project. Depends-on : - Returns : real Back to top","title":"rascal.testability.java.TestCoverage"},{"location":"user-guide/metrics/#transrascaloojavamif-java","text":"Short name : MIF_Java Friendly name : Method inheritance factor (Java) Method inheritance factor (Java) Depends-on : - Returns : map[loc, real] Back to top","title":"trans.rascal.OO.java.MIF-Java"},{"location":"user-guide/metrics/#transrascaloojavaca-java-quartiles","text":"Short name : Ca_Java_Q Friendly name : Afferent coupling quartiles (Java) Afferent coupling quartiles (Java) Depends-on : trans.rascal.OO.java.Ca-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.Ca-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavadac-java","text":"Short name : DAC_Java Friendly name : Data abstraction coupling (Java) Data abstraction coupling (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.DAC-Java"},{"location":"user-guide/metrics/#transrascaloojavacf-java","text":"Short name : CF_Java Friendly name : Coupling factor (Java) Coupling factor (Java) Depends-on : - Returns : real Back to top","title":"trans.rascal.OO.java.CF-Java"},{"location":"user-guide/metrics/#transrascaloojavai-java","text":"Short name : I_Java Friendly name : Instability (Java) Instability (Java) Depends-on : trans.rascal.OO.java.Ce-Java trans.rascal.OO.java.Ca-Java Returns : map[loc, real] Back to top","title":"trans.rascal.OO.java.I-Java"},{"location":"user-guide/metrics/#transrascaloojavadac-java-quartiles","text":"Short name : DAC_Java_Q Friendly name : Data abstraction coupling quartiles (Java) Data abstraction coupling quartiles (Java) Depends-on : trans.rascal.OO.java.DAC-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.DAC-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavampc-java-quartiles","text":"Short name : MPC_Java_Q Friendly name : Message passing coupling quartiles (Java) Message passing coupling quartiles (Java) Depends-on : trans.rascal.OO.java.MPC-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.MPC-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavanom-java","text":"Short name : NOM_Java Friendly name : Number of methods (Java) Number of methods (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.NOM-Java"},{"location":"user-guide/metrics/#transrascaloojavalcom-java","text":"Short name : LCOM_Java Friendly name : Lack of cohesion in methods (Java) Lack of cohesion in methods (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.LCOM-Java"},{"location":"user-guide/metrics/#transrascaloojavacbo-java","text":"Short name : CBO_Java Friendly name : Coupling between objects (Java) Coupling between objects (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.CBO-Java"},{"location":"user-guide/metrics/#transrascaloojavace-java","text":"Short name : Ce_Java Friendly name : Efferent coupling (Java) Efferent coupling (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.Ce-Java"},{"location":"user-guide/metrics/#transrascaloojavapf-java","text":"Short name : PF_Java Friendly name : Polymorphism factor (Java) Polymorphism factor (Java) Depends-on : - Returns : real Back to top","title":"trans.rascal.OO.java.PF-Java"},{"location":"user-guide/metrics/#transrascaloojavarfc-java-quartiles","text":"Short name : RFC_Java_Q Friendly name : Response for class quartiles (Java) Response for class quartiles (Java) Depends-on : trans.rascal.OO.java.RFC-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.RFC-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavai-java-quartiles","text":"Short name : I_Java_Q Friendly name : Instability quartiles (Java) Instability quartiles (Java) Depends-on : trans.rascal.OO.java.I-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.I-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavarfc-java","text":"Short name : RFC_Java Friendly name : Response for class (Java) Response for class (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.RFC-Java"},{"location":"user-guide/metrics/#transrascaloojavalcc-java","text":"Short name : LCC_Java Friendly name : Loose class cohesion (Java) Loose class cohesion (Java) Depends-on : - Returns : map[loc, real] Back to top","title":"trans.rascal.OO.java.LCC-Java"},{"location":"user-guide/metrics/#transrascaloojavamif-java-quartiles","text":"Short name : MIF_Java_Q Friendly name : Method inheritance factor quartiles (Java) Method inheritance factor quartiles (Java) Depends-on : trans.rascal.OO.java.MIF-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.MIF-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavadit-java","text":"Short name : DIT_Java Friendly name : Depth of inheritance tree (Java) Depth of inheritance tree (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.DIT-Java"},{"location":"user-guide/metrics/#transrascaloojavamhf-java","text":"Short name : MHF_Java Friendly name : Method hiding factor (Java) Method hiding factor (Java) Depends-on : - Returns : real Back to top","title":"trans.rascal.OO.java.MHF-Java"},{"location":"user-guide/metrics/#transrascaloojavatcc-java","text":"Short name : TCC_Java Friendly name : Tight class cohesion (Java) Tight class cohesion (Java) Depends-on : - Returns : map[loc, real] Back to top","title":"trans.rascal.OO.java.TCC-Java"},{"location":"user-guide/metrics/#transrascaloojavaahf-java","text":"Short name : AHF_Java Friendly name : Attribute hiding factor (Java) Attribute hiding factor (Java) Depends-on : - Returns : real Back to top","title":"trans.rascal.OO.java.AHF-Java"},{"location":"user-guide/metrics/#transrascaloojavalcom-java-quartiles","text":"Short name : LCOM_Java_Q Friendly name : Lack of cohesion in methods quartiles (Java) Lack of cohesion in methods quartiles (Java) Depends-on : trans.rascal.OO.java.LCOM-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.LCOM-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavaca-java","text":"Short name : Ca_Java Friendly name : Afferent coupling (Java) Afferent coupling (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.Ca-Java"},{"location":"user-guide/metrics/#transrascaloojavaa-java","text":"Short name : A_Java Friendly name : Abstractness (Java) Abstractness (Java) Depends-on : - Returns : real Back to top","title":"trans.rascal.OO.java.A-Java"},{"location":"user-guide/metrics/#transrascaloojavadit-java-quartiles","text":"Short name : DIT_Java_Q Friendly name : Depth of inheritance tree quartiles (Java) Depth of inheritance tree quartiles (Java) Depends-on : trans.rascal.OO.java.DIT-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.DIT-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavatcc-java-quartiles","text":"Short name : TCC_Java_Q Friendly name : Tight class cohesion quartiles (Java) Tight class cohesion quartiles (Java) Depends-on : trans.rascal.OO.java.TCC-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.TCC-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavalcom4-java-quartiles","text":"Short name : LCOM4_Java_Q Friendly name : Lack of cohesion in methods 4 quartiles (Java) Lack of cohesion in methods 4 quartiles (Java) Depends-on : trans.rascal.OO.java.LCOM4-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.LCOM4-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavalcom4-java","text":"Short name : LCOM4_Java Friendly name : Lack of cohesion in methods 4 (Java) Lack of cohesion in methods 4 (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.LCOM4-Java"},{"location":"user-guide/metrics/#transrascaloojavasr-java","text":"Short name : SR_Java Friendly name : Specialization ratio (Java) Specialization ratio (Java) Depends-on : - Returns : real Back to top","title":"trans.rascal.OO.java.SR-Java"},{"location":"user-guide/metrics/#transrascaloojavaaif-java-quartiles","text":"Short name : AIF_Java_Q Friendly name : Attribute inheritance factor quartiles (Java) Attribute inheritance factor quartiles (Java) Depends-on : trans.rascal.OO.java.AIF-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.AIF-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavanoc-java-quartiles","text":"Short name : NOC_Java_Q Friendly name : Number of children quartiles (Java) Number of children quartiles (Java) Depends-on : trans.rascal.OO.java.NOC-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.NOC-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavanoc-java","text":"Short name : NOC_Java Friendly name : Number of children (Java) Number of children (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.NOC-Java"},{"location":"user-guide/metrics/#transrascaloojavaaif-java","text":"Short name : AIF_Java Friendly name : Attribute inheritance factor (Java) Attribute inheritance factor (Java) Depends-on : - Returns : map[loc, real] Back to top","title":"trans.rascal.OO.java.AIF-Java"},{"location":"user-guide/metrics/#transrascaloojavarr-java","text":"Short name : RR_Java Friendly name : Reuse ratio (Java) Reuse ratio (Java) Depends-on : - Returns : real Back to top","title":"trans.rascal.OO.java.RR-Java"},{"location":"user-guide/metrics/#transrascaloojavalcc-java-quartiles","text":"Short name : LCC_Java_Q Friendly name : Loose class cohesion quartiles (Java) Loose class cohesion quartiles (Java) Depends-on : trans.rascal.OO.java.LCC-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.LCC-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavanoa-java","text":"Short name : NOA_Java Friendly name : Number of attributes (Java) Number of attributes (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.NOA-Java"},{"location":"user-guide/metrics/#transrascaloojavace-java-quartiles","text":"Short name : Ce_Java_Q Friendly name : Efferent coupling quartiles (Java) Efferent coupling quartiles (Java) Depends-on : trans.rascal.OO.java.Ce-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.Ce-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavanom-java-quartiles","text":"Short name : NOM_Java_Q Friendly name : Number of methods quartiles (Java) Number of methods quartiles (Java) Depends-on : trans.rascal.OO.java.NOM-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.NOM-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavanoa-java-quartiles","text":"Short name : NOA_Java_Q Friendly name : Number of attributes quartiles (Java) Number of attributes quartiles (Java) Depends-on : trans.rascal.OO.java.NOA-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.NOA-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavacbo-java-quartiles","text":"Short name : CBO_Java_Q Friendly name : Coupling between objects quartiles (Java) Coupling between objects quartiles (Java) Depends-on : trans.rascal.OO.java.CBO-Java Returns : map[str, real] Back to top","title":"trans.rascal.OO.java.CBO-Java-Quartiles"},{"location":"user-guide/metrics/#transrascaloojavampc-java","text":"Short name : MPC_Java Friendly name : Message passing coupling (Java) Message passing coupling (Java) Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.OO.java.MPC-Java"},{"location":"user-guide/metrics/#transrascallocjavalocoverjavaclass","text":"Short name : giniLOCOverClassJava Friendly name : Distribution of physical lines of code over Java classes, interfaces and enums The distribution of physical lines of code over Java classes, interfaces and enums explains how complexity is distributed over the design elements of a system. Depends-on : - Returns : real Back to top","title":"trans.rascal.LOC.java.LOCoverJavaClass"},{"location":"user-guide/metrics/#transrascaladvancedfeaturesjavaadvancedlanguagefeaturesjavaquartiles","text":"Short name : countUsesOfAdvancedLanguageFeaturesQ Friendly name : Usage of advanced Java features quartiles Quartiles of counts of advanced Java features (wildcards, union types and anonymous classes). The numbers indicate the thresholds that delimit the first 25%, 50% and 75% of the data as well as the maximum and minumum values. Depends-on : trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJava Returns : map[str, real] Back to top","title":"trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJavaQuartiles"},{"location":"user-guide/metrics/#transrascaladvancedfeaturesjavaadvancedlanguagefeaturesjava","text":"Short name : countUsesOfAdvancedLanguageFeatures Friendly name : Usage of advanced Java features Usage of advanced Java features (wildcards, union types and anonymous classes), reported per file and line number of the occurrence. This metric is for downstream processing by other metrics. Depends-on : - Returns : map[loc file, int count] Back to top","title":"trans.rascal.advancedfeatures.java.AdvancedLanguageFeaturesJava"},{"location":"user-guide/metrics/#transrascalccjavacchistogramjava","text":"Short name : CCHistogramJava Friendly name : Number of Java methods per CC risk factor Number of Java methods per CC risk factor, counts the number of methods which are in a low, medium or high risk factor. The histogram can be compared between projects to indicate which is probably easier to maintain on a method-by-method basis. Depends-on : trans.rascal.CC.java.CCJava Returns : map[str, int] Back to top","title":"trans.rascal.CC.java.CCHistogramJava"},{"location":"user-guide/metrics/#transrascalccjavaccoverjavamethods","text":"Short name : giniCCOverMethodsJava Friendly name : CC over Java methods Calculates how cyclomatic complexity is spread over the methods of a system. If high CC is localized, then this may be easily fixed but if many methods have high complexity, then the project may be at risk. This metric is good to compare between projects. Depends-on : trans.rascal.CC.java.CCJava Returns : real Back to top","title":"trans.rascal.CC.java.CCOverJavaMethods"},{"location":"user-guide/metrics/#transrascalccjavaccjava","text":"Short name : getCC Friendly name : McCabe's Cyclomatic Complexity Metric (Java) Cyclomatic complexity is a measure of the number of unique control flow paths in the methods of a class. This indicates how many different test cases you would need to test the method. A high number indicates also a lot of work to understand the method. This metric is a basic metric for further processing downstream. It is not easily compared between projects. Depends-on : - Returns : map[loc, int] Back to top","title":"trans.rascal.CC.java.CCJava"},{"location":"user-guide/metrics/#transrascalccjavawmcjava","text":"Short name : getWMC Friendly name : Weighted Method Count (Java) Cyclomatic complexity is a measure of the number of unique control flow paths in the methods of a class. This indicates how many different test cases you would need to test the method. A high number indicates also a lot of work to understand the method. The weighted method count for a class is the sum of the cyclomatic complexity measures of all methods in the class. This metric is a basic metric for further processing downstream. It is not easily compared between projects. Depends-on : trans.rascal.CC.java.CCJava Returns : map[loc class, int wmcCount] Back to top","title":"trans.rascal.CC.java.WMCJava"},{"location":"user-guide/metrics/#transient-metric-providers-for-osgi-dependencies","text":"These metrics are related to OSGi dependencies declared in MANIFEST.MF files. Back to top","title":"Transient Metric Providers for OSGi Dependencies"},{"location":"user-guide/metrics/#transrascaldependencynumberrequiredpackagesinsourcecode","text":"Short name : numberRequiredPackagesInSourceCode Friendly name : Number required packages in source code Retrieves the number of required packages found in the project source code. Depends-on : - Returns : int Back to top","title":"trans.rascal.dependency.numberRequiredPackagesInSourceCode"},{"location":"user-guide/metrics/#transrascaldependencyosgiallosgipackagedependencies","text":"Short name : allOSGiPackageDependencies Friendly name : All OSGi package dependencies Retrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). Depends-on : - Returns : set[loc] Back to top","title":"trans.rascal.dependency.osgi.allOSGiPackageDependencies"},{"location":"user-guide/metrics/#transrascaldependencyosgiunversionedosgirequiredbundles","text":"Short name : unversionedOSGiRequiredBundles Friendly name : Unversioned OSGi required bundles Retrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header). If returned value != {} there is a smell in the Manifest. Depends-on : - Returns : set[loc] Back to top","title":"trans.rascal.dependency.osgi.unversionedOSGiRequiredBundles"},{"location":"user-guide/metrics/#transrascaldependencyosgiunusedosgiimportedpackages","text":"Short name : unusedOSGiImportedPackages Friendly name : Unused OSGi imported packages Retrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell). Depends-on : - Returns : set[loc] Back to top","title":"trans.rascal.dependency.osgi.unusedOSGiImportedPackages"},{"location":"user-guide/metrics/#transrascaldependencyosginumberosgisplitimportedpackages","text":"Short name : numberOSGiSplitImportedPackages Friendly name : Number OSGi split imported packages Retrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest. Depends-on : - Returns : int Back to top","title":"trans.rascal.dependency.osgi.numberOSGiSplitImportedPackages"},{"location":"user-guide/metrics/#transrascaldependencyosgiratiounusedosgiimportedpackages","text":"Short name : ratioUnusedOSGiImportedPackages Friendly name : Ratio of unused OSGi imported packages Retrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages. Depends-on : trans.rascal.dependency.osgi.unusedOSGiImportedPackages Returns : real Back to top","title":"trans.rascal.dependency.osgi.ratioUnusedOSGiImportedPackages"},{"location":"user-guide/metrics/#transrascaldependencyosgiallosgibundledependencies","text":"Short name : allOSGiBundleDependencies Friendly name : All OSGi bundle dependencies Retrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies). Depends-on : - Returns : set[loc] Back to top","title":"trans.rascal.dependency.osgi.allOSGiBundleDependencies"},{"location":"user-guide/metrics/#transrascaldependencyosgiunversionedosgiexportedpackages","text":"Short name : unversionedOSGiExportedPackages Friendly name : Unversioned OSGi exported packages Retrieves the set of unversioned OSGi exported packages (declared in the Export-Package header). If returned value != {} there is a smell in the Manifest. Depends-on : - Returns : set[loc] Back to top","title":"trans.rascal.dependency.osgi.unversionedOSGiExportedPackages"},{"location":"user-guide/metrics/#transrascaldependencyosginumberosgisplitexportedpackages","text":"Short name : numberOSGiSplitExportedPackages Friendly name : Number OSGi split exported packages Retrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest. Depends-on : - Returns : int Back to top","title":"trans.rascal.dependency.osgi.numberOSGiSplitExportedPackages"},{"location":"user-guide/metrics/#transrascaldependencyosgiallosgidynamicimportedpackages","text":"Short name : allOSGiDynamicImportedPackages Friendly name : All OSGi dynamically imported packages Retrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file. Depends-on : - Returns : set[loc] Back to top","title":"trans.rascal.dependency.osgi.allOSGiDynamicImportedPackages"},{"location":"user-guide/metrics/#transrascaldependencyosginumberosgibundledependencies","text":"Short name : numberOSGiBundleDependencies Friendly name : Number all OSGi bundle dependencies Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies). Depends-on : - Returns : int Back to top","title":"trans.rascal.dependency.osgi.numberOSGiBundleDependencies"},{"location":"user-guide/metrics/#transrascaldependencyosgiratiounversionedosgiimportedpackages","text":"Short name : ratioUnversionedOSGiImportedPackages Friendly name : Ratio unversioned OSGi imported packages Retrieves the ratio of unversioned OSGi imported packages. Depends-on : trans.rascal.dependency.osgi.unversionedOSGiImportedPackages Returns : real Back to top","title":"trans.rascal.dependency.osgi.ratioUnversionedOSGiImportedPackages"},{"location":"user-guide/metrics/#transrascaldependencyosgiunversionedosgiimportedpackages","text":"Short name : unversionedOSGiImportedPackages Friendly name : Unversioned OSGi imported packages Retrieves the set of unversioned OSGi imported packages (declared in the Import-Package header). If returned value != {} there is a smell in the Manifest. Depends-on : - Returns : set[loc] Back to top","title":"trans.rascal.dependency.osgi.unversionedOSGiImportedPackages"},{"location":"user-guide/metrics/#transrascaldependencyosginumberosgipackagedependencies","text":"Short name : numberOSGiPackageDependencies Friendly name : Number of all OSGi package dependencies Retrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). Depends-on : - Returns : int Back to top","title":"trans.rascal.dependency.osgi.numberOSGiPackageDependencies"},{"location":"user-guide/metrics/#transrascaldependencyosgiratiounversionedosgirequiredbundles","text":"Short name : ratioUnversionedOSGiRequiredBundles Friendly name : Ratio unversioned OSGi required bundles Retrieves the ratio of unversioned OSGi required bundles. Depends-on : trans.rascal.dependency.osgi.unversionedOSGiRequiredBundles Returns : real Back to top","title":"trans.rascal.dependency.osgi.ratioUnversionedOSGiRequiredBundles"},{"location":"user-guide/metrics/#transrascaldependencyosgiusedosgiunimportedpackages","text":"Short name : usedOSGiUnimportedPackages Friendly name : Used OSGi unimported packages Retrieves the set of used but unimported packages. This metric does not consider packages implicitly imported through the Bundle-Require header. If set != {} then developers may be depending on the execution environment (smell). Depends-on : - Returns : set[loc] Back to top","title":"trans.rascal.dependency.osgi.usedOSGiUnimportedPackages"},{"location":"user-guide/metrics/#transrascaldependencyosgiratiounversionedosgiexportedpackages","text":"Short name : ratioUnversionedOSGiExportedPackages Friendly name : Ratio of unversioned OSGi exported packages Retrieves the ratio of unversioned OSGi exported packages. Depends-on : trans.rascal.dependency.osgi.unversionedOSGiExportedPackages Returns : real Back to top","title":"trans.rascal.dependency.osgi.ratioUnversionedOSGiExportedPackages"},{"location":"user-guide/metrics/#transrascaldependencyosgiratiousedosgiimportedpackages","text":"Short name : ratioUsedOSGiImportedPackages Friendly name : Ratio of used OSGi imported packages Retrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code. Depends-on : - Returns : real Back to top","title":"trans.rascal.dependency.osgi.ratioUsedOSGiImportedPackages"},{"location":"user-guide/metrics/#transient-metric-providers-for-maven-dependencies","text":"These metrics are related to Maven dependencies declared in pom.xml files. Back to top","title":"Transient Metric Providers for Maven dependencies"},{"location":"user-guide/metrics/#transrascaldependencynumberrequiredpackagesinsourcecode_1","text":"Short name : numberRequiredPackagesInSourceCode Friendly name : Number required packages in source code Retrieves the number of required packages found in the project source code. Depends-on : - Returns : int Back to top","title":"trans.rascal.dependency.numberRequiredPackagesInSourceCode"},{"location":"user-guide/metrics/#transrascaldependencymavenratiooptionalmavendependencies","text":"Short name : ratioOptionalMavenDependencies Friendly name : Ratio optional Maven dependencies Retrieves the ratio of optional Maven dependencies. Depends-on : trans.rascal.dependency.maven.allOptionalMavenDependencies Returns : real Back to top","title":"trans.rascal.dependency.maven.ratioOptionalMavenDependencies"},{"location":"user-guide/metrics/#transrascaldependencymavennumberuniquemavendependencies","text":"Short name : numberUniqueMavenDependencies Friendly name : Number unique Maven dependencies Retrieves the number of unique Maven dependencies. Depends-on : - Returns : int Back to top","title":"trans.rascal.dependency.maven.numberUniqueMavenDependencies"},{"location":"user-guide/metrics/#transrascaldependencymavenalloptionalmavendependencies","text":"Short name : allOptionalMavenDependencies Friendly name : All optional Maven dependencies Retrieves all the optional Maven dependencies. Depends-on : - Returns : set[loc] Back to top","title":"trans.rascal.dependency.maven.allOptionalMavenDependencies"},{"location":"user-guide/metrics/#transrascaldependencymavenisusingtycho","text":"Short name : isUsingTycho Friendly name : Is using Tycho Checks if the current project is a Tycho project. Depends-on : - Returns : bool Back to top","title":"trans.rascal.dependency.maven.isUsingTycho"},{"location":"user-guide/metrics/#transrascaldependencymavennumbermavendependencies","text":"Short name : numberMavenDependencies Friendly name : Number Maven dependencies Retrieves the number of Maven dependencies. Depends-on : - Returns : int Back to top","title":"trans.rascal.dependency.maven.numberMavenDependencies"},{"location":"user-guide/metrics/#transrascaldependencymavenallmavendependencies","text":"Short name : allMavenDependencies Friendly name : All Maven dependencies Retrieves all the Maven dependencies. Depends-on : - Returns : set[loc] Back to top","title":"trans.rascal.dependency.maven.allMavenDependencies"},{"location":"user-guide/metrics/#transient-metric-providers-for-docker-dependencies","text":"This metric is related to Docker dependencies declared in Dockerfiles. Back to top","title":"Transient Metric Providers for Docker Dependencies"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationdockerdependencies","text":"Short name : trans.configuration.docker.dependencies Friendly name : Dependencies declared in Dockerfiles Retrieves the names of the dependencies that are declared in the Dockerfiles of a project and additional information such as their version and type. Depends-on : None Returns : DockerDependency which contains: Variable Type dependencyName String dependencyVersion String type String subType String Back to top","title":"org.eclipse.scava.metricprovider.trans.configuration.docker.dependencies"},{"location":"user-guide/metrics/#transient-metric-providers-for-puppet-dependencies","text":"This metric is related to Puppet dependencies declared in Puppet manifests. Back to top","title":"Transient Metric Providers for Puppet Dependencies"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationpuppetdependencies","text":"Short name : trans.configuration.puppet.dependencies Friendly name : Dependencies declared in Puppet manifests Retrieves the names of the dependencies that are declared in the Puppet manifests of a project and additional information such as their version and type. Depends-on : None Returns : PuppetDependency which contains: Variable Type dependencyName String dependencyVersion String type String Back to top","title":"org.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies"},{"location":"user-guide/metrics/#transient-metric-providers-for-docker-smells","text":"This metric is related to Docker smells detected in Dockerfiles. Back to top","title":"Transient Metric Providers for Docker Smells"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationdockersmells","text":"Short name : trans.configuration.docker.smells Friendly name : Smells detected in Dockerfiles Detects the smells in the Dockerfiles of a project and additional information such as their reason, the file and the line that each smells is detected. Depends-on : None Returns : Smell which contains: Variable Type smellName String reason String code String fileName String line String Back to top","title":"org.eclipse.scava.metricprovider.trans.configuration.docker.smells"},{"location":"user-guide/metrics/#transient-metric-providers-for-puppet-smells","text":"These metrics are related to Puppet smells detected in Puppet manifests. Back to top","title":"Transient Metric Providers for Puppet Smells"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationpuppetdesignsmells","text":"Short name : trans.configuration.puppet.designsmells Friendly name : Design smells detected in Puppet manifests Detects the design smells in the Puppet manifests of a project and additional information such as their reason and the file that each smells is detected. Depends-on : None Returns : Smell which contains: Variable Type smellName String reason String fileName String Back to top","title":"org.eclipse.scava.metricprovider.trans.configuration.puppet.designsmells"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationpuppetimplementationsmells","text":"Short name : trans.configuration.puppet.implementationsmells Friendly name : Implementation smells detected in Puppet manifests Detects the implementation smells in the Puppet manifests of a project and additional information such as their reason, the file and the line that each smells is detected. Depends-on : None Returns : Smell which contains: Variable Type smellName String reason String fileName String line String Back to top","title":"org.eclipse.scava.metricprovider.trans.configuration.puppet.implementationsmells"},{"location":"user-guide/metrics/#transient-metric-providers-for-docker-antipatterns","text":"This metric is related to Docker antipatterns detected in Dockerfiles. Back to top","title":"Transient Metric Providers for Docker Antipatterns"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationdockerantipatterns","text":"Short name : trans.configuration.docker.antipatterns Friendly name : Antipatterns detected in Dockerfiles Detects the antipatterns in the Dockerfiles of a project and additional information such as their reason, the file and the line that each antipattern is detected and the commit and date that this antipattern is related. Depends-on : None Returns : DockerAntipattern which contains: Variable Type smellName String reason String code String fileName String line String commit String date String Back to top","title":"org.eclipse.scava.metricprovider.trans.configuration.docker.antipatterns"},{"location":"user-guide/metrics/#transient-metric-providers-for-puppet-antipatterns","text":"These metrics are related to Puppet antipatterns detected in Puppet manifests. Back to top","title":"Transient Metric Providers for Puppet Antipatterns"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationpuppetdesignantipatterns","text":"Short name : trans.configuration.puppet.designantipatterns Friendly name : Design antipatterns detected in Puppet manifests Detects the design antipatterns in the Puppet manifests of a project and additional information such as their reason, the file that each antipattern is detected and the commit and date that this antipattern is related. Depends-on : None Returns : DesignAntipattern which contains: Variable Type smellName String reason String fileName String commit String date String Back to top","title":"org.eclipse.scava.metricprovider.trans.configuration.puppet.designantipatterns"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationpuppetimplementationantipatterns","text":"Short name : trans.configuration.puppet.implementationantipatterns Friendly name : Implementation antipatterns detected in Puppet manifests Detects the implementation antipatterns in the Puppet manifests of a project and additional information such as their reason, the file and the line that each antipattern is detected and the commit and date that this antipattern is related. Depends-on : None Returns : Smell which contains: Variable Type smellName String reason String fileName String line String commit String date String org.eclipse.scava.metricprovider.trans.configuration.projects.relations Back to top","title":"org.eclipse.scava.metricprovider.trans.configuration.puppet.implementationantipatterns"},{"location":"user-guide/metrics/#transient-metric-providers-for-indexing","text":"These metrics facilitate data indexing unto the platform. Back to top","title":"Transient Metric Providers for Indexing"},{"location":"user-guide/metrics/#transient-metric-providers-for-projects-relations","text":"This metric is related to the relations between projects that are analysed at the platform. Back to top","title":"Transient Metric Providers for Projects Relations"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransconfigurationprojectsrelations","text":"Short name : trans.configuration.projects.relations Friendly name : Relations between projects Detects the relations between projects that are already analysed at the platform by determining if a project is used as dependency by another project. Depends-on : None Returns : ProjectRelation which contains: Variable Type relationName String dependencyType String Back to top","title":"org.eclipse.scava.metricprovider.trans.configuration.projects.relations"},{"location":"user-guide/metrics/#transient-metric-providers-for-new-versions","text":"These metrics are related to the new version of the dependencies of the projects that are analysed at the platform. Back to top","title":"Transient Metric Providers for New Versions"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewversiondocker","text":"Short name : trans.newversion.docker Friendly name : New versions of Docker dependencies Detects the new versions of dependencies of Docker based projects. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.docker.dependencies Returns : NewDockerVersion which contains: Variable Type packageName String oldVersion String newVersion String Back to top","title":"org.eclipse.scava.metricprovider.trans.newversion.docker"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewversionpuppet","text":"Short name : trans.newversion.puppet Friendly name : New versions of Puppet dependencies Detects the new versions of dependencies of Puppet based projects. Depends-on : org.eclipse.scava.metricprovider.trans.configuration.puppet.dependencies Returns : NewPuppetVersion which contains: Variable Type packageName String oldVersion String newVersion String Back to top","title":"org.eclipse.scava.metricprovider.trans.newversion.puppet"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewversionosgi","text":"Short name : trans.newversion.osgi Friendly name : New versions of OSGi dependencies Detects the new versions of dependencies of OSGi based projects. Depends-on : None Returns : NewOsgiVersion which contains: Variable Type packageName String oldVersion String newVersion String Back to top","title":"org.eclipse.scava.metricprovider.trans.newversion.osgi"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewversionmaven","text":"Short name : trans.newversion.maven Friendly name : New versions of Maven dependencies Detects the new versions of dependencies of Maven based projects. Depends-on : None Returns : NewMavenVersion which contains: Variable Type packageName String oldVersion String newVersion String Back to top","title":"org.eclipse.scava.metricprovider.trans.newversion.maven"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransindexingpreparation","text":"Short name : index preparation transmetric Friendly name : index preparation This identifies the metric(s) that have been chosen to be executed by the user in preparation for indexing (note: This is required to enable the indexing capabilities of the platform to be dynamic. Depends-on : None Returns : IndexPrepTransMetric which contains: Variable Type executedMetricProviders List<ExecutedMetricProviders> Additional Information : ExecutedMetricProviders : List<String> metricIdentifiers Back to top","title":"org.eclipse.scava.metricprovider.trans.indexing.preparation"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderindexingbugs","text":"Short name : bug indexing metric Friendly name : bug tracking system indexer This metric prepares and indexes documents relating to bug tracking systems. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : BugsIndexingMetric Back to top","title":"org.eclipse.scava.metricprovider.indexing.bugs"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderindexingcommits","text":"Short name : metricprovider.indexing.commits Friendly name : Commits indexer This metric prepares and indexes documents relating to commits. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : CommitsIndexingMetric Back to top","title":"org.eclipse.scava.metricprovider.indexing.commits"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderindexingcommunicationchannels","text":"Short name : communication channels indexing metric Friendly name : communication channels indexer This metric prepares and indexes documents relating to communication channels. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation Returns : CommunicationChannelsIndexingMetric Back to top","title":"org.eclipse.scava.metricprovider.indexing.communicationchannels"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderindexingdocumentation","text":"Short name : metricprovider.indexing.documentation Friendly name : Documentation indexer This metric prepares and indexes documents relating to documentation. Depends-on : org.eclipse.scava.metricprovider.trans.indexing.preparation , org.eclipse.scava.metricprovider.trans.documentation Returns : DocumentationIndexingMetric Back to top","title":"org.eclipse.scava.metricprovider.indexing.documentation"},{"location":"user-guide/metrics/#transient-metric-providers-for-api","text":"These transient metrics are related to the analysis and evolution of API Back to top","title":"Transient Metric Providers for API"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransmigrationissuesmaracas","text":"Short name : trans.migrationissuesmaracas Friendly name : Migration Issues Detection using Maracas This metric convert the changes found by Maracas into Regex useful for other metrics. Depends-on : trans.rascal.api.changedMethods Returns : MigrationIssueMaracasTransMetric which contains: Variable Type maracasMeasurements List<MaracasMeasurement> Additional Information : MaracasMeasurement : List<String> regex String change int lastUpdateDate Back to top","title":"org.eclipse.scava.metricprovider.trans.migrationissuesmaracas"},{"location":"user-guide/metrics/#factoids","text":"Factoids are plugins used to present data that has been mined and analysed using one or more historic and/or transient metric providers. Back to top","title":"Factoids"},{"location":"user-guide/metrics/#factoids-for-bug-trackers","text":"These factoids are related to bug tracking systems. Back to top","title":"Factoids for Bug Trackers"},{"location":"user-guide/metrics/#orgeclipsescavafactoidbugschannelusage","text":"Short name : factoid.bugs.channelusage Friendly name : Bug Tracker Usage data This plugin generates the factoid regarding usage data for bug trackers. For example, the total number of new bugs, comments or patches per year. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.newbugs , org.eclipse.scava.metricprovider.historic.bugs.comments , org.eclipse.scava.metricprovider.historic.bugs.patches Additional Information : Star rating information : 4 star number of bugs or comments > working days in a year (250) . 3 star 2 x number of bugs or comments > working days in a year (250) . 2 star 4 x number of bugs or comments > working days in a year (250) . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.bugs.channelusage"},{"location":"user-guide/metrics/#orgeclipsescavafactoidbugsemotion","text":"Short name : factoid.bugs.emotion Friendly name : Bug Tracker Emotions This plugin generates the factoid regarding emotions for bug trackers. For example, the percentage of positive, negative or surprise emotions expressed. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Anger, fear and sadness are considered negative while joy and love are considered positive. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.newbugs , org.eclipse.scava.metricprovider.historic.bugs.comments , org.eclipse.scava.metricprovider.historic.bugs.patches Additional Information : Star rating information : 4 star positive emotion percentage > 80 OR negative emotion percentage < 35 . 3 star positive emotion percentage > 65 OR negative emotion percentage < 50 . 2 star positive emotion percentage > 50 OR negative emotion percentage < 65 . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.bugs.emotion"},{"location":"user-guide/metrics/#orgeclipsescavafactoidbugshourly","text":"Short name : factoid.bugs.hourly Friendly name : Bug Tracker hourly data This plugin generates the factoid regarding hourly statistics for bug trackers. For example, the percentage of bugs, comments etc. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies Additional Information : Star rating information : 4 star maximum percentage of hourly comments > 2 x uniform percentage of comments per hour (100/24) . 3 star maximum percentage of hourly comments > 4 x uniform percentage of comments per hour (100/24) . 2 star maximum percentage of hourly comments > 6 x uniform percentage of comments per hour (100/24) . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.bugs.hourly"},{"location":"user-guide/metrics/#orgeclipsescavafactoidbugsresponsetime","text":"Short name : factoid.bugs.responsetime Friendly name : Bug Tracker Response Time This plugin generates the factoid regarding response time for bug trackers. This could be a cummulative average, yearly average etc. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.responsetime Additional Information : Star rating information : 4 star Zero(0) < yearly average response time < eight hours milliseconds (8 x 60 x 60 x 1000) . 3 star Zero(0) < yearly average response time < day milliseconds (3 x eight hour milliseconds) . 2 star Zero(0) < yearly average response time < week milliseconds (7 x week milliseconds) . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.bugs.responsetime"},{"location":"user-guide/metrics/#orgeclipsescavafactoidbugssentiment","text":"Short name : factoid.bugs.sentiment Friendly name : Bug Tracker Sentiment This plugin generates the factoid regarding sentiment for bug trackers. For example, the average sentiment in all bug trackers associated to a project. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment) Depends-on : org.eclipse.scava.metricprovider.historic.bugs.sentiment Additional Information : Star rating information : 4 star average sentiment > 0.5 OR thread end sentiment - thread begining sentiment > 0.25 && thread begining sentiment > 0.15 . 3 star average sentiment > 0.25 OR thread end sentiment - thread begining sentiment > 0.125 && thread begining sentiment > 0.0 . 2 star average sentiment > 0 OR thread end sentiment - thread begining sentiment > 0 . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.bugs.sentiment"},{"location":"user-guide/metrics/#orgeclipsescavafactoidbugsseverity","text":"Short name : factoid.bugs.severity Friendly name : Bug Tracker Severity This plugin generates the factoid regarding severity for bug trackers. For example, the number of bugs per severity level, the average sentiment for each severity etc. There are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. Also, blocker , critical and major are regarded as serious bugs. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.severity , org.eclipse.scava.metricprovider.historic.bugs.severitybugstatus , org.eclipse.scava.metricprovider.historic.bugs.severityresponsetime , org.eclipse.scava.metricprovider.historic.bugs.severitysentiment Additional Information : Star rating information : 1 star percentage of serious bugs > 50 . 2 star percentage of serious bugs > 25 . 3 star percentage of serious bugs > 12.5 . 4 star otherwise (i.e., fewer percentage of serious bugs). Back to top","title":"org.eclipse.scava.factoid.bugs.severity"},{"location":"user-guide/metrics/#orgeclipsescavafactoidbugssize","text":"Short name : factoid.bugs.size Friendly name : Bug Tracker Size This plugin generates the factoid regarding bug size for bug trackers. For example, the cumulative number of bug comments or patches. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.newbugs , org.eclipse.scava.metricprovider.historic.bugs.comments , org.eclipse.scava.metricprovider.historic.bugs.patches Additional Information : Star rating information : 4 star number of bugs or parches > 1000 OR number of comments > 10000 . 3 star 2 x number of bugs or parches > 1000 OR 2 x number of comments > 10000 . 2 star 4 x number of bugs or parches > 1000 OR 4 x number of comments > 10000 . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.bugs.size"},{"location":"user-guide/metrics/#orgeclipsescavafactoidbugsstatus","text":"Short name : factoid.bugs.status Friendly name : Bug Tracker Status This plugin generates the factoid regarding bug status for bug trackers. For example, the number of fixed bugs, duplicate bugs etc. There are 7 bug status labels (resolved, nonResolved, fixed, worksForMe, wontFix, invalid and duplicate). Depends-on : org.eclipse.scava.metricprovider.historic.bugs.status Additional Information : Star rating information : 4 star perventage of resolved bug > 75 . 3 star perventage of resolved bug > 50 . 2 star perventage of resolved bug > 25 . 1 star otherwise (i.e., very few resolved bugs) Back to top","title":"org.eclipse.scava.factoid.bugs.status"},{"location":"user-guide/metrics/#orgeclipsescavafactoidbugsthreadlength","text":"Short name : factoid.bugs.threadlength Friendly name : Bug Tracker Thread Length This plugin generates the factoid regarding bug thread length for bug trackers. For example, the average length of discussion associated to bugs. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.bugs Additional Information : Star rating information : 4 star Zero(0) < average comments < 5 . 3 star Zero(0) < average comments < 10 . 2 star Zero(0) < average comments < 20 . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.bugs.threadlength"},{"location":"user-guide/metrics/#orgeclipsescavafactoidbugsusers","text":"Short name : factoid.bugs.users Friendly name : Bug Tracker Users This plugin generates the factoid regarding users for bug trackers. For example, the average number of users associated to a project in a bug tracking system. Depends-on : org.eclipse.scava.metricprovider.historic.bugs.users , org.eclipse.scava.metricprovider.historic.bugs.bugs Additional Information : Star rating information : 4 star daily new users in last month > 8 x 0.25 OR daily active users in last month > 8 x 2.5 OR daily new users in last year > 4 x 0.25 OR daily active users in last year > 4 x 2.5*. 3 star daily new users in last month > 4 x 0.25 OR daily active users in last month > 4 x 2.5 OR daily new users in last year > 2 x 0.25 OR daily active users in last year > 2 x 2.5*. 2 star daily new users in last month > 2 x 0.25 OR daily active users in last month > 2 x 2.5 OR daily new users in last year > 0.25 OR daily active users in last year > 2.5 . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.bugs.users"},{"location":"user-guide/metrics/#orgeclipsescavafactoidbugsweekly","text":"Short name : factoid.bugs.weekly Friendly name : Bug Tracker Weekly This plugin generates the factoid regarding weekly user engagements for bug trackers. For example, the average number of bug comments per week. This can be used to present the most and least busy week in terms of engagement for a particular project. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.dailyrequestsreplies Additional Information : Star rating information : 4 star maximum percentage of weekly comments < 2 x uniform percentage of comments per week (100/7) . 3 star maximum percentage of weekly comments < 3 x uniform percentage of comments per week (100/7) . 2 star maximum percentage of weekly comments < 4 x uniform percentage of comments per week (100/7) . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.bugs.weekly"},{"location":"user-guide/metrics/#factoids-for-newsgroups-and-forums","text":"These factoids are related to communication channels. Back to top","title":"Factoids for Newsgroups and Forums"},{"location":"user-guide/metrics/#orgeclipsescavafactoidnewsgroupschannelusage","text":"Short name : factoid.newsgroups.channelusage Friendly name : Newsgroup Channel Usage This plugin generates the factoid regarding usage data for newsgroups. For example, the total number of new articles or threads per year. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.articles , org.eclipse.scava.metricprovider.historic.newsgroups.newthreads Additional Information : Star rating information : 4 star number of articles OR threads > working days in a year (250 ). 3 star 2 x number of articles OR threads > working days in a year (250) . 2 star 4 x number of articles OR threads > working days in a year (250) . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.newsgroups.channelusage"},{"location":"user-guide/metrics/#orgeclipsescavafactoidnewsgroupsemotion","text":"Short name : factoid.newsgroups.emotion Friendly name : Newsgroup Channel Emotion This plugin generates the factoid regarding emotions for newsgroups, such as the percentage of positive, negative or surprise emotions expressed. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Anger, fear and sadness are considered negative while joy and love are considered positive. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.emotions Additional Information : Star rating information : 4 star positive emotion percentage > 80 OR negative emotion percentage < 35 . 3 star positive emotion percentage > 65 OR negative emotion percentage < 50 . 2 star positive emotion percentage > 50 OR negative emotion percentage < 65 . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.newsgroups.emotion"},{"location":"user-guide/metrics/#orgeclipsescavafactoidnewsgroupshourly","text":"Short name : factoid.newsgroups.hourly Friendly name : Newsgroup Channel hourly data This plugin generates the factoid regarding hourly data for newsgroups, such as the percentage of articles, threads etc. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies Additional Information : Star rating information : 4 star maximum percentage of hourly articles > 2 x uniform percentage of articles per hour (100/24) . 3 star maximum percentage of hourly articles > 4 x uniform percentage of articles per hour (100/24) . 2 star maximum percentage of hourly articles > 6 x uniform percentage of articles per hour (100/24) . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.newsgroups.hourly"},{"location":"user-guide/metrics/#orgeclipsescavafactoidnewsgroupsresponsetime","text":"Short name : factoid.newsgroups.responsetime Friendly name : Newsgroup Channel Response Time This plugin generates the factoid regarding response time for newsgroups. This could be a cummulative average, yearly average etc. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.responsetime Additional Information : Star rating information : 4 star Zero(0) < yearly average response time < eight hours in milliseconds (8 x 60 x 60 x 1000) . 3 star Zero(0) < yearly average response time < day in milliseconds (3 x eight hours in milliseconds) . 2 star Zero(0) < yearly average response time < week in milliseconds (7 x day in milliseconds) . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.newsgroups.responsetime"},{"location":"user-guide/metrics/#orgeclipsescavafactoidnewsgroupssentiment","text":"Short name : factoid.newsgroups.sentiment Friendly name : Newsgroup Channel Sentiment This plugin generates the factoid regarding sentiments for newsgroups. For example, the average sentiment in all newsgroup channel associated to a project. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment) Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.sentiment Additional Information : Star rating information : 4 star average sentiment > 0.5 OR thread end sentiment - thread begining sentiment > 0.25 && thread begining sentiment > 0.15 . 3 star average sentiment > 0.25 OR thread end sentiment - thread begining sentiment > 0.125 && thread begining sentiment > 0.0 . 2 star average sentiment > 0 OR thread end sentiment - thread begining sentiment > 0 . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.newsgroups.sentiment"},{"location":"user-guide/metrics/#orgeclipsescavafactoidnewsgroupsseverity","text":"Short name : factoid.newsgroups.severity Friendly name : Newsgroup Channel Severity This plugin generates the factoid regarding severity for newsgroups. For example, the number of articles per severity level, the average sentiment for each severity etc. There are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Note: blocker , critical and major are regarded as serious bugs. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.severityresponsetime , org.eclipse.scava.metricprovider.historic.newsgroups.severity , org.eclipse.scava.metricprovider.historic.newsgroups.severitysentiment Additional Information : Star rating information : 1 star percentage of serious bugs > 50 . 2 star percentage of serious bugs > 25 . 3 star percentage of serious bugs > 12.5 . 4 star otherwise (i.e., fewer percentage of serious bugs). Back to top","title":"org.eclipse.scava.factoid.newsgroups.severity"},{"location":"user-guide/metrics/#orgeclipsescavafactoidnewsgroupssize","text":"Short name : factoid.newsgroups.size Friendly name : Newsgroup Channel Size This plugin generates the factoid regarding thread or article size for newsgroups. For example, the cummulative number of threads. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.articles , org.eclipse.scava.metricprovider.historic.newsgroups.newthreads Additional Information : Star rating information : 4 star number of threads > 1000 OR number of articles > 10000 . 3 star 2 x number of threads > 1000 OR 2 x number of articles > 10000 . 2 star 4 x number of threads > 1000 OR 4 x number of articles > 10000 . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.newsgroups.size"},{"location":"user-guide/metrics/#orgeclipsescavafactoidnewsgroupsstatus","text":"Short name : factoid.newsgroups.status Friendly name : Newsgroup Channel Status This plugin generates the factoid regarding thread or article status for newsgroups. For example, the number of requests and replies, unanswered threads etc. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.unansweredthreads , org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies , org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies.average Additional Information : Star rating information : 4 star perventage of replies > 75 . 3 star perventage of replies > 50 . 2 star perventage of replies > 25 . 1 star otherwise (i.e., very few replies) Back to top","title":"org.eclipse.scava.factoid.newsgroups.status"},{"location":"user-guide/metrics/#orgeclipsescavafactoidnewsgroupsthreadlength","text":"Short name : factoid.newsgroups.threadlength Friendly name : Newsgroup Channel Thread Length This plugin generates the factoid regarding thread length for newsgroups. For example, the average length of discussion per day, month etc. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.threads Additional Information : Star rating information : 4 star Zero(0) < average comments < 5 . 3 star Zero(0) < average comments < 10 . 2 star Zero(0) < average comments < 20 . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.newsgroups.threadlength"},{"location":"user-guide/metrics/#orgeclipsescavafactoidnewsgroupsusers","text":"Short name : factoid.newsgroups.users Friendly name : Newsgroup Channel Users This plugin generates the factoid regarding users for newsgroups. For example, the average number of users associated to a project in a newsgroup channel. Depends-on : org.eclipse.scava.metricprovider.historic.newsgroups.users , org.eclipse.scava.metricprovider.historic.newsgroups.threads Additional Information : Star rating information : 4 star daily new users in last month > 8 x 0.25 OR daily active users in last month > 8 x 2.5 OR daily new users in last year > 4 x 0.25 OR daily active users in last year > 4 x 2.5 . 3 star daily new users in last month > 4 x 0.25 OR daily active users in last month > 4 x 2.5 OR daily new users in last year > 2 x 0.25 OR daily active users in last year > 2 x 2.5 . 2 star daily new users in last month > 2 x 0.25 OR daily active users in last month > 2 x 2.5 OR daily new users in last year > 0.25 OR daily active users in last year > 2.5 . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.newsgroups.users"},{"location":"user-guide/metrics/#orgeclipsescavafactoidnewsgroupsweekly","text":"Short name : factoid.newsgroups.weekly Friendly name : Newsgroup Channel Weekly This plugin generates the factoid regarding weekly user engagement for newsgroups. For example, the average number of comments per week. This can be used to present the most and least busy week in terms of engagement for a particular project. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.dailyrequestsreplies Additional Information : Star rating information : 4 star maximum percentage of weekly articles < 2 x uniform percentage of articles per week (100/7) . 3 star maximum percentage of weekly articles < 3 x uniform percentage of articles per week (100/7) . 2 star maximum percentage of weekly articles < 4 x uniform percentage of articles per week (100/7) . 1 star otherwise Back to top","title":"org.eclipse.scava.factoid.newsgroups.weekly"},{"location":"user-guide/plugin/","text":"Eclipse Plugin","title":"Eclipse Plugin"},{"location":"user-guide/plugin/#eclipse-plugin","text":"","title":"Eclipse Plugin"},{"location":"user-guide/quickstart/","text":"Quick Start Guide \u201cHello world example\u201d with the aim of presenting an overall picture consisting of the steps presented here https://docs.google.com/presentation/d/1mugPrIqs6JqJhMmVt4x2cjKfpSxP-wwQFOCL9aWyV5I/edit#slide=id.g3a59249d20_2_211","title":"Quick Start Guide"},{"location":"user-guide/quickstart/#quick-start-guide","text":"\u201cHello world example\u201d with the aim of presenting an overall picture consisting of the steps presented here https://docs.google.com/presentation/d/1mugPrIqs6JqJhMmVt4x2cjKfpSxP-wwQFOCL9aWyV5I/edit#slide=id.g3a59249d20_2_211","title":"Quick Start Guide"},{"location":"user-guide/workflow/","text":"Workflow Engine Crossflow is a distributed data processing framework that supports dispensation of work across multiple opinionated and low-commitment workers. Docker Quick Start Pull container image from Docker Hub: docker pull crossminer/crossflow Startup container: docker run -it --rm -d --name crossflow -p 80:8080 -p 61616:61616 -p 61614:61614 -p 5672:5672 -p 61613:61613 -p 1883:1883 -p 8161:8161 -p 1099:1099 crossminer/crossflow:latest Access Crossflow web application: http://localhost/org.eclipse.scava.crossflow.web/ More details on running Crossflow with Docker are available here . Running from source To run Crossflow from source you will need Eclipse, Apache Tomcat and Apache Thrift. Brief instructions are provided below. Eclipse Start with a J2EE distribution from https://www.eclipse.org/downloads/packages/release/2018-09/r/eclipse-ide-java-ee-developers Install Emfatic from http://download.eclipse.org/emfatic/update/ (Untick the \"Group items by category\" check box) Install the Graphical Modelling Framework (GMF) Tooling SDK from http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ Install the following features from http://download.eclipse.org/epsilon/interim/ Epsilon Core Epsilon Core Develoment Tools Epsilon EMF Integration Epsilon GMF Integration Install Web Tools Platform SDK (WTP SDK) from http://download.eclipse.org/webtools/repository/photon Tomcat Download a copy of Tomcat from http://archive.apache.org/dist/tomcat/tomcat-9/v9.0.14/bin/apache-tomcat-9.0.14.zip Set up Tomcat in your Eclipse through the Servers view Thrift Install Apache Thrift (http://thrift.apache.org/) Standalone executable for Windows Homebrew for Mac Git Clone the https://github.com/crossminer/scava/ repository Switch to the crossflow branch Import all projects from the crossflow and the restmule folders Ivy We're using Apache Ivy for dependency management (i.e. so that we don't need to store jars in the repo) - Install the Ivy Eclipse plugin: http://www.apache.org/dist/ant/ivyde/updatesite - If Ivy doesn't run automatically, look for any projects that contain an ivy.xml, right-click and select Ivy -> Resolve Generating stuff You will need to run the ANT build-files below to generate stuff after you import all the crossflow and restmule projects. org.eclipse.scava.crossflow.tests/generate-all-tests.xml runs the Crossflow code generator against all models under /org.eclipse.scava.crossflow.tests/models org.eclipse.scava.crossflow.web/run-thrift.xml runs the Thrift code generator against crossflow.thrift to produce Java and JavaScript source code org.eclipse.scava.crossflow.web/build-war.xml builds a Tomcat WAR file from org.eclipse.scava.crossflow.web org.eclipse.scava.crossflow.examples/generate-all-examples.xml runs the Crossflow code generator against all models under /org.eclipse.scava.crossflow.examples/models Tests JUnit tests can be ran through the CrossflowTests class in org.eclipse.scava.crossflow.tests Web application To run the web application (port: 8080) right-click on org.eclipse.scava.crossflow.web and select Run as -> Run on Server The web app should be running on http://localhost:8080/org.eclipse.scava.crossflow.web/ Screenshots Figure : Main page listing available workflows and Upload New Workflow button. Figure : Calculator experiment page Advanced tab listing Calculator workflow configuration. Figure : Calculator experiment page Calculations tab listing Calculator workflow input calculations obtained from CSV source. Figure : Calculator experiment page Model tab listing Calculator workflow model. Figure : Calculator experiment page Log tab listing Calculator workflow log after experiment completion. Figure : Word Count experiment page Model tab listing Word Count workflow model before execution. Figure : Word Count experiment page Model tab listing Word Count workflow model during execution visualizing task status and queue size by means of color and rounded number, respectively. Task status (color) : STARTED (lightcyan), WAITING (skyblue), INPROGRESS (palegreen), BLOCKED (salmon), and FINISHED (slategrey). Figure : Word Count experiment page Model tab listing Word Count workflow model during execution with mouse hovering over initial queue depicting (queue) size, in-flight count, and subscriber count. Figure : Word Count experiment page Model tab listing Word Count workflow model during execution with mouse click inside empty model area, i.e. not on a particular task or queue, displaying context menu popup to clear the cache of all queues involved in the Word Count workflow. Figure : Word Count experiment page Model tab listing Word Count workflow model during execution with mouse click inside boundaries of WordFrequencies queue displaying context menu popup to clear the cache of all queues involved in the Word Count workflow. Figure : Upload New Workflow page allowing the upload and deployment of new experiments.","title":"Workflow Engine"},{"location":"user-guide/workflow/#workflow-engine","text":"Crossflow is a distributed data processing framework that supports dispensation of work across multiple opinionated and low-commitment workers.","title":"Workflow Engine"},{"location":"user-guide/workflow/#docker-quick-start","text":"Pull container image from Docker Hub: docker pull crossminer/crossflow Startup container: docker run -it --rm -d --name crossflow -p 80:8080 -p 61616:61616 -p 61614:61614 -p 5672:5672 -p 61613:61613 -p 1883:1883 -p 8161:8161 -p 1099:1099 crossminer/crossflow:latest Access Crossflow web application: http://localhost/org.eclipse.scava.crossflow.web/ More details on running Crossflow with Docker are available here .","title":"Docker Quick Start"},{"location":"user-guide/workflow/#running-from-source","text":"To run Crossflow from source you will need Eclipse, Apache Tomcat and Apache Thrift. Brief instructions are provided below.","title":"Running from source"},{"location":"user-guide/workflow/#eclipse","text":"Start with a J2EE distribution from https://www.eclipse.org/downloads/packages/release/2018-09/r/eclipse-ide-java-ee-developers Install Emfatic from http://download.eclipse.org/emfatic/update/ (Untick the \"Group items by category\" check box) Install the Graphical Modelling Framework (GMF) Tooling SDK from http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ Install the following features from http://download.eclipse.org/epsilon/interim/ Epsilon Core Epsilon Core Develoment Tools Epsilon EMF Integration Epsilon GMF Integration Install Web Tools Platform SDK (WTP SDK) from http://download.eclipse.org/webtools/repository/photon","title":"Eclipse"},{"location":"user-guide/workflow/#tomcat","text":"Download a copy of Tomcat from http://archive.apache.org/dist/tomcat/tomcat-9/v9.0.14/bin/apache-tomcat-9.0.14.zip Set up Tomcat in your Eclipse through the Servers view","title":"Tomcat"},{"location":"user-guide/workflow/#thrift","text":"Install Apache Thrift (http://thrift.apache.org/) Standalone executable for Windows Homebrew for Mac","title":"Thrift"},{"location":"user-guide/workflow/#git","text":"Clone the https://github.com/crossminer/scava/ repository Switch to the crossflow branch Import all projects from the crossflow and the restmule folders","title":"Git"},{"location":"user-guide/workflow/#ivy","text":"We're using Apache Ivy for dependency management (i.e. so that we don't need to store jars in the repo) - Install the Ivy Eclipse plugin: http://www.apache.org/dist/ant/ivyde/updatesite - If Ivy doesn't run automatically, look for any projects that contain an ivy.xml, right-click and select Ivy -> Resolve","title":"Ivy"},{"location":"user-guide/workflow/#generating-stuff","text":"You will need to run the ANT build-files below to generate stuff after you import all the crossflow and restmule projects. org.eclipse.scava.crossflow.tests/generate-all-tests.xml runs the Crossflow code generator against all models under /org.eclipse.scava.crossflow.tests/models org.eclipse.scava.crossflow.web/run-thrift.xml runs the Thrift code generator against crossflow.thrift to produce Java and JavaScript source code org.eclipse.scava.crossflow.web/build-war.xml builds a Tomcat WAR file from org.eclipse.scava.crossflow.web org.eclipse.scava.crossflow.examples/generate-all-examples.xml runs the Crossflow code generator against all models under /org.eclipse.scava.crossflow.examples/models","title":"Generating stuff"},{"location":"user-guide/workflow/#tests","text":"JUnit tests can be ran through the CrossflowTests class in org.eclipse.scava.crossflow.tests","title":"Tests"},{"location":"user-guide/workflow/#web-application","text":"To run the web application (port: 8080) right-click on org.eclipse.scava.crossflow.web and select Run as -> Run on Server The web app should be running on http://localhost:8080/org.eclipse.scava.crossflow.web/","title":"Web application"},{"location":"user-guide/workflow/#screenshots","text":"Figure : Main page listing available workflows and Upload New Workflow button. Figure : Calculator experiment page Advanced tab listing Calculator workflow configuration. Figure : Calculator experiment page Calculations tab listing Calculator workflow input calculations obtained from CSV source. Figure : Calculator experiment page Model tab listing Calculator workflow model. Figure : Calculator experiment page Log tab listing Calculator workflow log after experiment completion. Figure : Word Count experiment page Model tab listing Word Count workflow model before execution. Figure : Word Count experiment page Model tab listing Word Count workflow model during execution visualizing task status and queue size by means of color and rounded number, respectively. Task status (color) : STARTED (lightcyan), WAITING (skyblue), INPROGRESS (palegreen), BLOCKED (salmon), and FINISHED (slategrey). Figure : Word Count experiment page Model tab listing Word Count workflow model during execution with mouse hovering over initial queue depicting (queue) size, in-flight count, and subscriber count. Figure : Word Count experiment page Model tab listing Word Count workflow model during execution with mouse click inside empty model area, i.e. not on a particular task or queue, displaying context menu popup to clear the cache of all queues involved in the Word Count workflow. Figure : Word Count experiment page Model tab listing Word Count workflow model during execution with mouse click inside boundaries of WordFrequencies queue displaying context menu popup to clear the cache of all queues involved in the Word Count workflow. Figure : Upload New Workflow page allowing the upload and deployment of new experiments.","title":"Screenshots"}]}