{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Scava Documentation This web site is the main documentation place for the Eclipse Scava project. SCAVA Installation Guide The SCAVA installation guide provides instructions on how to install and configure the CORSSMINER Platform on a server and how to deploy the Eclipses Plugin in development environment. SCAVA User Guide The SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin. SCAVA Developers Guide The developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API. SCAVA Contributors Guide The SCAVA Contributors guide containes all the material related to the architecture of the platform which could be useful for projects members and external open sources contributors. Useful links: Eclipse Scava home project: Eclipse Scava @ Eclipse Eclipse Scava code repository: github.com/crossminer/scava Eclipse Scava deployment repository: github.com/crossminer/scava-deployment Eclipse Scava documentation repository: github.com/crossminer/scava-docs Old Stuf : Documentation which must be migrated on one of the platfomrs guides","title":"Home"},{"location":"#scava-documentation","text":"This web site is the main documentation place for the Eclipse Scava project.","title":"Scava Documentation"},{"location":"#scava-installation-guide","text":"The SCAVA installation guide provides instructions on how to install and configure the CORSSMINER Platform on a server and how to deploy the Eclipses Plugin in development environment.","title":"SCAVA Installation Guide"},{"location":"#scava-user-guide","text":"The SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin.","title":"SCAVA User Guide"},{"location":"#scava-developers-guide","text":"The developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API.","title":"SCAVA Developers Guide"},{"location":"#scava-contributors-guide","text":"The SCAVA Contributors guide containes all the material related to the architecture of the platform which could be useful for projects members and external open sources contributors.","title":"SCAVA Contributors Guide"},{"location":"#useful-links","text":"Eclipse Scava home project: Eclipse Scava @ Eclipse Eclipse Scava code repository: github.com/crossminer/scava Eclipse Scava deployment repository: github.com/crossminer/scava-deployment Eclipse Scava documentation repository: github.com/crossminer/scava-docs Old Stuf : Documentation which must be migrated on one of the platfomrs guides","title":"Useful links:"},{"location":"contributors-guide/","text":"SCAVA Contributors Guide The Contributors Guide summarize the material related to the architecture of the platform which could be useful for projects members and external open sources contributors. Contributors Guidelines Guidelines related to the organisation ofthe SCAVA developement process. SCAVA Repository Organisation SCAVA Development Process SCAVA Testing recommendations SCAVA Naming recommendations SCAVA Licensing recommendations Technical Guidelines Tutorial and Guidelignes related to the various technologies used by the SCAVA Platform How to configure the API Gateway in order to integrate a new REST service How to consume a SCAVA REST services How to implement Restlet services How to extend the SCAVA data model How to generate REST API Documentation How to access to MongoDB using PONGO Architecture Guidelines Guidelines related to the architecture of some components of the SCAVA Platform API Gateway Component Authentication Component","title":"SCAVA Contributors Guide"},{"location":"contributors-guide/#scava-contributors-guide","text":"The Contributors Guide summarize the material related to the architecture of the platform which could be useful for projects members and external open sources contributors.","title":"SCAVA Contributors Guide"},{"location":"contributors-guide/#contributors-guidelines","text":"Guidelines related to the organisation ofthe SCAVA developement process. SCAVA Repository Organisation SCAVA Development Process SCAVA Testing recommendations SCAVA Naming recommendations SCAVA Licensing recommendations","title":"Contributors Guidelines"},{"location":"contributors-guide/#technical-guidelines","text":"Tutorial and Guidelignes related to the various technologies used by the SCAVA Platform How to configure the API Gateway in order to integrate a new REST service How to consume a SCAVA REST services How to implement Restlet services How to extend the SCAVA data model How to generate REST API Documentation How to access to MongoDB using PONGO","title":"Technical Guidelines"},{"location":"contributors-guide/#architecture-guidelines","text":"Guidelines related to the architecture of some components of the SCAVA Platform API Gateway Component Authentication Component","title":"Architecture Guidelines"},{"location":"contributors-guide/architecture-guidelignes/","text":"Architecture Guidelines Guidelines related to the architecture of some components of the SCAVA Platform API Gateway Component The API Gateway Component provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application) and a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services. Authentication Component The Scava Authentication service provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform and user management services, including user registration process, user profile editing and roles based authorization management.","title":"Architecture Guidelines"},{"location":"contributors-guide/architecture-guidelignes/#architecture-guidelines","text":"Guidelines related to the architecture of some components of the SCAVA Platform API Gateway Component The API Gateway Component provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application) and a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services. Authentication Component The Scava Authentication service provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform and user management services, including user registration process, user profile editing and roles based authorization management.","title":"Architecture Guidelines"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/","text":"API Gateway component The Scava API Gateway : Provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application). Provide a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services. API Gateway Architecture The API Gateway is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change. The API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the API Gateway will manage the authentication for all services of the platform. Authentication Mechanism JSON Web Tokens The Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io) Authentication Architecture In Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway. Authentication Flow To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Implementation The implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures. https://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul API Gateway Configuration The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters. Server Configuration id : server.port default : 8086 Port of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port. JWT Security Configuration id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. id : apigateway.security.jwt.url default : /login URL Path of the authentication service. id : apigateway.security.jwt.expiration default : 86400 (24H) Port of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port. Routing : Authentication Service Configuration id : zuul.routes.auth-center.path default : /api/authentication/** Relative path of the authentication service. id : zuul.routes.auth-center.url default : NA URL of the authentification server. Example: http://127.0.0.1:8081/ id : zuul.routes.auth-center.sensitiveHeaders default : Cookie,Set-Cookie Specify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers. id : zuul.routes.auth-center.stripPrefix default : false Switch off the stripping of the service-specific prefix from individual routes Routing : Service Configuration id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1 Configuration file example #API Gateway Port server.port=8086 # JWT Configuration apigateway.security.jwt.secret=otherpeopledontknowit apigateway.security.jwt.url=/api/authentication apigateway.security.jwt.expiration=86400 # Rooting Configuration : Authentication Service zuul.routes.auth-center.path=/api/authentication/** zuul.routes.auth-center.url=http://127.0.0.1:8081/ zuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie zuul.routes.auth-center.stripPrefix=false # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2 Control access API The Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including: * \u201cROLE_ADMIN\u201d * \u201cROLE_PROJECT_MANAGER\u201d * \u201cROLE_USER\u201d By the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d. # Filtering private restApi scava.routes.config.adminAccessApi[0]=/api/users scava.routes.config.adminAccessApi[1]=/api/user/** scava.routes.config.projectManagerAccessApi[0]=/administration/projects/create scava.routes.config.projectManagerAccessApi[1]=/administration/projects/import scava.routes.config.projectManagerAccessApi[2]=/administration/analysis/** scava.routes.config.userAccessApi[0]=/administration/projects scava.routes.config.userAccessApi[1]=/administration/projects/p/** scava.routes.config.userAccessApi[2]=/api/users/** scava.routes.config.userAccessApi[3]=/api/account Packaging Form Sources Maven Packaging mvn -Pprod install API Gateway Execution complete an put the \"application.properties\" configuration file in the execute directory. Execute the crossmeter-api-gateway-1.0.0.jar Jar. java -jar scava-api-gateway-1.0.0.jar Client Implementation How to consume a Scava REST services ? \\ This guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.","title":"API Gateway component"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-component","text":"The Scava API Gateway : Provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application). Provide a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services.","title":"API Gateway component"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-architecture","text":"The API Gateway is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change. The API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the API Gateway will manage the authentication for all services of the platform.","title":"API Gateway Architecture"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#authentication-mechanism","text":"","title":"Authentication Mechanism"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#json-web-tokens","text":"The Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io)","title":"JSON Web Tokens"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#authentication-architecture","text":"In Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway.","title":"Authentication Architecture"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#authentication-flow","text":"To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service.","title":"Authentication Flow"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#implementation","text":"The implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures. https://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul","title":"Implementation"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-configuration","text":"The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.","title":"API Gateway Configuration"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#server-configuration","text":"id : server.port default : 8086 Port of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port.","title":"Server Configuration"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#jwt-security-configuration","text":"id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. id : apigateway.security.jwt.url default : /login URL Path of the authentication service. id : apigateway.security.jwt.expiration default : 86400 (24H) Port of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port.","title":"JWT Security Configuration"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#routing-authentication-service-configuration","text":"id : zuul.routes.auth-center.path default : /api/authentication/** Relative path of the authentication service. id : zuul.routes.auth-center.url default : NA URL of the authentification server. Example: http://127.0.0.1:8081/ id : zuul.routes.auth-center.sensitiveHeaders default : Cookie,Set-Cookie Specify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers. id : zuul.routes.auth-center.stripPrefix default : false Switch off the stripping of the service-specific prefix from individual routes","title":"Routing : Authentication Service Configuration"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#routing-service-configuration","text":"id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1","title":"Routing : Service Configuration"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#configuration-file-example","text":"#API Gateway Port server.port=8086 # JWT Configuration apigateway.security.jwt.secret=otherpeopledontknowit apigateway.security.jwt.url=/api/authentication apigateway.security.jwt.expiration=86400 # Rooting Configuration : Authentication Service zuul.routes.auth-center.path=/api/authentication/** zuul.routes.auth-center.url=http://127.0.0.1:8081/ zuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie zuul.routes.auth-center.stripPrefix=false # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2","title":"Configuration file example"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#control-access-api","text":"The Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including: * \u201cROLE_ADMIN\u201d * \u201cROLE_PROJECT_MANAGER\u201d * \u201cROLE_USER\u201d By the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d. # Filtering private restApi scava.routes.config.adminAccessApi[0]=/api/users scava.routes.config.adminAccessApi[1]=/api/user/** scava.routes.config.projectManagerAccessApi[0]=/administration/projects/create scava.routes.config.projectManagerAccessApi[1]=/administration/projects/import scava.routes.config.projectManagerAccessApi[2]=/administration/analysis/** scava.routes.config.userAccessApi[0]=/administration/projects scava.routes.config.userAccessApi[1]=/administration/projects/p/** scava.routes.config.userAccessApi[2]=/api/users/** scava.routes.config.userAccessApi[3]=/api/account","title":"Control access API"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#packaging-form-sources","text":"Maven Packaging mvn -Pprod install","title":"Packaging Form Sources"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#api-gateway-execution","text":"complete an put the \"application.properties\" configuration file in the execute directory. Execute the crossmeter-api-gateway-1.0.0.jar Jar. java -jar scava-api-gateway-1.0.0.jar","title":"API Gateway Execution"},{"location":"contributors-guide/architecture-guidelignes/api-gateway/#client-implementation","text":"How to consume a Scava REST services ? \\ This guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.","title":"Client Implementation"},{"location":"contributors-guide/architecture-guidelignes/authentication/","text":"Authentication Component The Scava Authentication service: Provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform. Provides user management services, including user registration process, user profile editing and roles based authorization management. Authentication API The Authentication server is a component of The Scava platform which manages the authentication for all services accessible behind the API Gateway. Authenticate User POST /api/authentication Login as a registered user. ### JSON Web Tokens (JWT) JSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are: * Header * Payload * Signature This solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/). ### JWT Authentication Implementation * Users have to login to the authentication service API using their credentials username and password. curl -i -X POST -H \"Content-Type:application/json\" http://localhost:8086/api/authentication -d '{\"username\":\"admin\", \"password\": \"admin\"}' * Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header. * The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway. curl -i -X GET -H \"Content-Type:application/json\" -H \"Authorization:Bearer eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1dGhvcml0aWVzIjpbIlJPTEVfQURNSU4iLCJST0xFX1BST0pFQ1RfTUFOQUdFUiIsIlJPTEVfVVNFUiJdLCJpYXQiOjE1MzE4OTk3NDMsImV4cCI6MTUzMTk4NjE0M30.l-iCJcnae-1mlhMb3_y09HM4HZYFaHxe_JctWi2FRUY\" http://localhost:8086/api/users ## User Management API The Authentication component provides web services for CRUD user account. Register User POST /api/register Register new user. Activate User GET /api/activate Activate the registered user. Update User PUT /api/users Update an existing user. Retrieve Users GET /api/users Get all registered users. Retrieve Login User GET /api/users/{login} Get the \"login\" user. Delete User DELETE /api/users/{login} Delete the \"login\" user. Authentication Server Configuration The Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component. Server Configuration id : server.port default : 8085 Port of the Authentication API server. Each REST request sent to the gateway must be adressed to this port. JWT Security Configuration id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. Default ADMIN configuration Property Description Default Value scava.administration.username The administrator username admin scava.administration.password The administrator password admin scava.administration.admin-role The admin role ADMIN scava.administration.project-manager-role The project manager role PROJECT_MANAGER scava.administration.project-user-role The user role USER Mongodb Database Configuration Property Description Default Value spring.data.mongodb.uri Url of the MongoDB database server mongodb://localhost:27017 spring.data.mongodb.database Name of the MongoDB database scava Mail Server configuration In order to register new users, you have to configure a mail server. Property Description Default Value spring.mail.host Url of the mail service smtp.gmail.com spring.mail.port Port of the mail service 587 spring.mail.username Login of the mail account spring.mail.password Password of the mail account spring.mail.protocol mail protocole smtp spring.mail.tls - true spring.mail.properties.mail.smtp.auth - true spring.mail.properties.mail.smtp.starttls.enable - true spring.mail.properties.mail.smtp.ssl.trust= - smtp.gmail.com Administration Dashboard Setting id : scava.administration.base-url default : http://localhost:4200 The SCAVA administration base URL to generate the activation account URL. Packaging From Sources Maven Packaging mvn -Pprod install Authentication Server Execution complete an put the \"application.properties\" configuration file in the execution directory. Execute the scava-auth-service-1.0.0.jar Jar. java -jar scava-auth-service-1.0.0.jar","title":"Authentication Component"},{"location":"contributors-guide/architecture-guidelignes/authentication/#authentication-component","text":"The Scava Authentication service: Provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform. Provides user management services, including user registration process, user profile editing and roles based authorization management.","title":"Authentication Component"},{"location":"contributors-guide/architecture-guidelignes/authentication/#authentication-api","text":"The Authentication server is a component of The Scava platform which manages the authentication for all services accessible behind the API Gateway. Authenticate User POST /api/authentication Login as a registered user. ### JSON Web Tokens (JWT) JSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are: * Header * Payload * Signature This solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/). ### JWT Authentication Implementation * Users have to login to the authentication service API using their credentials username and password. curl -i -X POST -H \"Content-Type:application/json\" http://localhost:8086/api/authentication -d '{\"username\":\"admin\", \"password\": \"admin\"}' * Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header. * The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway. curl -i -X GET -H \"Content-Type:application/json\" -H \"Authorization:Bearer eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1dGhvcml0aWVzIjpbIlJPTEVfQURNSU4iLCJST0xFX1BST0pFQ1RfTUFOQUdFUiIsIlJPTEVfVVNFUiJdLCJpYXQiOjE1MzE4OTk3NDMsImV4cCI6MTUzMTk4NjE0M30.l-iCJcnae-1mlhMb3_y09HM4HZYFaHxe_JctWi2FRUY\" http://localhost:8086/api/users ## User Management API The Authentication component provides web services for CRUD user account. Register User POST /api/register Register new user. Activate User GET /api/activate Activate the registered user. Update User PUT /api/users Update an existing user. Retrieve Users GET /api/users Get all registered users. Retrieve Login User GET /api/users/{login} Get the \"login\" user. Delete User DELETE /api/users/{login} Delete the \"login\" user.","title":"Authentication API"},{"location":"contributors-guide/architecture-guidelignes/authentication/#authentication-server-configuration","text":"The Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component.","title":"Authentication Server Configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#server-configuration","text":"id : server.port default : 8085 Port of the Authentication API server. Each REST request sent to the gateway must be adressed to this port.","title":"Server Configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#jwt-security-configuration","text":"id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA.","title":"JWT Security Configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#default-admin-configuration","text":"Property Description Default Value scava.administration.username The administrator username admin scava.administration.password The administrator password admin scava.administration.admin-role The admin role ADMIN scava.administration.project-manager-role The project manager role PROJECT_MANAGER scava.administration.project-user-role The user role USER","title":"Default ADMIN configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#mongodb-database-configuration","text":"Property Description Default Value spring.data.mongodb.uri Url of the MongoDB database server mongodb://localhost:27017 spring.data.mongodb.database Name of the MongoDB database scava","title":"Mongodb Database Configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#mail-server-configuration","text":"In order to register new users, you have to configure a mail server. Property Description Default Value spring.mail.host Url of the mail service smtp.gmail.com spring.mail.port Port of the mail service 587 spring.mail.username Login of the mail account spring.mail.password Password of the mail account spring.mail.protocol mail protocole smtp spring.mail.tls - true spring.mail.properties.mail.smtp.auth - true spring.mail.properties.mail.smtp.starttls.enable - true spring.mail.properties.mail.smtp.ssl.trust= - smtp.gmail.com","title":"Mail Server configuration"},{"location":"contributors-guide/architecture-guidelignes/authentication/#administration-dashboard-setting","text":"id : scava.administration.base-url default : http://localhost:4200 The SCAVA administration base URL to generate the activation account URL.","title":"Administration Dashboard Setting"},{"location":"contributors-guide/architecture-guidelignes/authentication/#packaging-from-sources","text":"Maven Packaging mvn -Pprod install","title":"Packaging From Sources"},{"location":"contributors-guide/architecture-guidelignes/authentication/#authentication-server-execution","text":"complete an put the \"application.properties\" configuration file in the execution directory. Execute the scava-auth-service-1.0.0.jar Jar. java -jar scava-auth-service-1.0.0.jar","title":"Authentication Server Execution"},{"location":"contributors-guide/contributors-guidelignes/","text":"Contributors Guidelines Guidelines related to the organisation ofthe SCAVA developement process. SCAVA Repository Organisation SCAVA Development Process SCAVA Testing recommendations SCAVA Naming recommendations SCAVA Licensing recommendations","title":"Contributors Guidelines"},{"location":"contributors-guide/contributors-guidelignes/#contributors-guidelines","text":"Guidelines related to the organisation ofthe SCAVA developement process. SCAVA Repository Organisation SCAVA Development Process SCAVA Testing recommendations SCAVA Naming recommendations SCAVA Licensing recommendations","title":"Contributors Guidelines"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/","text":"SCAVA Development Process Introduction This document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development. Process overview The development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system. It is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests. The different elements shown below are individually described in the remaining of the section. Figure 1. Overview of the CROSSMINER development process and tools. Source Code Repository Different branches will be created in the repository. In particular, the master branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The dev branch contains the most recent code integrated by partners before release. Other branches will be created for the different features/components, which will be eventually merged in the dev branch. The name of each branch contains the id of the issue it is supposed to implement. All source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository. Code: https://github.com/crossminer/scava Product documentation: https://github.com/crossminer/scava-docs Product deployment: https://github.com/crossminer/scava-deployment Since all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details). By taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3. Figure 2: Branching model Tests Each branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below. Figure 3: Explanatory master and branches BRANCH: Set of branches TEST: Set of tests INTGRATION_TEST < TEST UNIT_TEST < TEST function test: BRANCH -> TEST test(master) -> test(cool-feature) \u22c3 {itu, \u2026 itz} Continuous Integration Server Continuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus reduce cost and time . A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all unit tests . Integration tests are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities: Allow developers to write unit tests that can be executed automatically Perform automated tests against newly written code Show a list of tests that have passed and failed Quality analysis on source code Perform all the necessary actions to create a fully functioning build of the software when all tests have passed The currently used CI server is available at http://ci5.castalia.camp:8080/ Development and Production environments According to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform. Naming and change conventions The repository will contain the code of the high-level components shown in Table 1 Components Corresponding folder in repository Leader DevOps Dashboard BIT Workflow Diagram Editor /crossflow YORK Administration Web Application /administration SFT IDE /eclipse-based-ide FEA API Gateway /api-gateway SFT DevOps Backend BIT Knowledge Base /knowledge-base UDA Project Analyser /metric-platform SFT Data Collector /metric-platform SFT Project Scheduler /metric-platform SFT Metric Providers /metric-platform YORK, CWI, AUEB, EHU Data Storage /metric-platform SFT Web-based dashboards /web-dashboards BIT Table 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes Figure 4: Scava components architecture For each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus: when developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons; when a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR. The partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder: readme.md : this \ufb01le will contain the entry-point to all documentation for the component. Jenkinsfile : in order to compile the component, run tests, etc on the CI server. test : this is a folder containing the unit and/or integration tests. As a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base The following items are typical practices when applying CI: * Maintain a code repository * Automate the build * Make the build self-testing * Everyone commits to the baseline every day * Every commit (to baseline) should be built * Keep the build fast * Test in a clone of the production environment * Make it easy to get the latest deliverables * Everyone can see the results of the latest build * Automate deployment Communication and collaboration means The development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of: Slack channel: http://crossminer.slack.com Eclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev GitHub repository for documentation: https://github.com/crossminer/scava-docs/","title":"SCAVA Development Process"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#scava-development-process","text":"","title":"SCAVA Development Process"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#introduction","text":"This document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development.","title":"Introduction"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#process-overview","text":"The development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system. It is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests. The different elements shown below are individually described in the remaining of the section. Figure 1. Overview of the CROSSMINER development process and tools.","title":"Process overview"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#source-code-repository","text":"Different branches will be created in the repository. In particular, the master branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The dev branch contains the most recent code integrated by partners before release. Other branches will be created for the different features/components, which will be eventually merged in the dev branch. The name of each branch contains the id of the issue it is supposed to implement. All source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository. Code: https://github.com/crossminer/scava Product documentation: https://github.com/crossminer/scava-docs Product deployment: https://github.com/crossminer/scava-deployment Since all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details). By taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3. Figure 2: Branching model","title":"Source Code Repository"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#tests","text":"Each branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below. Figure 3: Explanatory master and branches BRANCH: Set of branches TEST: Set of tests INTGRATION_TEST < TEST UNIT_TEST < TEST function test: BRANCH -> TEST test(master) -> test(cool-feature) \u22c3 {itu, \u2026 itz}","title":"Tests"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#continuous-integration-server","text":"Continuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus reduce cost and time . A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all unit tests . Integration tests are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities: Allow developers to write unit tests that can be executed automatically Perform automated tests against newly written code Show a list of tests that have passed and failed Quality analysis on source code Perform all the necessary actions to create a fully functioning build of the software when all tests have passed The currently used CI server is available at http://ci5.castalia.camp:8080/","title":"Continuous Integration Server"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#development-and-production-environments","text":"According to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform.","title":"Development and Production environments"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#naming-and-change-conventions","text":"The repository will contain the code of the high-level components shown in Table 1 Components Corresponding folder in repository Leader DevOps Dashboard BIT Workflow Diagram Editor /crossflow YORK Administration Web Application /administration SFT IDE /eclipse-based-ide FEA API Gateway /api-gateway SFT DevOps Backend BIT Knowledge Base /knowledge-base UDA Project Analyser /metric-platform SFT Data Collector /metric-platform SFT Project Scheduler /metric-platform SFT Metric Providers /metric-platform YORK, CWI, AUEB, EHU Data Storage /metric-platform SFT Web-based dashboards /web-dashboards BIT Table 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes Figure 4: Scava components architecture For each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus: when developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons; when a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR. The partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder: readme.md : this \ufb01le will contain the entry-point to all documentation for the component. Jenkinsfile : in order to compile the component, run tests, etc on the CI server. test : this is a folder containing the unit and/or integration tests. As a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base The following items are typical practices when applying CI: * Maintain a code repository * Automate the build * Make the build self-testing * Everyone commits to the baseline every day * Every commit (to baseline) should be built * Keep the build fast * Test in a clone of the production environment * Make it easy to get the latest deliverables * Everyone can see the results of the latest build * Automate deployment","title":"Naming and change conventions"},{"location":"contributors-guide/contributors-guidelignes/scava-developement-process/#communication-and-collaboration-means","text":"The development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of: Slack channel: http://crossminer.slack.com Eclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev GitHub repository for documentation: https://github.com/crossminer/scava-docs/","title":"Communication and collaboration means"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/","text":"SCAVA Licensing recommendations Content The Scava project is licensed under Eclipse Public License - v 2.0 license. As consequence of our status of project hosted by the eclipse foundation, all Scava components must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header. \"Eclipse Public License\" licensing file The text below must be integrated to the root folder of your project on a text file name \"LICENSE\". Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership. Source File header All sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...). Optionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization. Example of Java license header file /******************************************************************************* * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"} * This program and the accompanying materials are made * available under the terms of the Eclipse Public License 2.0 * which is available at https://www.eclipse.org/legal/epl-2.0/ * * SPDX-License-Identifier: EPL-2.0 ******************************************************************************/ Comment n Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files. This plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.","title":"SCAVA Licensing recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#scava-licensing-recommendations","text":"","title":"SCAVA Licensing recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#content","text":"The Scava project is licensed under Eclipse Public License - v 2.0 license. As consequence of our status of project hosted by the eclipse foundation, all Scava components must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header.","title":"Content"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#eclipse-public-license-licensing-file","text":"The text below must be integrated to the root folder of your project on a text file name \"LICENSE\". Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership.","title":"\"Eclipse Public License\" licensing file"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#source-file-header","text":"All sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...). Optionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization. Example of Java license header file /******************************************************************************* * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"} * This program and the accompanying materials are made * available under the terms of the Eclipse Public License 2.0 * which is available at https://www.eclipse.org/legal/epl-2.0/ * * SPDX-License-Identifier: EPL-2.0 ******************************************************************************/","title":"Source File header"},{"location":"contributors-guide/contributors-guidelignes/scava-licensing-recomendation/#comment","text":"n Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files. This plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.","title":"Comment"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/","text":"SCAVA Naming recommendations Component Naming As consequence of our status of project hosted by the eclipse foundation, we have to follow a specific naming schema for all components implemented in context of SCAVA project. In this section, \"component\" means big funcional componen of the SCAVA project. Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration Project Naming For Eclipse Plugin org.eclipse.scava.{componentname} For Maven Project : the name of the project if the ArtifactId If your component is composed of one project : {component-name} If your component is composed of several sub projects : {sub-component-name} ``` ## Source Code Namsespace All sources must be nemspaces by : org.eclipse.scava.{componentname} ## Maven Ids For the Maven projects: * If your component is composed of one project : Group Id : org.eclipse.scava ArtifactId : {component-name} * If your component is composed of several sub projects : Group Id : org.eclipse.scava.{component-name} ArtifactId : {sub-component-name} ``` REST services Naming The REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of platform services , we need to used a common naming schema for all REST services provided by the platform. How to name a REST service ? /{ componentid }/{ categoryname }/{ servicename } / componentid / : Name of the Architectural component which provide the service / categoryname / (Optional) : optional category of the service / servicename / : Name of the rest service Component Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration","title":"SCAVA Naming recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#scava-naming-recommendations","text":"","title":"SCAVA Naming recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#component-naming","text":"As consequence of our status of project hosted by the eclipse foundation, we have to follow a specific naming schema for all components implemented in context of SCAVA project. In this section, \"component\" means big funcional componen of the SCAVA project. Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration","title":"Component Naming"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#project-naming","text":"For Eclipse Plugin org.eclipse.scava.{componentname} For Maven Project : the name of the project if the ArtifactId If your component is composed of one project : {component-name} If your component is composed of several sub projects : {sub-component-name} ``` ## Source Code Namsespace All sources must be nemspaces by : org.eclipse.scava.{componentname} ## Maven Ids For the Maven projects: * If your component is composed of one project : Group Id : org.eclipse.scava ArtifactId : {component-name} * If your component is composed of several sub projects : Group Id : org.eclipse.scava.{component-name} ArtifactId : {sub-component-name} ```","title":"Project Naming"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#rest-services-naming","text":"The REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of platform services , we need to used a common naming schema for all REST services provided by the platform.","title":"REST services Naming"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#how-to-name-a-rest-service","text":"/{ componentid }/{ categoryname }/{ servicename } / componentid / : Name of the Architectural component which provide the service / categoryname / (Optional) : optional category of the service / servicename / : Name of the rest service","title":"How to name a REST service ?"},{"location":"contributors-guide/contributors-guidelignes/scava-naming-recomendations/#component","text":"Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration","title":"Component"},{"location":"contributors-guide/contributors-guidelignes/scava-repository-organisation/","text":"SCAVA Repository Organisation The SCAVA code repository is organized by functional components with one package for each of this components. General organisation metric-platform platform : Core projects of the metric platform platform-extensions : Extensions of the metric platform metric-providers : Metric Providers implementations projects factoids : Factoids implementations projects tests : Test projects related to the metric-platform web-dashboards : DevOps Dashboard and DevOpsDashboard components knowledge-base : Knowledge Base implementation Workflow Execution Engine : Workflow Execution Engine implementation eclipse-based-ide : SCAVA Eclipse plugin api-gateway : API Gateway implementation. administration : Administration component implementation mingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project.","title":"SCAVA Repository Organisation"},{"location":"contributors-guide/contributors-guidelignes/scava-repository-organisation/#scava-repository-organisation","text":"The SCAVA code repository is organized by functional components with one package for each of this components.","title":"SCAVA Repository Organisation"},{"location":"contributors-guide/contributors-guidelignes/scava-repository-organisation/#general-organisation","text":"metric-platform platform : Core projects of the metric platform platform-extensions : Extensions of the metric platform metric-providers : Metric Providers implementations projects factoids : Factoids implementations projects tests : Test projects related to the metric-platform web-dashboards : DevOps Dashboard and DevOpsDashboard components knowledge-base : Knowledge Base implementation Workflow Execution Engine : Workflow Execution Engine implementation eclipse-based-ide : SCAVA Eclipse plugin api-gateway : API Gateway implementation. administration : Administration component implementation mingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project.","title":"General organisation"},{"location":"contributors-guide/contributors-guidelignes/scava-testing-organisation/","text":"SCAVA Testing recommendations Knowledge Base This builds needs some configuration to run successfully. In the application.properties files: * /knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties * /knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties Edit the following parameters: lucene.index.folder=/tmp/scava_lucene/ egit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57 To generate the GitHub access token you need to go to your own GitHub account and create a new one. Once it's done simply restart the tests, they should pass.","title":"SCAVA Testing recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-testing-organisation/#scava-testing-recommendations","text":"","title":"SCAVA Testing recommendations"},{"location":"contributors-guide/contributors-guidelignes/scava-testing-organisation/#knowledge-base","text":"This builds needs some configuration to run successfully. In the application.properties files: * /knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties * /knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties Edit the following parameters: lucene.index.folder=/tmp/scava_lucene/ egit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57 To generate the GitHub access token you need to go to your own GitHub account and create a new one. Once it's done simply restart the tests, they should pass.","title":"Knowledge Base"},{"location":"contributors-guide/technical-guidelignes/","text":"Technical Guidelignes Tutorial and Guidelignes related to the various technologies used by the SCAVA Platform How to configure the API Gateway in order to integrate a new REST service How to consume a SCAVA REST services How to implement Restlet services How to extend the SCAVA data model How to generate REST API Documentation How to access to MongoDB using PONGO","title":"Technical Guidelignes"},{"location":"contributors-guide/technical-guidelignes/#technical-guidelignes","text":"Tutorial and Guidelignes related to the various technologies used by the SCAVA Platform How to configure the API Gateway in order to integrate a new REST service How to consume a SCAVA REST services How to implement Restlet services How to extend the SCAVA data model How to generate REST API Documentation How to access to MongoDB using PONGO","title":"Technical Guidelignes"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/","text":"How to access to MongoDB using PONGO When to use ? This guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas. Context The Scava platform use a MongoDb data base to store his data. We go through PONGO , a template based Java POJO generator to access MongoDB database. With Pongo we can define the data model which generates strongly-typed Java classes. In this guideligne, we will present : * The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform. * The access to MongoDb Document on from an external Java application. * How to preform basic CRUD operation with a PONGO Java data model. We consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model : Extend MongoDB Data Model . You want to access to MongoDB Document from an Eclipse Plugin ? 1. Add a dependency to the Java Data Model Edit the plugin.xml file of your plugin. In Dependency section, add a dependency to the plugin which contained the data model you went to access. To org.ossmeter.repository.model to access data related to project administration , metric execution process and authentification system. To org.ossmeter.repository.model.'project delta manager' to access configuration informations related to source codes managemeny tools. To specific metric provider plugins to access data related to a specific metric provider implementation contains his once data model. ... others plugin which contained the data model In Dependency section, add a dependency to com.googlecode.pongo.runtime plugin 2. Initiate a Connection to the MongoDb In context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base. In Dependency section of plugin.xml file , add a dependency to the org.ossmeter.platform plugin. You can now create a new connection to the database using the Configuration service. Mongo mongo = Configuration.getInstance().getMongoConnection(); You want to access to MongoDB Document on from an External Java Application ? 1. Add a dependency to the Java Data Model Add to your java project a dependency to the jar which contained the data model you went to access. You will have to deliver this jar with your application. Add o your java project a dependency to the pongo.jar jar file which can be download at this url : https://github.com/kolovos/pongo/releases 2. Initiate a Connection to MongoDb // Define ServerAddress of the MongoDb database List<ServerAddress> mongoHostAddresses = new ArrayList<>(); mongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1]))); // Create Connection Mongo mongo = new Mongo(mongoHostAddresses); Once the connection to MongoDb has been created, you have to make the link between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document. DB db = mongo.getDB(\"databasename\"); // Initiate the Project Java model ProjectRepository = new ProjectRepository(db); Basic CRUD with a PONGO Java data model 1. CREATE // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true); 2. READ // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to access object properties metricprovider.getname(); ...... 3. UPDATE // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true); 4. DELETE Mongo mongo = new Mongo(); mongo.dropDatabase(\"databasename\");","title":"How to access to MongoDB using PONGO"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#how-to-access-to-mongodb-using-pongo","text":"","title":"How to access to MongoDB using PONGO"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#when-to-use","text":"This guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas.","title":"When to use ?"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#context","text":"The Scava platform use a MongoDb data base to store his data. We go through PONGO , a template based Java POJO generator to access MongoDB database. With Pongo we can define the data model which generates strongly-typed Java classes. In this guideligne, we will present : * The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform. * The access to MongoDb Document on from an external Java application. * How to preform basic CRUD operation with a PONGO Java data model. We consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model : Extend MongoDB Data Model .","title":"Context"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#you-want-to-access-to-mongodb-document-from-an-eclipse-plugin","text":"","title":"You want to access to MongoDB Document from an Eclipse Plugin ?"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#1-add-a-dependency-to-the-java-data-model","text":"Edit the plugin.xml file of your plugin. In Dependency section, add a dependency to the plugin which contained the data model you went to access. To org.ossmeter.repository.model to access data related to project administration , metric execution process and authentification system. To org.ossmeter.repository.model.'project delta manager' to access configuration informations related to source codes managemeny tools. To specific metric provider plugins to access data related to a specific metric provider implementation contains his once data model. ... others plugin which contained the data model In Dependency section, add a dependency to com.googlecode.pongo.runtime plugin","title":"1. Add a dependency to the Java Data Model"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#2-initiate-a-connection-to-the-mongodb","text":"In context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base. In Dependency section of plugin.xml file , add a dependency to the org.ossmeter.platform plugin. You can now create a new connection to the database using the Configuration service. Mongo mongo = Configuration.getInstance().getMongoConnection();","title":"2. Initiate a Connection to the MongoDb"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#you-want-to-access-to-mongodb-document-on-from-an-external-java-application","text":"","title":"You want to access to MongoDB Document on from an External Java Application ?"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#1-add-a-dependency-to-the-java-data-model_1","text":"Add to your java project a dependency to the jar which contained the data model you went to access. You will have to deliver this jar with your application. Add o your java project a dependency to the pongo.jar jar file which can be download at this url : https://github.com/kolovos/pongo/releases","title":"1. Add a dependency to the Java Data Model"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#2-initiate-a-connection-to-mongodb","text":"// Define ServerAddress of the MongoDb database List<ServerAddress> mongoHostAddresses = new ArrayList<>(); mongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1]))); // Create Connection Mongo mongo = new Mongo(mongoHostAddresses); Once the connection to MongoDb has been created, you have to make the link between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document. DB db = mongo.getDB(\"databasename\"); // Initiate the Project Java model ProjectRepository = new ProjectRepository(db);","title":"2. Initiate a Connection to MongoDb"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#basic-crud-with-a-pongo-java-data-model","text":"","title":"Basic CRUD with a PONGO Java data model"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#1-create","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true);","title":"1. CREATE"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#2-read","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to access object properties metricprovider.getname(); ......","title":"2. READ"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#3-update","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true);","title":"3. UPDATE"},{"location":"contributors-guide/technical-guidelignes/access-mongodb-pongo/#4-delete","text":"Mongo mongo = new Mongo(); mongo.dropDatabase(\"databasename\");","title":"4. DELETE"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/","text":"How to configure the API Gateway in order to integrate a new REST service When to use this guideline ? This guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API. Context The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters. Routing : Service Configuration To reference a new remote REST API in the gateway, you have to add 2 new properties in the application.properties configuration file : the relative path of services which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process. Examples : * api gateway url = http://85.36.10.13:8080 * path = /administration/** * url = http://85.36.10.12:8082/administration The request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1 Configuration file example # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2","title":"How to configure the API Gateway in order to integrate a new REST service"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/#how-to-configure-the-api-gateway-in-order-to-integrate-a-new-rest-service","text":"","title":"How to configure the API Gateway in order to integrate a new REST service"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/#when-to-use-this-guideline","text":"This guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API.","title":"When to use this guideline ?"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/#context","text":"The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.","title":"Context"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/#routing-service-configuration","text":"To reference a new remote REST API in the gateway, you have to add 2 new properties in the application.properties configuration file : the relative path of services which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process. Examples : * api gateway url = http://85.36.10.13:8080 * path = /administration/** * url = http://85.36.10.12:8082/administration The request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1","title":"Routing : Service Configuration"},{"location":"contributors-guide/technical-guidelignes/api-gateway-configuration/#configuration-file-example","text":"# Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2","title":"Configuration file example"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/","text":"How to consume a SCAVA REST services When to use ? This guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications. REST API Reference The reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]]. API Gateway The Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway. All web service request form clients have to go through the gateway. The api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform. Platform Authentication The CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io). 1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. 1. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Authentication in Java Retrieve a Web Tokens from authentication service private String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException { // Authentication Service URI URL url = new URL(\"http://localhost:8086/api/authentication\"); // AUthentication Request HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setDoOutput(true); connection.setRequestMethod(\"POST\"); connection.setRequestProperty(\"Content-Type\", \"application/json\"); String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\"; OutputStream os = connection.getOutputStream(); os.write(input.getBytes()); os.flush(); if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) { throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode()); } connection.disconnect(); // A JWT Token is return in the Header of the response return connection.getHeaderField(\"Authorization\"); } REST Service Call curl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication Service Consumption To consume a REST service provided by the integrated platform, the client must include the Token in the header of his request. ```java // Service URL URL url = new URL(\"http://localhost:8086/api/users\"); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setRequestMethod(\"GET\"); // Add Token to the request header connection.setRequestProperty(\"Authorization\",token);","title":"How to consume a SCAVA REST services"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#how-to-consume-a-scava-rest-services","text":"","title":"How to consume a SCAVA REST services"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#when-to-use","text":"This guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications.","title":"When to use ?"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#rest-api-reference","text":"The reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]].","title":"REST API Reference"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#api-gateway","text":"The Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway. All web service request form clients have to go through the gateway. The api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform.","title":"API Gateway"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#platform-authentication","text":"The CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io). 1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. 1. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Authentication in Java Retrieve a Web Tokens from authentication service private String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException { // Authentication Service URI URL url = new URL(\"http://localhost:8086/api/authentication\"); // AUthentication Request HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setDoOutput(true); connection.setRequestMethod(\"POST\"); connection.setRequestProperty(\"Content-Type\", \"application/json\"); String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\"; OutputStream os = connection.getOutputStream(); os.write(input.getBytes()); os.flush(); if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) { throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode()); } connection.disconnect(); // A JWT Token is return in the Header of the response return connection.getHeaderField(\"Authorization\"); } REST Service Call curl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication","title":"Platform Authentication"},{"location":"contributors-guide/technical-guidelignes/consume-scava-rest-services/#service-consumption","text":"To consume a REST service provided by the integrated platform, the client must include the Token in the header of his request. ```java // Service URL URL url = new URL(\"http://localhost:8086/api/users\"); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setRequestMethod(\"GET\"); // Add Token to the request header connection.setRequestProperty(\"Authorization\",token);","title":"Service Consumption"},{"location":"contributors-guide/technical-guidelignes/rest-api-doc-generation/","text":"How to generate REST API Documentation REST API Tutorial files Install First of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install. Execute the following steps. Clone repository: https://github.com/patrickneubauer/crossminer-workflow Import projects from the cloned repo to empty Eclipse workspace: Import root via Maven > Existing Maven Projects , then the rest via General > Existing projects into Workspace without checking Search for nested projects . Install new software packages: Install Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling [web page] Install Epsilon (Stable): http://download.eclipse.org/epsilon/updates/ [update site] Update Epsilon (Interim): http://download.eclipse.org/epsilon/interim/ [update site] Install Emfatic: http://download.eclipse.org/emfatic/update/ [update site] Note: I didn't see the the feature until Group items by category was unchecked. Install GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ [update site] Import the given json projects ( org.eclipse.epsilon.emc.json and org.eclipse.epsilon.emc.json.dt ) into the workspace via General > Existing projects into Workspace . Now right-click on org.eclipse.crossmeter.workflow project and select Maven > Update Project.... Copy the give M2M_Environment_oxygen.launch (for Windows 10) to org.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator and overwrite the old one. Refresh project org.eclipse.crossmeter.workflow.restmule.generator in Eclipse. Before running the .launch file, right-click on it, Run as > Run configurations.... Here go to Plug-ins tab and click on Add Required Plug-ins . Now you can run the .launch file TBA:How? In the Runtime Eclipse import the org.eclipse.crossmeter.workflow.restmule.generator project as Maven Project. Run generateFromOAS.launch as you can see in the videos https://youtu.be/BJXuozHJPeg. Now, you should see it generating the project. For further steps, watch Patrick's videos . github client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg github client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI github client generation example \u2014 https://youtu.be/ltSNnSZRETA kafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc kafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4 kafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA MDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk Example I've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following: /square/{number} . You can replace the {number} with any number. The repsonse for this request is a JSON object, which looks like this: { \"squared\": {squaredValue} } It contains only one field, named squared , which has the value of the square of the given number. From the specification the generator will provide us a Java Project which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines: ITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings). int number = 7; //The number to be squared. IData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server. NumberValue numberValue; //NumberValue is the class which represents the object received from the server numberValue = squared.observe().blockingSingle(); //Get the actually received object from the response. System.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer. Everything, including the IEntityApi interface and the NumberValue class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through Java to the objects/field inside of the received JSON object. And once again, they are all generated from the OpenAPI specification. So, to generate the mentioned Java Project , you have to put your OpenAPI specification file into the schemas folder inside the org.eclipse.crossmeter.workflow.restmule.generator project. I'll call mine as TestAPI.json , and it's made up of the following: { \"swagger\": \"2.0\", \"schemes\": [ \"http\" ], \"host\": \"localhost:8080\", \"basePath\": \"/\", \"info\": { \"description\": \"This is a test API, only for demonstration of the generator.\", \"termsOfService\": \"\", \"title\": \"TestAPI\", \"version\": \"v1\" }, \"consumes\": [ \"application/json\" ], \"produces\": [ \"application/json\" ], \"securityDefinitions\": { }, \"paths\": { \"/square/{number}\": { \"get\": { \"description\": \"Square a number.\", \"parameters\": [ { \"description\": \"The number to be squared\", \"in\": \"path\", \"name\": \"number\", \"required\": true, \"type\": \"integer\" } ], \"responses\": { \"200\": { \"description\": \"OK\", \"schema\": { \"$ref\": \"#/definitions/NumberValue\" } } } } } }, \"definitions\": { \"NumberValue\": { \"properties\": { \"squared\": { \"type\": \"integer\" } }, \"type\": \"object\" } } } You can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own. Then we have to set the generator to use our new schema. To do this, open the build.xml in the generator project and modify the api and json.model.file property to testapi and schemas/TestAPI.json , respectively. <!--API Variables --> <property name=\"api\" value=\"testapi\" /> <property name=\"json.model.file\" value=\"schemas/TestAPI.json\" /> We are almost done, but we have to take one more small step. We have to provide an .eol file in the epsilon/util/fix/ folder in the generator project. Its name must be the same as the value of the api property in the previous step, so for this example we use the testapi.eol . The content of this file: import \"../restmule.eol\"; var api = RestMule!API.all.first(); // RATE LIMITS var search = new RestMule!RatePolicyScope; search.scope = \"Search\"; var entity = new RestMule!RatePolicyScope; entity.scope = \"Entity\"; // RATE POLICY var policy = new RestMule!RatePolicy; policy.scopes.add(entity); policy.scopes.add(search); var reset = new RestMule!ResponseHeader; var resetInt = new RestMule!TInteger; resetInt.label = \"X-RateLimit-Reset\"; reset.type = resetInt; policy.reset = reset; var limit = new RestMule!ResponseHeader; var limitInt = new RestMule!TInteger; limitInt.label = \"X-RateLimit-Limit\"; limit.type = limitInt; policy.limit = limit; var remaining = new RestMule!ResponseHeader; var remainingInt = new RestMule!TInteger; remainingInt.label = \"X-RateLimit-Remaining\"; remaining.type = remainingInt; policy.remaining = remaining; api.ratePolicy = policy; // PAGINATION var pagination= new RestMule!PaginationPolicy; pagination.start = 1; pagination.max = 10; pagination.increment = 1; pagination.maxPerIteration = 100; var perIteration = new RestMule!Query; perIteration.description = \"Items per page\"; perIteration.required = false; var type = new RestMule!TInteger; type.label = \"per_page\"; type.name = type.label; perIteration.type = type; pagination.perIteration = perIteration; var page = new RestMule!Query; page.description = \"Page identifier\"; page.required = false; var type1 = new RestMule!TInteger; type1.label = \"page\"; type1.name = type1.label; page.type = type1; pagination.page = page; var link = new RestMule!ResponseHeader; link.description = \"Page links\"; var format = new RestMule!TFormattedString; format.label = \"Link\"; format.name = format.label; link.type = format; pagination.links = link; api.pagination = pagination; // WRAPPER var wrapper = new RestMule!Wrapper; wrapper.name = \"Wrapper\"; var items = new RestMule!ListType; items.label = \"items\"; wrapper.items = items; wrapper.totalLabel= \"total_count\"; wrapper.incompleteLabel = \"incomplete_results\"; api.pageWrapper = wrapper; // ADD RATE & WRAPPER TO REQUESTS (FIXME) for (r in RestMule!Request.all){ if (r.parent.path.startsWith(\"/search\")){ r.scope = search; } else { r.scope = entity; } r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader))); for (resp in r.responses.select(s|s.responseType <> null)){ resp.unwrap(); } } /* ////////// * OPERATIONS *////////// operation RestMule!ObjectType hasWrapper() : Boolean { var wrapper = RestMule!Wrapper.all.first; var lists = self.listFields.collect(a|a.label); return (not lists.isEmpty()) and lists .includes(wrapper.items.label); } operation RestMule!Response unwrap() : RestMule!ObjectType{ if (self.responseType.instanceOf(ObjectType)){ if (self.responseType.hasWrapper()){ (\"Unwrapping : \"+ self.responseType.name).println; var wrapper = RestMule!Wrapper.all.first; var name = self.responseType.name.println; self.responseType = self.responseType.println.listFields .select(b| b.label == wrapper.items.label).first.elements.first; self.responseType.name = name; self.pageWrapped = true; self.responseType.description = \"UNWRAPPED: \" + self.responseType.description; } } } After this, we can run the generator and it will generate our Java Project from the specification. In the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.","title":"How to generate REST API Documentation"},{"location":"contributors-guide/technical-guidelignes/rest-api-doc-generation/#how-to-generate-rest-api-documentation","text":"REST API Tutorial files","title":"How to generate REST API Documentation"},{"location":"contributors-guide/technical-guidelignes/rest-api-doc-generation/#install","text":"First of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install. Execute the following steps. Clone repository: https://github.com/patrickneubauer/crossminer-workflow Import projects from the cloned repo to empty Eclipse workspace: Import root via Maven > Existing Maven Projects , then the rest via General > Existing projects into Workspace without checking Search for nested projects . Install new software packages: Install Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling [web page] Install Epsilon (Stable): http://download.eclipse.org/epsilon/updates/ [update site] Update Epsilon (Interim): http://download.eclipse.org/epsilon/interim/ [update site] Install Emfatic: http://download.eclipse.org/emfatic/update/ [update site] Note: I didn't see the the feature until Group items by category was unchecked. Install GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ [update site] Import the given json projects ( org.eclipse.epsilon.emc.json and org.eclipse.epsilon.emc.json.dt ) into the workspace via General > Existing projects into Workspace . Now right-click on org.eclipse.crossmeter.workflow project and select Maven > Update Project.... Copy the give M2M_Environment_oxygen.launch (for Windows 10) to org.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator and overwrite the old one. Refresh project org.eclipse.crossmeter.workflow.restmule.generator in Eclipse. Before running the .launch file, right-click on it, Run as > Run configurations.... Here go to Plug-ins tab and click on Add Required Plug-ins . Now you can run the .launch file TBA:How? In the Runtime Eclipse import the org.eclipse.crossmeter.workflow.restmule.generator project as Maven Project. Run generateFromOAS.launch as you can see in the videos https://youtu.be/BJXuozHJPeg. Now, you should see it generating the project. For further steps, watch Patrick's videos . github client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg github client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI github client generation example \u2014 https://youtu.be/ltSNnSZRETA kafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc kafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4 kafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA MDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk","title":"Install"},{"location":"contributors-guide/technical-guidelignes/rest-api-doc-generation/#example","text":"I've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following: /square/{number} . You can replace the {number} with any number. The repsonse for this request is a JSON object, which looks like this: { \"squared\": {squaredValue} } It contains only one field, named squared , which has the value of the square of the given number. From the specification the generator will provide us a Java Project which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines: ITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings). int number = 7; //The number to be squared. IData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server. NumberValue numberValue; //NumberValue is the class which represents the object received from the server numberValue = squared.observe().blockingSingle(); //Get the actually received object from the response. System.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer. Everything, including the IEntityApi interface and the NumberValue class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through Java to the objects/field inside of the received JSON object. And once again, they are all generated from the OpenAPI specification. So, to generate the mentioned Java Project , you have to put your OpenAPI specification file into the schemas folder inside the org.eclipse.crossmeter.workflow.restmule.generator project. I'll call mine as TestAPI.json , and it's made up of the following: { \"swagger\": \"2.0\", \"schemes\": [ \"http\" ], \"host\": \"localhost:8080\", \"basePath\": \"/\", \"info\": { \"description\": \"This is a test API, only for demonstration of the generator.\", \"termsOfService\": \"\", \"title\": \"TestAPI\", \"version\": \"v1\" }, \"consumes\": [ \"application/json\" ], \"produces\": [ \"application/json\" ], \"securityDefinitions\": { }, \"paths\": { \"/square/{number}\": { \"get\": { \"description\": \"Square a number.\", \"parameters\": [ { \"description\": \"The number to be squared\", \"in\": \"path\", \"name\": \"number\", \"required\": true, \"type\": \"integer\" } ], \"responses\": { \"200\": { \"description\": \"OK\", \"schema\": { \"$ref\": \"#/definitions/NumberValue\" } } } } } }, \"definitions\": { \"NumberValue\": { \"properties\": { \"squared\": { \"type\": \"integer\" } }, \"type\": \"object\" } } } You can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own. Then we have to set the generator to use our new schema. To do this, open the build.xml in the generator project and modify the api and json.model.file property to testapi and schemas/TestAPI.json , respectively. <!--API Variables --> <property name=\"api\" value=\"testapi\" /> <property name=\"json.model.file\" value=\"schemas/TestAPI.json\" /> We are almost done, but we have to take one more small step. We have to provide an .eol file in the epsilon/util/fix/ folder in the generator project. Its name must be the same as the value of the api property in the previous step, so for this example we use the testapi.eol . The content of this file: import \"../restmule.eol\"; var api = RestMule!API.all.first(); // RATE LIMITS var search = new RestMule!RatePolicyScope; search.scope = \"Search\"; var entity = new RestMule!RatePolicyScope; entity.scope = \"Entity\"; // RATE POLICY var policy = new RestMule!RatePolicy; policy.scopes.add(entity); policy.scopes.add(search); var reset = new RestMule!ResponseHeader; var resetInt = new RestMule!TInteger; resetInt.label = \"X-RateLimit-Reset\"; reset.type = resetInt; policy.reset = reset; var limit = new RestMule!ResponseHeader; var limitInt = new RestMule!TInteger; limitInt.label = \"X-RateLimit-Limit\"; limit.type = limitInt; policy.limit = limit; var remaining = new RestMule!ResponseHeader; var remainingInt = new RestMule!TInteger; remainingInt.label = \"X-RateLimit-Remaining\"; remaining.type = remainingInt; policy.remaining = remaining; api.ratePolicy = policy; // PAGINATION var pagination= new RestMule!PaginationPolicy; pagination.start = 1; pagination.max = 10; pagination.increment = 1; pagination.maxPerIteration = 100; var perIteration = new RestMule!Query; perIteration.description = \"Items per page\"; perIteration.required = false; var type = new RestMule!TInteger; type.label = \"per_page\"; type.name = type.label; perIteration.type = type; pagination.perIteration = perIteration; var page = new RestMule!Query; page.description = \"Page identifier\"; page.required = false; var type1 = new RestMule!TInteger; type1.label = \"page\"; type1.name = type1.label; page.type = type1; pagination.page = page; var link = new RestMule!ResponseHeader; link.description = \"Page links\"; var format = new RestMule!TFormattedString; format.label = \"Link\"; format.name = format.label; link.type = format; pagination.links = link; api.pagination = pagination; // WRAPPER var wrapper = new RestMule!Wrapper; wrapper.name = \"Wrapper\"; var items = new RestMule!ListType; items.label = \"items\"; wrapper.items = items; wrapper.totalLabel= \"total_count\"; wrapper.incompleteLabel = \"incomplete_results\"; api.pageWrapper = wrapper; // ADD RATE & WRAPPER TO REQUESTS (FIXME) for (r in RestMule!Request.all){ if (r.parent.path.startsWith(\"/search\")){ r.scope = search; } else { r.scope = entity; } r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader))); for (resp in r.responses.select(s|s.responseType <> null)){ resp.unwrap(); } } /* ////////// * OPERATIONS *////////// operation RestMule!ObjectType hasWrapper() : Boolean { var wrapper = RestMule!Wrapper.all.first; var lists = self.listFields.collect(a|a.label); return (not lists.isEmpty()) and lists .includes(wrapper.items.label); } operation RestMule!Response unwrap() : RestMule!ObjectType{ if (self.responseType.instanceOf(ObjectType)){ if (self.responseType.hasWrapper()){ (\"Unwrapping : \"+ self.responseType.name).println; var wrapper = RestMule!Wrapper.all.first; var name = self.responseType.name.println; self.responseType = self.responseType.println.listFields .select(b| b.label == wrapper.items.label).first.elements.first; self.responseType.name = name; self.pageWrapped = true; self.responseType.description = \"UNWRAPPED: \" + self.responseType.description; } } } After this, we can run the generator and it will generate our Java Project from the specification. In the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.","title":"Example"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/","text":"How to implement Restlet services When to use this guideline ? This guideline present how to create a new REST service using the RESTLET framework in the Scava platform. Context Scava project manages REST services with the RESTLET framework. The usage of Restlet framework has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin : org.eclipse.crossmeter.platform.client.api. The RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in. You want to access to create a new REST Service ? 1. Create a new Route To register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service. Naming the Route The routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service : Naming-Scava-REST-Services.html Register the Route The org.scava.platform.services plug-in contained the class PlatformRoute.java responsible for declaring routes. package org.scava.platform.services; import org.restlet.Application; import org.restlet.Restlet; import org.restlet.routing.Router; public class PlatformRoute extends Application { @Override public Restlet createInboundRoot() { Router router = new Router(getContext()); router.attach(\"/\", PingResource.class); router.attach(\"/search\", SearchProjectResource.class); ... router.attach(\"/projects/p/{projectid}\", ProjectResource.class); router.attach(\"/raw/metrics\", RawMetricListResource.class); ... return router; } } Route Example : router.attach(\"/raw/metrics\", RawMetricListResource.class); \"/raw/metrics\" : Represent the route URL. \"RawMetricListResource.class\" : Represent the class where the service to be implemented for this path. A route can contained some parameters. In this case, parameters are identified by a name with curly brackets {} . 2. Implement the Service A service implementation is a Java class which extend the ServerResource class provided by the RESTLET framework. To create a new service create a new Class : * Named \" ServiceName \" + Resource. Ex : ProjectCreationResource.java * On a namespace based on the route. Ex : org.scava.platform.services.administration for platform administration services. * Who extend the org.restlet.resource.ServerResource class. GET Service To implement a service of type GET, create a new method : * Based on the following signature : public final Representation represent() * Add the @Get(\"json\") annotation @Get(\"json\") public final Representation represent() { // Initialise Response Header Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\"); if (responseHeaders == null) { responseHeaders = new Series(Header.class); getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders); } responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\")); responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\")); // Get Route parameter if required {projectid} String projectId = (String) getRequest().getAttributes().get(\"projectid\"); try { .... // Provide Result getResponse().setStatus(Status.SUCCESS_OK); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } You can also extend the AbstractApiResource , a service class provided by the platform and dedicate to services who request a connection to MongoDb database instead of the ServerResource . In this case you will have to implement the doRepresent() method. public class RawMetricListResource extends AbstractApiResource { public Representation doRepresent() { ObjectNode res = mapper.createObjectNode(); ArrayNode metrics = mapper.createArrayNode(); res.put(\"metrics\", metrics); ... return Util.createJsonRepresentation(res); } } POST Service To implement a service of type POST, crate a new method : * Based on the following signature : public Representation myServiceName (Representation entity) * Add the @Post annotation @Post public Representation myServiceName(Representation entity) { try { // Read Json Datas JsonNode json = mapper.readTree(entity.getText()); ... // Provide Result getResponse().setStatus(Status.SUCCESS_CREATED); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } DELETE Service To do .... 3. Document the Service The REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to allow an easy integration, this contract must be documented :","title":"How to implement Restlet services"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#how-to-implement-restlet-services","text":"","title":"How to implement Restlet services"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#when-to-use-this-guideline","text":"This guideline present how to create a new REST service using the RESTLET framework in the Scava platform.","title":"When to use this guideline ?"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#context","text":"Scava project manages REST services with the RESTLET framework. The usage of Restlet framework has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin : org.eclipse.crossmeter.platform.client.api. The RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in.","title":"Context"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#you-want-to-access-to-create-a-new-rest-service","text":"","title":"You want to access to create a new REST Service ?"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#1-create-a-new-route","text":"To register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service.","title":"1. Create a new Route"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#naming-the-route","text":"The routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service : Naming-Scava-REST-Services.html","title":"Naming the Route"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#register-the-route","text":"The org.scava.platform.services plug-in contained the class PlatformRoute.java responsible for declaring routes. package org.scava.platform.services; import org.restlet.Application; import org.restlet.Restlet; import org.restlet.routing.Router; public class PlatformRoute extends Application { @Override public Restlet createInboundRoot() { Router router = new Router(getContext()); router.attach(\"/\", PingResource.class); router.attach(\"/search\", SearchProjectResource.class); ... router.attach(\"/projects/p/{projectid}\", ProjectResource.class); router.attach(\"/raw/metrics\", RawMetricListResource.class); ... return router; } } Route Example : router.attach(\"/raw/metrics\", RawMetricListResource.class); \"/raw/metrics\" : Represent the route URL. \"RawMetricListResource.class\" : Represent the class where the service to be implemented for this path. A route can contained some parameters. In this case, parameters are identified by a name with curly brackets {} .","title":"Register the Route"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#2-implement-the-service","text":"A service implementation is a Java class which extend the ServerResource class provided by the RESTLET framework. To create a new service create a new Class : * Named \" ServiceName \" + Resource. Ex : ProjectCreationResource.java * On a namespace based on the route. Ex : org.scava.platform.services.administration for platform administration services. * Who extend the org.restlet.resource.ServerResource class.","title":"2. Implement the Service"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#get-service","text":"To implement a service of type GET, create a new method : * Based on the following signature : public final Representation represent() * Add the @Get(\"json\") annotation @Get(\"json\") public final Representation represent() { // Initialise Response Header Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\"); if (responseHeaders == null) { responseHeaders = new Series(Header.class); getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders); } responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\")); responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\")); // Get Route parameter if required {projectid} String projectId = (String) getRequest().getAttributes().get(\"projectid\"); try { .... // Provide Result getResponse().setStatus(Status.SUCCESS_OK); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } You can also extend the AbstractApiResource , a service class provided by the platform and dedicate to services who request a connection to MongoDb database instead of the ServerResource . In this case you will have to implement the doRepresent() method. public class RawMetricListResource extends AbstractApiResource { public Representation doRepresent() { ObjectNode res = mapper.createObjectNode(); ArrayNode metrics = mapper.createArrayNode(); res.put(\"metrics\", metrics); ... return Util.createJsonRepresentation(res); } }","title":"GET Service"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#post-service","text":"To implement a service of type POST, crate a new method : * Based on the following signature : public Representation myServiceName (Representation entity) * Add the @Post annotation @Post public Representation myServiceName(Representation entity) { try { // Read Json Datas JsonNode json = mapper.readTree(entity.getText()); ... // Provide Result getResponse().setStatus(Status.SUCCESS_CREATED); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } }","title":"POST Service"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#delete-service","text":"To do ....","title":"DELETE Service"},{"location":"contributors-guide/technical-guidelignes/restlet-service-implementation/#3-document-the-service","text":"The REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to allow an easy integration, this contract must be documented :","title":"3. Document the Service"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/","text":"How to extend the SCAVA data model When to use ? In this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order the data layer of Scava platform during the evolution. Context The Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class. Each MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model. The current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins : The Platform data model (org.ossmeter.repository.model) : Contains data related to project administration , metric execution process and authentification system. Source Code Repositroy managers (org.ossmeter.repository.model.'project delta manager') : Contains configuration informations related to source codes managemeny tools. Metric Providers data models (in each metric provider plugins) : Each metric provider implementation contains his once data model. You need to Extend an Existing Data Model ? The first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same. 1. Locate the *.emf file of this data model A presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model. Ex : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf) 2. Update the Data Model description A data model description file contains a statical description of a MongoDb document. @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . attr String stackTrace; } ... If we would like to add one more attribute to the element ProjectError , we could add it this way : @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . . . attr String stackTrace; + attr String TestAttribute; } ... You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines 3. Generate the Java Class using the Pongo Tool. Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Replace the existing Java class by the new generated java class. More information about Pongo : https://github.com/kolovos/pongo/wiki You need to Create a new Data Model ? The second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model. In this case, we invite you to create a new plugin which will contain your data model. 1. Create a new Eclipse Plug-In Create a new Eclipse Plug-In Project ( In Eclipse Toolbar : File > New > Plug-In Project Name of the project : org.scava. mycomponent .repository.model Disable the generation of an Activator Class / contribution to the ui Edit the MANIFEST.MF file In Dependency : add a dependency to the org.eclipse.core.runtime plugin In Dependency : add a dependency to the com.googlecode.pongo.runtime plugin In Dependency : add a dependency to the org.apache.commons.lang3 plugin In Extentions : reference an extension point named com.googlecode.pongo.runtime.osgi In source directory Create a package named org.scava. mycomponent .repository.model In this package create an emf file named mycomponent.emf A presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file : ```package org.scava.mycomponent.repository.model; @db class MyComponent { .... } ``` You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines 2. Generate the Java Class using the Pongo Tool. Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Add this class in your org.scava. mycomponent .repository.model package More information about Pongo : https://github.com/kolovos/pongo/wiki","title":"How to extend the SCAVA data model"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#how-to-extend-the-scava-data-model","text":"","title":"How to extend the SCAVA data model"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#when-to-use","text":"In this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order the data layer of Scava platform during the evolution.","title":"When to use ?"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#context","text":"The Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class. Each MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model. The current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins : The Platform data model (org.ossmeter.repository.model) : Contains data related to project administration , metric execution process and authentification system. Source Code Repositroy managers (org.ossmeter.repository.model.'project delta manager') : Contains configuration informations related to source codes managemeny tools. Metric Providers data models (in each metric provider plugins) : Each metric provider implementation contains his once data model.","title":"Context"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#you-need-to-extend-an-existing-data-model","text":"The first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same.","title":"You need to Extend an Existing Data Model ?"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#1-locate-the-emf-file-of-this-data-model","text":"A presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model. Ex : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf)","title":"1. Locate the *.emf file of this data model"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#2-update-the-data-model-description","text":"A data model description file contains a statical description of a MongoDb document. @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . attr String stackTrace; } ... If we would like to add one more attribute to the element ProjectError , we could add it this way : @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . . . attr String stackTrace; + attr String TestAttribute; } ... You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines","title":"2. Update the Data  Model description"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#3-generate-the-java-class-using-the-pongo-tool","text":"Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Replace the existing Java class by the new generated java class. More information about Pongo : https://github.com/kolovos/pongo/wiki","title":"3. Generate the Java Class using the Pongo Tool."},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#you-need-to-create-a-new-data-model","text":"The second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model. In this case, we invite you to create a new plugin which will contain your data model.","title":"You need to Create a new Data Model ?"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#1-create-a-new-eclipse-plug-in","text":"Create a new Eclipse Plug-In Project ( In Eclipse Toolbar : File > New > Plug-In Project Name of the project : org.scava. mycomponent .repository.model Disable the generation of an Activator Class / contribution to the ui Edit the MANIFEST.MF file In Dependency : add a dependency to the org.eclipse.core.runtime plugin In Dependency : add a dependency to the com.googlecode.pongo.runtime plugin In Dependency : add a dependency to the org.apache.commons.lang3 plugin In Extentions : reference an extension point named com.googlecode.pongo.runtime.osgi In source directory Create a package named org.scava. mycomponent .repository.model In this package create an emf file named mycomponent.emf A presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file : ```package org.scava.mycomponent.repository.model; @db class MyComponent { .... } ``` You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines","title":"1. Create a new Eclipse Plug-In"},{"location":"contributors-guide/technical-guidelignes/scava-data-model-extention/#2-generate-the-java-class-using-the-pongo-tool","text":"Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Add this class in your org.scava. mycomponent .repository.model package More information about Pongo : https://github.com/kolovos/pongo/wiki","title":"2. Generate the Java Class using the Pongo Tool."},{"location":"developers-guide/","text":"SCAVA Developers Guide The developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API. Running SCAVA Platform form Sources The following section provide informations related to how the main platform components can be run from sources in developers mode Analysis Platform Administration Application Visualisation Dashboard Eclipse Plugin Metric Provider Development Guide The following section provide the key informations required to develop a new metric provider and to integrate it on the platforme REST API Reference Guide Reference Guide of REST API exposed by the main platform components Access to the REST API via the API Gateway Analysis Platform Knowledge Base Visualisation Dashboard Workflow Engine","title":"SCAVA Developers Guide"},{"location":"developers-guide/#scava-developers-guide","text":"The developers guide is dedicated to who peoples which went to extend the capability of the platform or integrate external tools by the intermediary of the public REST API.","title":"SCAVA Developers Guide"},{"location":"developers-guide/#running-scava-platform-form-sources","text":"The following section provide informations related to how the main platform components can be run from sources in developers mode Analysis Platform Administration Application Visualisation Dashboard Eclipse Plugin","title":"Running SCAVA Platform form Sources"},{"location":"developers-guide/#metric-provider-development-guide","text":"The following section provide the key informations required to develop a new metric provider and to integrate it on the platforme","title":"Metric Provider Development Guide"},{"location":"developers-guide/#rest-api-reference-guide","text":"Reference Guide of REST API exposed by the main platform components Access to the REST API via the API Gateway Analysis Platform Knowledge Base Visualisation Dashboard Workflow Engine","title":"REST API Reference Guide"},{"location":"developers-guide/api-reference-guide/","text":"REST API Reference Guide Reference Guide of REST API exposed by the main platform components Access to the REST API via the API Gateway (Softeam) Analysis Platform (Softeam) Knowledge Base (UDA) Visualisation Dashboard (Bitergia) Workflow Engine (York)","title":"REST API Reference Guide"},{"location":"developers-guide/api-reference-guide/#rest-api-reference-guide","text":"Reference Guide of REST API exposed by the main platform components Access to the REST API via the API Gateway (Softeam) Analysis Platform (Softeam) Knowledge Base (UDA) Visualisation Dashboard (Bitergia) Workflow Engine (York)","title":"REST API Reference Guide"},{"location":"developers-guide/api-reference-guide/analysis-platform/","text":"Analysis Platform REST API reference Guide The MetricPlatform APIs documentation generated with swagger UI is available : SCAVA MetricPlatform APIs","title":"Analysis Platform REST API reference Guide"},{"location":"developers-guide/api-reference-guide/analysis-platform/#analysis-platform-rest-api-reference-guide","text":"The MetricPlatform APIs documentation generated with swagger UI is available : SCAVA MetricPlatform APIs","title":"Analysis Platform REST API reference Guide"},{"location":"developers-guide/api-reference-guide/api-gateway/","text":"Access to the REST API via the API Gateway","title":"Access to the REST API via the API Gateway"},{"location":"developers-guide/api-reference-guide/api-gateway/#access-to-the-rest-api-via-the-api-gateway","text":"","title":"Access to the REST API via the API Gateway"},{"location":"developers-guide/api-reference-guide/knowledge-base/","text":"Knowledge Base REST API reference Guide","title":"Knowledge Base REST API reference Guide"},{"location":"developers-guide/api-reference-guide/knowledge-base/#knowledge-base-rest-api-reference-guide","text":"","title":"Knowledge Base REST API reference Guide"},{"location":"developers-guide/api-reference-guide/visualisation-dashboard/","text":"Visualisation Platform REST API reference Guide","title":"Visualisation Platform REST API reference Guide"},{"location":"developers-guide/api-reference-guide/visualisation-dashboard/#visualisation-platform-rest-api-reference-guide","text":"","title":"Visualisation Platform REST API reference Guide"},{"location":"developers-guide/api-reference-guide/workflow-engine/","text":"Workflow Engine REST API reference Guide","title":"Workflow Engine REST API reference Guide"},{"location":"developers-guide/api-reference-guide/workflow-engine/#workflow-engine-rest-api-reference-guide","text":"","title":"Workflow Engine REST API reference Guide"},{"location":"developers-guide/metric-provider-developement-guide/","text":"Metric Provider Developement Guide In this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a transient metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a historical metric provider, will count the total number of commits over time. We'll go through the steps above for each metric provider. Pre-requisites Eclipse The Emfatic plug-in should be installed in your Eclipse The Pongo plug-in should be installed in your Eclipse The OSSMETER source code should be in your workspace The Transient Metric Provider This metric provider will store a complete history of the commits in the version control system(s) used by a project. 0. Setup Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list. 1. The data model We define the data model using the Emfatic language. In your newly created plug-in, create a package called org.ossmeter.metricprovider.trans.commits.model . In that package create an empty file called commits.emf . In this file, we will define our data model. First of all, we need to state the name of the package. package org.ossmeter.metricprovider.trans.commits.model; This is used by the Pongo code generator - the generated classes will be put in this package. We then define the database for our model: @db(qualifiedCollectionNames=\"true\") class Commits { val Repository[*] repositories; val Commit[*] commits; } The @db annotation tells Pongo that this will be the container database for the data. Adding the qualifiedCollectionNames=true property will prepend the database name to all Mongo collections. The Commits class above says that we want a database with two collections, name repositories and commits . If qualifiedCollectionNames is set to true , the collections will be named Commits.repositories and Commits.commits . We now define the schema of the Commits.repositories collection: ~~~java class Repository { @searchable attr String url; attr String repoType; attr String revision; attr int totalNumberOfCommits; ref CommitData[*] commits; } This class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection. The `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time. Each commit is represented in the `Commits.commits` collection by the following model: ~~~~java class Commit { @searchable attr Date date; attr String identifier; attr String message; attr String author; } For each commit, we store its date , identifier (revision ID), the commit message , and the author . We also create an index on the date to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated findByDate(String date) or findOneByDate(String date) methods). Now we need to use Pongo to generate code from this model. Right-click on the commits.emf file and select Pongo > Generate Pongos and plugin.xml . You should see some Java classes appear in your package. Now that we have our data model, we can implement the metric provider. 2. The metric provider Create a Java class called org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider This class should extend AbstractTransientMetricProvider and specify Commits for the generic argument: public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits> The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. adapt(DB db) This method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows: @Override public Commits adapt(DB db) { return new Commits(db); } The next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider: @Override public String getShortIdentifier() { // This may be deprecated very soon return \"transient-commits\"; } @Override public String getFriendlyName() { return \"Commit History\"; } @Override public String getSummaryInformation() { return \"The commit history of the project.\"; } The next method allows you to declare whether the metric provider is applicable to a given project: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } Our metric applies to any project that has at least one VCS repository. Finally, we have the measure(...) method that performs the actual metric calculation: @Override public void measure(Project project, ProjectDelta delta, Commits db) { for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) { Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl()); if (repo == null) { repo = new Repository(); repo.setUrl(repoDelta.getRepository().getUrl()); db.getRepositories().add(repo); } for (VcsCommit commit : repoDelta.getCommits()) { Commit c = new Commit(); c.setDate(commit.getJavaDate()); c.setMessage(commit.getMessage()); c.setAuthor(commit.getAuthor()); c.setIdentifier(commit.getRevision()); repo.getCommits().add(c); db.getCommits().add(c); } } db.getCommits().sync(); db.getRepositories().sync(); } The above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new Commit object is created for each commit in the delta. 3. Make the metric provider discoverable Metric providers are registered with the OSSMETER platform using extension points : Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsTransientMetricProvider' class Now everything is ready for the metric to be executed :) The Historic Metric Provider This metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value. 0. Setup Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list. 1. The data model In your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model: package org.ossmeter.metricprovider.historic.commits.model; class HistoricCommits { attr int numberOfCommits; } The data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed. 2. The metric provider Create a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider This class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)): public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. First of all, complete the typical information-related methods: @Override public String getShortIdentifier() { return \"historicalcommits\"; } @Override public String getFriendlyName() { return \"Historical commits\"; } @Override public String getSummaryInformation() { return \"...\"; } Now complete the standard appliesTo method: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } We now need to specify a dependency on the transient metric provider that we just implemented. @Override public List<String> getIdentifiersOfUses() { return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName()); } This tells the platform that we need access to the CommitsTransientMetricProvider database. The platform will assign this to the uses field that is available to the historical metric provider, as you'll see in the measure method: @Override public Pongo measure(Project project) { Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0)); int commits = (int) transDb.getCommits().size(); HistoricCommits hist = new HistoricCommits(); hist.setNumberOfCommits(commits); return hist; } First of all, we get hold of the database of the transient commits metric provider, and simply count the size of the commits collection. We save this in an instance of the HistoricCommits Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project. 3. Make the metric provider discoverable This process is the same as the transient metric provider: Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class Now everything is ready for both metrics to be executed :) But first. Let's specify how we want this historical metric to be visualised. 4. Define a MetVis visualisation specification MetVis is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis web page has numerous examples of this. Create a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider: { \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\", \"vis\" : [ { \"id\" : \"historicalcommits\", \"name\" : \"Commits over time\", \"description\" : \"This metric shows when the projects commits occurred\", \"type\" : \"LineChart\", \"datatable\" : { \"cols\" : [ { \"name\" : \"Date\", \"field\" : \"$__date\" }, { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" } ] }, \"x\" : \"Date\", \"y\" : \"Commits\" } ] } The metricId field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use $__date . The final two fields ( x and y ) are references to column names in the datatable specification. The type states that the data should be plotted as a line chart. You can test your MetVis specifications on the MetVis playpen . 5. Make the visualisation specification discoverable As with metric providers, visualisation specifications are registered using extension points. Add the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.visualisation.metric On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'historicalcommits.json' file Good job. Running the metric providers See Running from Source Homework Adapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.","title":"Metric Provider Developement Guide"},{"location":"developers-guide/metric-provider-developement-guide/#metric-provider-developement-guide","text":"In this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a transient metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a historical metric provider, will count the total number of commits over time. We'll go through the steps above for each metric provider.","title":"Metric Provider Developement Guide"},{"location":"developers-guide/metric-provider-developement-guide/#pre-requisites","text":"Eclipse The Emfatic plug-in should be installed in your Eclipse The Pongo plug-in should be installed in your Eclipse The OSSMETER source code should be in your workspace","title":"Pre-requisites"},{"location":"developers-guide/metric-provider-developement-guide/#the-transient-metric-provider","text":"This metric provider will store a complete history of the commits in the version control system(s) used by a project.","title":"The Transient Metric Provider"},{"location":"developers-guide/metric-provider-developement-guide/#0-setup","text":"Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.","title":"0. Setup"},{"location":"developers-guide/metric-provider-developement-guide/#1-the-data-model","text":"We define the data model using the Emfatic language. In your newly created plug-in, create a package called org.ossmeter.metricprovider.trans.commits.model . In that package create an empty file called commits.emf . In this file, we will define our data model. First of all, we need to state the name of the package. package org.ossmeter.metricprovider.trans.commits.model; This is used by the Pongo code generator - the generated classes will be put in this package. We then define the database for our model: @db(qualifiedCollectionNames=\"true\") class Commits { val Repository[*] repositories; val Commit[*] commits; } The @db annotation tells Pongo that this will be the container database for the data. Adding the qualifiedCollectionNames=true property will prepend the database name to all Mongo collections. The Commits class above says that we want a database with two collections, name repositories and commits . If qualifiedCollectionNames is set to true , the collections will be named Commits.repositories and Commits.commits . We now define the schema of the Commits.repositories collection: ~~~java class Repository { @searchable attr String url; attr String repoType; attr String revision; attr int totalNumberOfCommits; ref CommitData[*] commits; } This class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection. The `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time. Each commit is represented in the `Commits.commits` collection by the following model: ~~~~java class Commit { @searchable attr Date date; attr String identifier; attr String message; attr String author; } For each commit, we store its date , identifier (revision ID), the commit message , and the author . We also create an index on the date to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated findByDate(String date) or findOneByDate(String date) methods). Now we need to use Pongo to generate code from this model. Right-click on the commits.emf file and select Pongo > Generate Pongos and plugin.xml . You should see some Java classes appear in your package. Now that we have our data model, we can implement the metric provider.","title":"1. The data model"},{"location":"developers-guide/metric-provider-developement-guide/#2-the-metric-provider","text":"Create a Java class called org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider This class should extend AbstractTransientMetricProvider and specify Commits for the generic argument: public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits> The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn.","title":"2. The metric provider"},{"location":"developers-guide/metric-provider-developement-guide/#adaptdb-db","text":"This method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows: @Override public Commits adapt(DB db) { return new Commits(db); } The next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider: @Override public String getShortIdentifier() { // This may be deprecated very soon return \"transient-commits\"; } @Override public String getFriendlyName() { return \"Commit History\"; } @Override public String getSummaryInformation() { return \"The commit history of the project.\"; } The next method allows you to declare whether the metric provider is applicable to a given project: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } Our metric applies to any project that has at least one VCS repository. Finally, we have the measure(...) method that performs the actual metric calculation: @Override public void measure(Project project, ProjectDelta delta, Commits db) { for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) { Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl()); if (repo == null) { repo = new Repository(); repo.setUrl(repoDelta.getRepository().getUrl()); db.getRepositories().add(repo); } for (VcsCommit commit : repoDelta.getCommits()) { Commit c = new Commit(); c.setDate(commit.getJavaDate()); c.setMessage(commit.getMessage()); c.setAuthor(commit.getAuthor()); c.setIdentifier(commit.getRevision()); repo.getCommits().add(c); db.getCommits().add(c); } } db.getCommits().sync(); db.getRepositories().sync(); } The above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new Commit object is created for each commit in the delta.","title":"adapt(DB db)"},{"location":"developers-guide/metric-provider-developement-guide/#3-make-the-metric-provider-discoverable","text":"Metric providers are registered with the OSSMETER platform using extension points : Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsTransientMetricProvider' class Now everything is ready for the metric to be executed :)","title":"3. Make the metric provider discoverable"},{"location":"developers-guide/metric-provider-developement-guide/#the-historic-metric-provider","text":"This metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value.","title":"The Historic Metric Provider"},{"location":"developers-guide/metric-provider-developement-guide/#0-setup_1","text":"Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.","title":"0. Setup"},{"location":"developers-guide/metric-provider-developement-guide/#1-the-data-model_1","text":"In your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model: package org.ossmeter.metricprovider.historic.commits.model; class HistoricCommits { attr int numberOfCommits; } The data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed.","title":"1. The data model"},{"location":"developers-guide/metric-provider-developement-guide/#2-the-metric-provider_1","text":"Create a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider This class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)): public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. First of all, complete the typical information-related methods: @Override public String getShortIdentifier() { return \"historicalcommits\"; } @Override public String getFriendlyName() { return \"Historical commits\"; } @Override public String getSummaryInformation() { return \"...\"; } Now complete the standard appliesTo method: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } We now need to specify a dependency on the transient metric provider that we just implemented. @Override public List<String> getIdentifiersOfUses() { return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName()); } This tells the platform that we need access to the CommitsTransientMetricProvider database. The platform will assign this to the uses field that is available to the historical metric provider, as you'll see in the measure method: @Override public Pongo measure(Project project) { Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0)); int commits = (int) transDb.getCommits().size(); HistoricCommits hist = new HistoricCommits(); hist.setNumberOfCommits(commits); return hist; } First of all, we get hold of the database of the transient commits metric provider, and simply count the size of the commits collection. We save this in an instance of the HistoricCommits Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project.","title":"2. The metric provider"},{"location":"developers-guide/metric-provider-developement-guide/#3-make-the-metric-provider-discoverable_1","text":"This process is the same as the transient metric provider: Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class Now everything is ready for both metrics to be executed :) But first. Let's specify how we want this historical metric to be visualised.","title":"3. Make the metric provider discoverable"},{"location":"developers-guide/metric-provider-developement-guide/#4-define-a-metvis-visualisation-specification","text":"MetVis is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis web page has numerous examples of this. Create a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider: { \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\", \"vis\" : [ { \"id\" : \"historicalcommits\", \"name\" : \"Commits over time\", \"description\" : \"This metric shows when the projects commits occurred\", \"type\" : \"LineChart\", \"datatable\" : { \"cols\" : [ { \"name\" : \"Date\", \"field\" : \"$__date\" }, { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" } ] }, \"x\" : \"Date\", \"y\" : \"Commits\" } ] } The metricId field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use $__date . The final two fields ( x and y ) are references to column names in the datatable specification. The type states that the data should be plotted as a line chart. You can test your MetVis specifications on the MetVis playpen .","title":"4. Define a MetVis visualisation specification"},{"location":"developers-guide/metric-provider-developement-guide/#5-make-the-visualisation-specification-discoverable","text":"As with metric providers, visualisation specifications are registered using extension points. Add the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.visualisation.metric On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'historicalcommits.json' file Good job.","title":"5. Make the visualisation specification discoverable"},{"location":"developers-guide/metric-provider-developement-guide/#running-the-metric-providers","text":"See Running from Source","title":"Running the metric providers"},{"location":"developers-guide/metric-provider-developement-guide/#homework","text":"Adapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.","title":"Homework"},{"location":"developers-guide/runing-from-sources/","text":"Running SCAVA Platform form Sources The following section provide informations related to how the main platform components can be run from sources in developers mode Analysis Platform (Softeam) Administration Application (Softeam) Visualisation Dashboard (Bitergia) Eclipse Plugin (FrontendArt)","title":"Running SCAVA Platform form Sources"},{"location":"developers-guide/runing-from-sources/#running-scava-platform-form-sources","text":"The following section provide informations related to how the main platform components can be run from sources in developers mode Analysis Platform (Softeam) Administration Application (Softeam) Visualisation Dashboard (Bitergia) Eclipse Plugin (FrontendArt)","title":"Running SCAVA Platform form Sources"},{"location":"developers-guide/runing-from-sources/administration-application/","text":"Running the Administration Application form Sources Prerequisites: Start running the Metric Platform . Development Toolkits Scava Administration is a based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI 6.1.4 tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart ). sudo npm install @angular/cli@6.1.4 Get the Code Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev Run the Administration webapp The following instructions show how to run the dashboard web app from source: * Enter the administration/scava-administration/ directory within the scava repository. * Install Angular dependency using npm install * Run the web app on port 4200 using angular-cli: ng serve * Navigate to http://localhost:4200/","title":"Running the Administration Application form Sources"},{"location":"developers-guide/runing-from-sources/administration-application/#running-the-administration-application-form-sources","text":"","title":"Running the Administration Application form Sources"},{"location":"developers-guide/runing-from-sources/administration-application/#prerequisites","text":"Start running the Metric Platform .","title":"Prerequisites:"},{"location":"developers-guide/runing-from-sources/administration-application/#development-toolkits","text":"Scava Administration is a based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI 6.1.4 tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart ). sudo npm install @angular/cli@6.1.4","title":"Development Toolkits"},{"location":"developers-guide/runing-from-sources/administration-application/#get-the-code","text":"Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev","title":"Get the Code"},{"location":"developers-guide/runing-from-sources/administration-application/#run-the-administration-webapp","text":"The following instructions show how to run the dashboard web app from source: * Enter the administration/scava-administration/ directory within the scava repository. * Install Angular dependency using npm install * Run the web app on port 4200 using angular-cli: ng serve * Navigate to http://localhost:4200/","title":"Run the Administration webapp"},{"location":"developers-guide/runing-from-sources/analysis-platform/","text":"Running the Analysis Platform form Sources This is a quick start guide to get the SCAVA platform running from source with Eclipse. Prerequisite Install MongoDB You can download MongoDB from the MongoDb website . Install EclipseIDE Although these instructions may apply to other versions of Eclipse IDE, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package). Get the Code Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev Configuration Configure The Eclipse IDE Import Projects into Eclipse Workspace Open Eclipse and import all projects from the top level directory of the Scava code ( File -> Import -> Maven -> Existing Maven Projects ), and wait for all the projects to compile without errors. Meanwhile, the Eclipse IDE would suggest you to install a set of m2e-connectors iincluding Tycho Plugin . Figure-01: Install m2e-connectors plugins. Configure the Target Platform The Scava Analysis Platform is based on the Mars version of the RCP Eclipse. In order to run the platform in a newer eclipse environment, you will have to download the Eclipse Mars platform and configure it as Target Platform. Download Eclipse Mars platform Download the compleate Eclipse Mars platform. In command ligne : ./eclipse -nosplash -verbose -application org.eclipse.equinox.p2.metadata.repository.mirrorApplication -source http://download.eclipse.org/releases/mars -destination {your platform folder} ./eclipse -nosplash -verbose -application org.eclipse.equinox.p2.artifact.repository.mirrorApplication -source http://download.eclipse.org/releases/mars -destination {your platform folder} Then, extract the its context somewehere on your machine. Download Rascal dependencies Download two external libraries required to run metrics providers based on Rascal. Repository URL : Rascal Dependencies impulse_0.3.0.xxx.jar rascal_eclipse_0.12.0.xxx.jar Configure the Target Platform Open the Eclipse preferences on ( Window -> Preferencies ), then choose the Target Platform Definition ( Plug-in Development -> Target Platform ). Figure-02: Eclipse Preferences. Add a new Target Platform configuration for the project, choose the Default initialization then click on ( Next ). Figure-03: Eclipse Target Platforms Initialization. Give a name to the new target platform, hit the ( Add ) button to add the Eclipse Mars dependencies and the external rascal libraries content to the target, then click on ( Finish ). Figure-04: Eclipse Target Platforms Dependencies. Finally, check the new target Configuration, then click on ( Apply and Close ) to save the changes. Figure-05: Eclipse Target Platforms. Configure the Analysis Platform identifier=<your name> The identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID. If you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers. log.type=console|file|rolling You can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify file or rolling , you must complete the log.file.path or log.rolling.path property as well. If the property is not specified, it will default to the console logger. log.file.path=<path> The path to the file to store the log. E.g. log.file.path=/tmp/lovelylog.log log.rolling.path=<path> The path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify log.rolling.path=/tmp/mylovelylog.log , it will store files like so: /tmp/mylovelylog.log.2014-12-17-00 and /tmp/mylovelylog.log.2014-12-17-12 . maven_executable=<path> The path to where Maven is installed. E.g. maven_executable=/usr/bin/mvn storage_path=<path> The path to where files should be stored. E.g. storage_path=/mnt/ossmeter/ mongo_hosts A comma-separated list of the hosts and ports in a replica set. E.g. ua002:27017,ua009:27017,ua019:27017,ua020:27017 Run the Analysis Platform Start MongoDB Instructions for starting mongo can be found in the MongoDB manual . For example: sudo systemctl start mongod or sudo service mongod start Run the api-gateway Right click on scava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java Then click on Run As -> Java Application Run the authentication service Right click on scava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java Then click on Run As -> Java Application Validate and Run the Platform Open releng/org.eclipse.scava.product/scava.product * Click the Validate... icon in the top right of the product configuration editor (the icon is a piece of paper with a tick) * If things do not validate, there's something wrong -- get in touch :) Problems related to org.eclipse.e4.core.di aren't critical. * Then, click the Export an Eclipse product on the left of the Validate... button. Uncheck the Generate p2 repository checkbox, select a destination directory and validate. After a while, the SCAVA platform will be generated in the selected directory. * The platform can then be run using the generated eclipse binary; it accepts the following arguments: * -apiServer : Starts up the client API on localhost:8182 * -worker ${id-worker} : Spawns a thread that analyses registered projects * To get a full platform running, first launch a worker thread, then the API server. When starting the platform, you can pass a configuration file to control the behaviour of the platform: ./eclipse -worker ${id-worker} -config myconfiguration.properties","title":"Running the Analysis Platform form Sources"},{"location":"developers-guide/runing-from-sources/analysis-platform/#running-the-analysis-platform-form-sources","text":"This is a quick start guide to get the SCAVA platform running from source with Eclipse.","title":"Running the Analysis Platform form Sources"},{"location":"developers-guide/runing-from-sources/analysis-platform/#prerequisite","text":"","title":"Prerequisite"},{"location":"developers-guide/runing-from-sources/analysis-platform/#install-mongodb","text":"You can download MongoDB from the MongoDb website .","title":"Install MongoDB"},{"location":"developers-guide/runing-from-sources/analysis-platform/#install-eclipseide","text":"Although these instructions may apply to other versions of Eclipse IDE, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package).","title":"Install EclipseIDE"},{"location":"developers-guide/runing-from-sources/analysis-platform/#get-the-code","text":"Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev","title":"Get the Code"},{"location":"developers-guide/runing-from-sources/analysis-platform/#configuration","text":"","title":"Configuration"},{"location":"developers-guide/runing-from-sources/analysis-platform/#configure-the-eclipse-ide","text":"","title":"Configure The Eclipse IDE"},{"location":"developers-guide/runing-from-sources/analysis-platform/#import-projects-into-eclipse-workspace","text":"Open Eclipse and import all projects from the top level directory of the Scava code ( File -> Import -> Maven -> Existing Maven Projects ), and wait for all the projects to compile without errors. Meanwhile, the Eclipse IDE would suggest you to install a set of m2e-connectors iincluding Tycho Plugin . Figure-01: Install m2e-connectors plugins.","title":"Import Projects into Eclipse Workspace"},{"location":"developers-guide/runing-from-sources/analysis-platform/#configure-the-target-platform","text":"The Scava Analysis Platform is based on the Mars version of the RCP Eclipse. In order to run the platform in a newer eclipse environment, you will have to download the Eclipse Mars platform and configure it as Target Platform. Download Eclipse Mars platform Download the compleate Eclipse Mars platform. In command ligne : ./eclipse -nosplash -verbose -application org.eclipse.equinox.p2.metadata.repository.mirrorApplication -source http://download.eclipse.org/releases/mars -destination {your platform folder} ./eclipse -nosplash -verbose -application org.eclipse.equinox.p2.artifact.repository.mirrorApplication -source http://download.eclipse.org/releases/mars -destination {your platform folder} Then, extract the its context somewehere on your machine. Download Rascal dependencies Download two external libraries required to run metrics providers based on Rascal. Repository URL : Rascal Dependencies impulse_0.3.0.xxx.jar rascal_eclipse_0.12.0.xxx.jar Configure the Target Platform Open the Eclipse preferences on ( Window -> Preferencies ), then choose the Target Platform Definition ( Plug-in Development -> Target Platform ). Figure-02: Eclipse Preferences. Add a new Target Platform configuration for the project, choose the Default initialization then click on ( Next ). Figure-03: Eclipse Target Platforms Initialization. Give a name to the new target platform, hit the ( Add ) button to add the Eclipse Mars dependencies and the external rascal libraries content to the target, then click on ( Finish ). Figure-04: Eclipse Target Platforms Dependencies. Finally, check the new target Configuration, then click on ( Apply and Close ) to save the changes. Figure-05: Eclipse Target Platforms.","title":"Configure the Target Platform"},{"location":"developers-guide/runing-from-sources/analysis-platform/#configure-the-analysis-platform","text":"","title":"Configure the Analysis Platform"},{"location":"developers-guide/runing-from-sources/analysis-platform/#identifierltyour-namegt","text":"The identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID. If you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers.","title":"identifier=&lt;your name&gt;"},{"location":"developers-guide/runing-from-sources/analysis-platform/#logtypeconsolefilerolling","text":"You can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify file or rolling , you must complete the log.file.path or log.rolling.path property as well. If the property is not specified, it will default to the console logger.","title":"log.type=console|file|rolling"},{"location":"developers-guide/runing-from-sources/analysis-platform/#logfilepathltpathgt","text":"The path to the file to store the log. E.g. log.file.path=/tmp/lovelylog.log","title":"log.file.path=&lt;path&gt;"},{"location":"developers-guide/runing-from-sources/analysis-platform/#logrollingpathltpathgt","text":"The path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify log.rolling.path=/tmp/mylovelylog.log , it will store files like so: /tmp/mylovelylog.log.2014-12-17-00 and /tmp/mylovelylog.log.2014-12-17-12 .","title":"log.rolling.path=&lt;path&gt;"},{"location":"developers-guide/runing-from-sources/analysis-platform/#maven_executableltpathgt","text":"The path to where Maven is installed. E.g. maven_executable=/usr/bin/mvn","title":"maven_executable=&lt;path&gt;"},{"location":"developers-guide/runing-from-sources/analysis-platform/#storage_pathltpathgt","text":"The path to where files should be stored. E.g. storage_path=/mnt/ossmeter/","title":"storage_path=&lt;path&gt;"},{"location":"developers-guide/runing-from-sources/analysis-platform/#mongo_hosts","text":"A comma-separated list of the hosts and ports in a replica set. E.g. ua002:27017,ua009:27017,ua019:27017,ua020:27017","title":"mongo_hosts"},{"location":"developers-guide/runing-from-sources/analysis-platform/#run-the-analysis-platform","text":"","title":"Run the Analysis Platform"},{"location":"developers-guide/runing-from-sources/analysis-platform/#start-mongodb","text":"Instructions for starting mongo can be found in the MongoDB manual . For example: sudo systemctl start mongod or sudo service mongod start","title":"Start MongoDB"},{"location":"developers-guide/runing-from-sources/analysis-platform/#run-the-api-gateway","text":"Right click on scava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java Then click on Run As -> Java Application","title":"Run the api-gateway"},{"location":"developers-guide/runing-from-sources/analysis-platform/#run-the-authentication-service","text":"Right click on scava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java Then click on Run As -> Java Application","title":"Run the authentication service"},{"location":"developers-guide/runing-from-sources/analysis-platform/#validate-and-run-the-platform","text":"Open releng/org.eclipse.scava.product/scava.product * Click the Validate... icon in the top right of the product configuration editor (the icon is a piece of paper with a tick) * If things do not validate, there's something wrong -- get in touch :) Problems related to org.eclipse.e4.core.di aren't critical. * Then, click the Export an Eclipse product on the left of the Validate... button. Uncheck the Generate p2 repository checkbox, select a destination directory and validate. After a while, the SCAVA platform will be generated in the selected directory. * The platform can then be run using the generated eclipse binary; it accepts the following arguments: * -apiServer : Starts up the client API on localhost:8182 * -worker ${id-worker} : Spawns a thread that analyses registered projects * To get a full platform running, first launch a worker thread, then the API server. When starting the platform, you can pass a configuration file to control the behaviour of the platform: ./eclipse -worker ${id-worker} -config myconfiguration.properties","title":"Validate and Run the Platform"},{"location":"developers-guide/runing-from-sources/eclipse-plugin/","text":"Running the Eclipse Plugin form Sources (To be completed by FrontendArt)","title":"Running the Eclipse Plugin form Sources (To be completed by FrontendArt)"},{"location":"developers-guide/runing-from-sources/eclipse-plugin/#running-the-eclipse-plugin-form-sources-to-be-completed-by-frontendart","text":"","title":"Running the Eclipse Plugin form Sources (To be completed by FrontendArt)"},{"location":"developers-guide/runing-from-sources/visualisation-dashboard/","text":"Running the Visualisation Dashboard form Sources (To be reviewed and completed by Bitergia) All the documentation below describes how to setup and run the different components related to the Web Dashboards. Install steps for the different components Install GrimoireLab Python Env The data processing is done with GrimoireLab python platform. A virtual env in Python is used to install the tools needed. In Debian/Ubuntu you need to execute: sudo apt-get install python3-venv To create the python virtualenv and activate it: mkdir ~/venvs python3 -m venv ~/venvs/crossminer source ~/venvs/crossminer/bin/activate pip3 install grimoire-elk Install Elasticsearch and Kibana An Elasticsearch and Kibana services are needed. docker-compose can be used to start them. Elasticsearch needs this config: sudo sh -c \"echo 262144 > /proc/sys/vm/max_map_count\"","title":"Running the Visualisation Dashboard form Sources (To be reviewed and completed by Bitergia)"},{"location":"developers-guide/runing-from-sources/visualisation-dashboard/#running-the-visualisation-dashboard-form-sources-to-be-reviewed-and-completed-by-bitergia","text":"All the documentation below describes how to setup and run the different components related to the Web Dashboards.","title":"Running the Visualisation Dashboard form Sources (To be reviewed and completed by Bitergia)"},{"location":"developers-guide/runing-from-sources/visualisation-dashboard/#install-steps-for-the-different-components","text":"","title":"Install steps for the different components"},{"location":"developers-guide/runing-from-sources/visualisation-dashboard/#install-grimoirelab-python-env","text":"The data processing is done with GrimoireLab python platform. A virtual env in Python is used to install the tools needed. In Debian/Ubuntu you need to execute: sudo apt-get install python3-venv To create the python virtualenv and activate it: mkdir ~/venvs python3 -m venv ~/venvs/crossminer source ~/venvs/crossminer/bin/activate pip3 install grimoire-elk","title":"Install GrimoireLab Python Env"},{"location":"developers-guide/runing-from-sources/visualisation-dashboard/#install-elasticsearch-and-kibana","text":"An Elasticsearch and Kibana services are needed. docker-compose can be used to start them. Elasticsearch needs this config: sudo sh -c \"echo 262144 > /proc/sys/vm/max_map_count\"","title":"Install Elasticsearch and Kibana"},{"location":"installation-guide/","text":"SCAVA Installation Guide The SCAVA installation guide provides instructions on how to install and configure the SCAVA Platform on a server and how to deploy the Eclipses Plugin in development environment. Platform Installation using DOCKER images Platform Installation using individual components Platform Data Base Backup And Restoration Eclipse Plugin Installation Platform Configuration Licencing Information","title":"SCAVA Installation Guide"},{"location":"installation-guide/#scava-installation-guide","text":"The SCAVA installation guide provides instructions on how to install and configure the SCAVA Platform on a server and how to deploy the Eclipses Plugin in development environment. Platform Installation using DOCKER images Platform Installation using individual components Platform Data Base Backup And Restoration Eclipse Plugin Installation Platform Configuration Licencing Information","title":"SCAVA Installation Guide"},{"location":"installation-guide/database-backup/","text":"Platform Data Base Backup And Restoration The SCAVA Platform use a Mongo database to strore data related to analysed projets (project data, analyse process data and measurements collected durnig the analysis process). A backup of this data can be perfroemd using the default Mongo backup service. SCAVA Paltform Data Model The SCAVA Data mode is composed of several data bases. Each of this databases must be backuped individualy Database Descripton scava The scava databases containes informations related to Analysed Projects including the list of all repository related to this project scava-analysis The scava-analysis databases containes informations related to analysis process. For each analysed project, this database containes information related to the defined analysed tasks and information related the execution of this anlaysis tasks {AnalysedProjectName} For each analysed project, the SCAVA Platform store the result of the analyse (collected measurements) a is a separate database. The Name of this database is the shortName of the project. The shortName of a project cab be retrive using the REST API exposed by the platforme ( http:/{platforme url}:{platforme port}/project) Backup Process The Mongo installation package(windows and linux) provide the mongodump service whic allow to create a database dump mongodump --gzip --archive= --db --port Example : mongodump --gzip --archive=scava.20180906.gz --db scava --port 27018 // project database mongodump --gzip --archive=scava-analysis.20180906.gz --db scava-analysis --port 27018 // analysis process database mongodump --gzip --archive=QualityGuard.20180906.gz --db QualityGuard --port 27018 // QualityGuard project database Restoration Process The Mongo installation package( windows and linux) provide the mongorestore service which allow to restore a database dump mongorestore --drop --gzip --archive=Activemq.20181203.gz Example : mongorestore --drop --gzip --archive=scava.20181203.gz // project database mongorestore --drop --gzip --archive=scava-analysis.20181203.gz // analysis process database mongorestore --drop --gzip --archive=QualityGuard.20181203.gz // QualityGuard project database","title":"Platform Data Base Backup And Restoration"},{"location":"installation-guide/database-backup/#platform-data-base-backup-and-restoration","text":"The SCAVA Platform use a Mongo database to strore data related to analysed projets (project data, analyse process data and measurements collected durnig the analysis process). A backup of this data can be perfroemd using the default Mongo backup service.","title":"Platform Data Base Backup And Restoration"},{"location":"installation-guide/database-backup/#scava-paltform-data-model","text":"The SCAVA Data mode is composed of several data bases. Each of this databases must be backuped individualy Database Descripton scava The scava databases containes informations related to Analysed Projects including the list of all repository related to this project scava-analysis The scava-analysis databases containes informations related to analysis process. For each analysed project, this database containes information related to the defined analysed tasks and information related the execution of this anlaysis tasks {AnalysedProjectName} For each analysed project, the SCAVA Platform store the result of the analyse (collected measurements) a is a separate database. The Name of this database is the shortName of the project. The shortName of a project cab be retrive using the REST API exposed by the platforme ( http:/{platforme url}:{platforme port}/project)","title":"SCAVA Paltform Data Model"},{"location":"installation-guide/database-backup/#backup-process","text":"The Mongo installation package(windows and linux) provide the mongodump service whic allow to create a database dump mongodump --gzip --archive= --db --port Example : mongodump --gzip --archive=scava.20180906.gz --db scava --port 27018 // project database mongodump --gzip --archive=scava-analysis.20180906.gz --db scava-analysis --port 27018 // analysis process database mongodump --gzip --archive=QualityGuard.20180906.gz --db QualityGuard --port 27018 // QualityGuard project database","title":"Backup Process"},{"location":"installation-guide/database-backup/#restoration-process","text":"The Mongo installation package( windows and linux) provide the mongorestore service which allow to restore a database dump mongorestore --drop --gzip --archive=Activemq.20181203.gz Example : mongorestore --drop --gzip --archive=scava.20181203.gz // project database mongorestore --drop --gzip --archive=scava-analysis.20181203.gz // analysis process database mongorestore --drop --gzip --archive=QualityGuard.20181203.gz // QualityGuard project database","title":"Restoration Process"},{"location":"installation-guide/licencing/","text":"Licencing for Scava The Scava project is licensed under Eclipse Public License - v 2.0 license. Eclipse Public License licensing Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership.","title":"Licencing for Scava"},{"location":"installation-guide/licencing/#licencing-for-scava","text":"The Scava project is licensed under Eclipse Public License - v 2.0 license.","title":"Licencing for Scava"},{"location":"installation-guide/licencing/#eclipse-public-license-licensing","text":"Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership.","title":"Eclipse Public License licensing"},{"location":"installation-guide/docker-installation/","text":"Platform Installation using DOCKER images This page is about how to deploy a SCAVA instance on the behalf of Docker. At the actual stage of the project, there two ways to get started with the docker images: 1. Ready-to-use images are stored on the Crossminer Docker-hub account . 1. Build them from the scava-deployment repository. They have to be built from various Dockerfile's and with help of a docker-compose file. Summary of containers The whole Docker stack consists of 11 services: Docker service name Full Name Default port Comments admin-webapp Administration UI 5601 Built from /web-admin oss-app Metric Plateform 8182 Built from /metric-platform oss-db MongoDB (metrics storage) 27017 Built from /metric-platform to start with a pre-loaded database (few projects analysed) or use a clean MongoDB image for a clean installation ( image: mongo:3.4 ). Can be used to connect a MongoDB visualisation tool kb Knowledge base 8080 Built from /KB kb-db Knowledge base DB (based on MongoDB) 27018 Built from /KB-db. Can be used to connect a MongoDB visualisation tool api-gw API Gateway 8086 Built from /api-gw auth Authentication 8085 Built from /auth elasticsearch ElasticSearch 9200 Pulled from docker hub acsdocker/elasticsearch:6.3.1-secured. Can be used to connect an ElasticSearch visualisation tool kibiter Kibiter (Bitergia\u2019s customized Kibana) 80 Pulled from docker hub acsdocker/grimoirelab-kibiter:crossminer-6.3.1 dashb-importer Dashboard importer (to kibiter) No port exposed on the host prosoul Prosoul Quality Model Viewer 8000 Pulled from docker hub bitergia/prosoul Prerequisites In order to run Scava, you need to: Edit the hosts of your machine, creating a new record for the IP address: 127.0.0.1 with hostname: admin-webapp Edit the docker-compose-build.yml file and change: The environment variable API-GATEWAY_VAR , replacing localhost by the IP address of your host on service admin-webapp Make sure that the ports defined in the file are not already used on the host, and adjust the various ports as required for your setup. Note that the Kibiter dashboard is on port 80 by default. Update the ALLOWED_HOST directive to include the host name on service prosoul . This is used by Django on the prosoul image to publish the quality model used by Crossminer. Building the Docker images The deployment setup is hosted in the scava-deployment repository. One needs to clone the repository locally in order to build and run the docker images. To build all the required Docker images, simply go to the root of the cloned repository and issue the following command. This will rebuild all images, dismissing any cached layers. $ docker-compose -f docker-compose-build.yml build --no-cache This will build required images using the latest version of the binaries on the CI server and pull images hosted on docker hub. Setup Configurations Some optional configurations can be made to customize the dockerized SCAVA instance. These configuration are applied to the docker-compose description files docker-compose-build.yml or docker-compose-dockerhub.yml and are applied every time that the instance runs. Data persistence Volumes can be created to persist the data of the databases between execution of the Scava platform. To enable the creation of volumes uncomemnt the corresponding lines on the oss-db and kb-db services: On oss-db : volumes: #creates volume on container - ~/oss-data:/data/db On kb-db : volumes: #creates volume on container - ~/kb-data:/data/kb-db Workers Configuration Running the locally built docker images To run the locally built images, run the following command. Note that if the images are not available they will be rebuilt. $ docker-compose -f docker-compose-build.yml up Access the administration web app by using the following address in the web browser: http://admin-webapp/ For login use user: admin pass: admin Running the pre-built docker images Please note that the docker hub images are not yet ready! We're working on it! :-) The easiest way to run the full Scava setup is to use the docker images stored on Docker Hub . Use the docker-compose-dockerhub.yml file to download all required images and start the stack: $ docker-compose -f docker-compose-dockerhub.yml up Access the administration web app by using the following address in the web browser: http://admin-webapp/ For login use user: admin pass: admin Post-install tasks Configuring the GitHub token In order to use the GitHub connectors, one needs to setup an authentication mechanism. Simply create a authentication token in yourGitHub account, and create a new property in the webadmin UI: key: githubToken value: the github token created on the github account. Kibana dashboard The first time Kibana is started, it will ask for a default index pattern. To select one, log into the dashboard (admin/admin), go to the dashboard menu item and select metrics-scava . Then click on the star on the top right. Continuous integration We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the #ci channel in Slack.","title":"Platform Installation using DOCKER images"},{"location":"installation-guide/docker-installation/#platform-installation-using-docker-images","text":"This page is about how to deploy a SCAVA instance on the behalf of Docker. At the actual stage of the project, there two ways to get started with the docker images: 1. Ready-to-use images are stored on the Crossminer Docker-hub account . 1. Build them from the scava-deployment repository. They have to be built from various Dockerfile's and with help of a docker-compose file.","title":"Platform Installation using DOCKER images"},{"location":"installation-guide/docker-installation/#summary-of-containers","text":"The whole Docker stack consists of 11 services: Docker service name Full Name Default port Comments admin-webapp Administration UI 5601 Built from /web-admin oss-app Metric Plateform 8182 Built from /metric-platform oss-db MongoDB (metrics storage) 27017 Built from /metric-platform to start with a pre-loaded database (few projects analysed) or use a clean MongoDB image for a clean installation ( image: mongo:3.4 ). Can be used to connect a MongoDB visualisation tool kb Knowledge base 8080 Built from /KB kb-db Knowledge base DB (based on MongoDB) 27018 Built from /KB-db. Can be used to connect a MongoDB visualisation tool api-gw API Gateway 8086 Built from /api-gw auth Authentication 8085 Built from /auth elasticsearch ElasticSearch 9200 Pulled from docker hub acsdocker/elasticsearch:6.3.1-secured. Can be used to connect an ElasticSearch visualisation tool kibiter Kibiter (Bitergia\u2019s customized Kibana) 80 Pulled from docker hub acsdocker/grimoirelab-kibiter:crossminer-6.3.1 dashb-importer Dashboard importer (to kibiter) No port exposed on the host prosoul Prosoul Quality Model Viewer 8000 Pulled from docker hub bitergia/prosoul","title":"Summary of containers"},{"location":"installation-guide/docker-installation/#prerequisites","text":"In order to run Scava, you need to: Edit the hosts of your machine, creating a new record for the IP address: 127.0.0.1 with hostname: admin-webapp Edit the docker-compose-build.yml file and change: The environment variable API-GATEWAY_VAR , replacing localhost by the IP address of your host on service admin-webapp Make sure that the ports defined in the file are not already used on the host, and adjust the various ports as required for your setup. Note that the Kibiter dashboard is on port 80 by default. Update the ALLOWED_HOST directive to include the host name on service prosoul . This is used by Django on the prosoul image to publish the quality model used by Crossminer.","title":"Prerequisites"},{"location":"installation-guide/docker-installation/#building-the-docker-images","text":"The deployment setup is hosted in the scava-deployment repository. One needs to clone the repository locally in order to build and run the docker images. To build all the required Docker images, simply go to the root of the cloned repository and issue the following command. This will rebuild all images, dismissing any cached layers. $ docker-compose -f docker-compose-build.yml build --no-cache This will build required images using the latest version of the binaries on the CI server and pull images hosted on docker hub.","title":"Building the Docker images"},{"location":"installation-guide/docker-installation/#setup-configurations","text":"Some optional configurations can be made to customize the dockerized SCAVA instance. These configuration are applied to the docker-compose description files docker-compose-build.yml or docker-compose-dockerhub.yml and are applied every time that the instance runs.","title":"Setup Configurations"},{"location":"installation-guide/docker-installation/#data-persistence","text":"Volumes can be created to persist the data of the databases between execution of the Scava platform. To enable the creation of volumes uncomemnt the corresponding lines on the oss-db and kb-db services: On oss-db : volumes: #creates volume on container - ~/oss-data:/data/db On kb-db : volumes: #creates volume on container - ~/kb-data:/data/kb-db","title":"Data persistence"},{"location":"installation-guide/docker-installation/#workers-configuration","text":"","title":"Workers Configuration"},{"location":"installation-guide/docker-installation/#running-the-locally-built-docker-images","text":"To run the locally built images, run the following command. Note that if the images are not available they will be rebuilt. $ docker-compose -f docker-compose-build.yml up Access the administration web app by using the following address in the web browser: http://admin-webapp/ For login use user: admin pass: admin","title":"Running the locally built docker images"},{"location":"installation-guide/docker-installation/#running-the-pre-built-docker-images","text":"Please note that the docker hub images are not yet ready! We're working on it! :-) The easiest way to run the full Scava setup is to use the docker images stored on Docker Hub . Use the docker-compose-dockerhub.yml file to download all required images and start the stack: $ docker-compose -f docker-compose-dockerhub.yml up Access the administration web app by using the following address in the web browser: http://admin-webapp/ For login use user: admin pass: admin","title":"Running the pre-built docker images"},{"location":"installation-guide/docker-installation/#post-install-tasks","text":"","title":"Post-install tasks"},{"location":"installation-guide/docker-installation/#configuring-the-github-token","text":"In order to use the GitHub connectors, one needs to setup an authentication mechanism. Simply create a authentication token in yourGitHub account, and create a new property in the webadmin UI: key: githubToken value: the github token created on the github account.","title":"Configuring the GitHub token"},{"location":"installation-guide/docker-installation/#kibana-dashboard","text":"The first time Kibana is started, it will ask for a default index pattern. To select one, log into the dashboard (admin/admin), go to the dashboard menu item and select metrics-scava . Then click on the star on the top right.","title":"Kibana dashboard"},{"location":"installation-guide/docker-installation/#continuous-integration","text":"We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the #ci channel in Slack.","title":"Continuous integration"},{"location":"installation-guide/platform-configuration/","text":"Platform Installation using individual components (UNPARALLEL)","title":"Platform Installation using individual components (UNPARALLEL)"},{"location":"installation-guide/platform-configuration/#platform-installation-using-individual-components-unparallel","text":"","title":"Platform Installation using individual components (UNPARALLEL)"},{"location":"installation-guide/platform-installation/","text":"Platform Installation using individual components (UNPARALLEL)","title":"Platform Installation using individual components (UNPARALLEL)"},{"location":"installation-guide/platform-installation/#platform-installation-using-individual-components-unparallel","text":"","title":"Platform Installation using individual components (UNPARALLEL)"},{"location":"installation-guide/plugin-installation/","text":"Eclipse Plugin Installation","title":"Eclipse Plugin Installation"},{"location":"installation-guide/plugin-installation/#eclipse-plugin-installation","text":"","title":"Eclipse Plugin Installation"},{"location":"others/","text":"Welcome to the Scava documentation This web site is the main documentation place for the Eclipse Scava project. Useful links: Eclipse Scava home project: Eclipse Scava @ Eclipse Eclipse Scava code repository: github.com/crossminer/scava Eclipse Scava deployment repository: github.com/crossminer/scava-deployment Eclipse Scava documentation repository: github.com/crossminer/scava-docs Platform installation Docker-SCAVA How to build and run the Scava docker image. Running the platform Quick start guide to get the Scava platform running from source on an Eclipse development environment. Configuring the platform Quick start guide to present how to configure the platform using a configuration file. Docker-OSSMETER How to build and run the Ossmeter docker image. Administration Scava Administration The administration dashboard take care of managing Scava's services. API Gateway Configuration Extending MongoDB Data Model Users Scava metrics lists metrics computed by the various Scava Components. Consuming the REST services This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues Running Scava in Eclipse How to setup and run the Scava Eclipse IDE plugin. REST API Documentation Reference documentation of REST services provided by the Scava platform. REST API Generation Tutorial about automatic generation of REST API Scava library using OpenAPI. Accessing Scava resources A summary of where to find the various outputs of the Scava platform. Development Contributing Collection of Architectural and Technical guidelines dedicated to Scava developers. Development guidelines Rules and guidelines used for the development of the Scava project. Testing Guidelines Collection of testing guidelines dedicated to Scava developers. Repository-Organisation How to develop a metric provider Want to add a new metric provider? Here are some hints. Licensing Information about licensing used within Scava. Architecture API Gateway Component The API Gateway allows to access to all Scava REST services using a centralised and securised common gateway. Authentication Component The administration dashboard takes care of managing Scava's services.","title":"Welcome to the Scava documentation"},{"location":"others/#welcome-to-the-scava-documentation","text":"This web site is the main documentation place for the Eclipse Scava project. Useful links: Eclipse Scava home project: Eclipse Scava @ Eclipse Eclipse Scava code repository: github.com/crossminer/scava Eclipse Scava deployment repository: github.com/crossminer/scava-deployment Eclipse Scava documentation repository: github.com/crossminer/scava-docs","title":"Welcome to the Scava documentation"},{"location":"others/#platform-installation","text":"Docker-SCAVA How to build and run the Scava docker image. Running the platform Quick start guide to get the Scava platform running from source on an Eclipse development environment. Configuring the platform Quick start guide to present how to configure the platform using a configuration file. Docker-OSSMETER How to build and run the Ossmeter docker image.","title":"Platform installation"},{"location":"others/#administration","text":"Scava Administration The administration dashboard take care of managing Scava's services. API Gateway Configuration Extending MongoDB Data Model","title":"Administration"},{"location":"others/#users","text":"Scava metrics lists metrics computed by the various Scava Components. Consuming the REST services This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues Running Scava in Eclipse How to setup and run the Scava Eclipse IDE plugin. REST API Documentation Reference documentation of REST services provided by the Scava platform. REST API Generation Tutorial about automatic generation of REST API Scava library using OpenAPI. Accessing Scava resources A summary of where to find the various outputs of the Scava platform.","title":"Users"},{"location":"others/#development","text":"Contributing Collection of Architectural and Technical guidelines dedicated to Scava developers. Development guidelines Rules and guidelines used for the development of the Scava project. Testing Guidelines Collection of testing guidelines dedicated to Scava developers. Repository-Organisation How to develop a metric provider Want to add a new metric provider? Here are some hints. Licensing Information about licensing used within Scava.","title":"Development"},{"location":"others/#architecture","text":"API Gateway Component The API Gateway allows to access to all Scava REST services using a centralised and securised common gateway. Authentication Component The administration dashboard takes care of managing Scava's services.","title":"Architecture"},{"location":"others/admin/API-Gateway-Configuration/","text":"API Gateway configuration When to use this guideline ? This guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API. Context The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters. Routing : Service Configuration To reference a new remote REST API in the gateway, you have to add 2 new properties in the application.properties configuration file : the relative path of services which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process. Examples : * api gateway url = http://85.36.10.13:8080 * path = /administration/** * url = http://85.36.10.12:8082/administration The request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1 Configuration file example # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2 Comments More information about API Gateway configuration: API Gateway Component","title":"API Gateway configuration"},{"location":"others/admin/API-Gateway-Configuration/#api-gateway-configuration","text":"","title":"API Gateway configuration"},{"location":"others/admin/API-Gateway-Configuration/#when-to-use-this-guideline","text":"This guideline presents how to configure the Scava Gateway in order to integrate a new remote REST API.","title":"When to use this guideline ?"},{"location":"others/admin/API-Gateway-Configuration/#context","text":"The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.","title":"Context"},{"location":"others/admin/API-Gateway-Configuration/#routing-service-configuration","text":"To reference a new remote REST API in the gateway, you have to add 2 new properties in the application.properties configuration file : the relative path of services which will be integrated to this route and the redirection URL.All requests sent to the gateway which start by this relatice path will be redirected to the output url after the authentication process. Examples : * api gateway url = http://85.36.10.13:8080 * path = /administration/** * url = http://85.36.10.12:8082/administration The request http://85.36.10.13:8080/administration/project/create will be redirected to http://85.36.10.12:8082/administration/project/create id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1","title":"Routing : Service Configuration"},{"location":"others/admin/API-Gateway-Configuration/#configuration-file-example","text":"# Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2","title":"Configuration file example"},{"location":"others/admin/API-Gateway-Configuration/#comments","text":"More information about API Gateway configuration: API Gateway Component","title":"Comments"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/","text":"When to use ? This guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas. Context The Scava platform use a MongoDb data base to store his data. We go through PONGO , a template based Java POJO generator to access MongoDB database. With Pongo we can define the data model which generates strongly-typed Java classes. In this guideligne, we will present : * The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform. * The access to MongoDb Document on from an external Java application. * How to preform basic CRUD operation with a PONGO Java data model. We consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model : Extend MongoDB Data Model . You want to access to MongoDB Document from an Eclipse Plugin ? 1. Add a dependency to the Java Data Model Edit the plugin.xml file of your plugin. In Dependency section, add a dependency to the plugin which contained the data model you went to access. To org.ossmeter.repository.model to access data related to project administration , metric execution process and authentification system. To org.ossmeter.repository.model.'project delta manager' to access configuration informations related to source codes managemeny tools. To specific metric provider plugins to access data related to a specific metric provider implementation contains his once data model. ... others plugin which contained the data model In Dependency section, add a dependency to com.googlecode.pongo.runtime plugin 2. Initiate a Connection to the MongoDb In context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base. In Dependency section of plugin.xml file , add a dependency to the org.ossmeter.platform plugin. You can now create a new connection to the database using the Configuration service. Mongo mongo = Configuration.getInstance().getMongoConnection(); You want to access to MongoDB Document on from an External Java Application ? 1. Add a dependency to the Java Data Model Add to your java project a dependency to the jar which contained the data model you went to access. You will have to deliver this jar with your application. Add o your java project a dependency to the pongo.jar jar file which can be download at this url : https://github.com/kolovos/pongo/releases 2. Initiate a Connection to MongoDb // Define ServerAddress of the MongoDb database List<ServerAddress> mongoHostAddresses = new ArrayList<>(); mongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1]))); // Create Connection Mongo mongo = new Mongo(mongoHostAddresses); Once the connection to MongoDb has been created, you have to make the link between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document. DB db = mongo.getDB(\"databasename\"); // Initiate the Project Java model ProjectRepository = new ProjectRepository(db); Basic CRUD with a PONGO Java data model 1. CREATE // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true); 2. READ // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to access object properties metricprovider.getname(); ...... 3. UPDATE // Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true); 4. DELETE Mongo mongo = new Mongo(); mongo.dropDatabase(\"databasename\"); Comment This wiki has dealt with the access of MongoDB database using PONGO. To continue learning how to modify and make a new model with Pongo, we have another page here Extend MongoDB Data Model .","title":"Access to MongoDB database using PONGO"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#when-to-use","text":"This guideline present How to the Scava Platform can access to his data stored in a MongoDb data base. The guideline describe how to create a connection to the MongoDb database and how to perform CRUD operation on platform datas.","title":"When to use ?"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#context","text":"The Scava platform use a MongoDb data base to store his data. We go through PONGO , a template based Java POJO generator to access MongoDB database. With Pongo we can define the data model which generates strongly-typed Java classes. In this guideligne, we will present : * The access to MongoDb Document on from aneclipse plugin integrate to the Scava platform. * The access to MongoDb Document on from an external Java application. * How to preform basic CRUD operation with a PONGO Java data model. We consider that the PONGO Java data model already exist. If it's not the case, please refer the following guideline to create the data model : Extend MongoDB Data Model .","title":"Context"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#you-want-to-access-to-mongodb-document-from-an-eclipse-plugin","text":"","title":"You want to access to MongoDB Document from an Eclipse Plugin ?"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#1-add-a-dependency-to-the-java-data-model","text":"Edit the plugin.xml file of your plugin. In Dependency section, add a dependency to the plugin which contained the data model you went to access. To org.ossmeter.repository.model to access data related to project administration , metric execution process and authentification system. To org.ossmeter.repository.model.'project delta manager' to access configuration informations related to source codes managemeny tools. To specific metric provider plugins to access data related to a specific metric provider implementation contains his once data model. ... others plugin which contained the data model In Dependency section, add a dependency to com.googlecode.pongo.runtime plugin","title":"1. Add a dependency to the Java Data Model"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#2-initiate-a-connection-to-the-mongodb","text":"In context of the Scava platform, a Configuration service allow you to initiate a connection whit the mongoDb data base. In Dependency section of plugin.xml file , add a dependency to the org.ossmeter.platform plugin. You can now create a new connection to the database using the Configuration service. Mongo mongo = Configuration.getInstance().getMongoConnection();","title":"2. Initiate a Connection to the MongoDb"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#you-want-to-access-to-mongodb-document-on-from-an-external-java-application","text":"","title":"You want to access to MongoDB Document on from an External Java Application ?"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#1-add-a-dependency-to-the-java-data-model_1","text":"Add to your java project a dependency to the jar which contained the data model you went to access. You will have to deliver this jar with your application. Add o your java project a dependency to the pongo.jar jar file which can be download at this url : https://github.com/kolovos/pongo/releases","title":"1. Add a dependency to the Java Data Model"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#2-initiate-a-connection-to-mongodb","text":"// Define ServerAddress of the MongoDb database List<ServerAddress> mongoHostAddresses = new ArrayList<>(); mongoHostAddresses.add(new ServerAddress(s[0], Integer.valueOf(s[1]))); // Create Connection Mongo mongo = new Mongo(mongoHostAddresses); Once the connection to MongoDb has been created, you have to make the link between the PONGO Java model and the database. On a MongoDb Server, data are organize by database. You need to know the name of the database to link the Java model with the MongoDb document. DB db = mongo.getDB(\"databasename\"); // Initiate the Project Java model ProjectRepository = new ProjectRepository(db);","title":"2. Initiate a Connection to MongoDb"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#basic-crud-with-a-pongo-java-data-model","text":"","title":"Basic CRUD with a PONGO Java data model"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#1-create","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true);","title":"1. CREATE"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#2-read","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to access object properties metricprovider.getname(); ......","title":"2. READ"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#3-update","text":"// Connect the Data model to the database DB db = mongo.getDB(\"databasename\"); MetricProvider metricprovider = new MetricProvider(db); // Used accessors to intialise the object metricprovider.setName(\"Metric1\"). ...... // Create the Document metricprovider.sync(true);","title":"3. UPDATE"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#4-delete","text":"Mongo mongo = new Mongo(); mongo.dropDatabase(\"databasename\");","title":"4. DELETE"},{"location":"others/admin/Access-to-MongoDB-database-using-PONGO/#comment","text":"This wiki has dealt with the access of MongoDB database using PONGO. To continue learning how to modify and make a new model with Pongo, we have another page here Extend MongoDB Data Model .","title":"Comment"},{"location":"others/admin/SCAVA-Administration/","text":"Scava Administration The SCAVA administration dashboard take care of: Provide user administration feature, including user profile activation service and roles based authorization management. Provide services to analyse automatically open source software projects. Administration Dashboard Installation Prerequired The SCAVA administration dashboard is front end web application based on Angular Framework. It can be executed in both Linux or Windows systems where it's required the installation of the following tools: Node.js Download Node.js ver. 8.11.2 (includes npm 5.6.0) or above : https://nodejs.org/en/download/ Yarn Package Manager Download Yarn ver. 1.7.0 or above : https://yarnpkg.com Get Started Scava Administration Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart). Scava Dashboard Deployment In order to deploy the Scava Administration, you must to build and copy the output directory to a web server (For instance Apache HTTP Server). Using the development profile: Execute the development build using the Angular CLI command line : ng build . Copy everything within the output folder (dist/ by default) to a folder on the server. If you copy the files into a server sub-folder, append the build flag, --base-href and set the appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to like this. or simply you can run: ng build --base-href=/administration/ Using the production profile: You can generate an optimized build with additional CLI command line flags: ng build -- prod . Copy everything within the output folder (dist/ by default) to a folder on the server. If you copy the files into a server sub-folder, append the build flag, --base-href and set the appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to like this. or simply you can run: ng build --base-href=/administration/ .","title":"Scava Administration"},{"location":"others/admin/SCAVA-Administration/#scava-administration","text":"The SCAVA administration dashboard take care of: Provide user administration feature, including user profile activation service and roles based authorization management. Provide services to analyse automatically open source software projects.","title":"Scava Administration"},{"location":"others/admin/SCAVA-Administration/#administration-dashboard-installation","text":"","title":"Administration Dashboard Installation"},{"location":"others/admin/SCAVA-Administration/#prerequired","text":"The SCAVA administration dashboard is front end web application based on Angular Framework. It can be executed in both Linux or Windows systems where it's required the installation of the following tools:","title":"Prerequired"},{"location":"others/admin/SCAVA-Administration/#nodejs","text":"Download Node.js ver. 8.11.2 (includes npm 5.6.0) or above : https://nodejs.org/en/download/","title":"Node.js"},{"location":"others/admin/SCAVA-Administration/#yarn-package-manager","text":"Download Yarn ver. 1.7.0 or above : https://yarnpkg.com","title":"Yarn Package Manager"},{"location":"others/admin/SCAVA-Administration/#get-started-scava-administration","text":"Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart).","title":"Get Started Scava Administration"},{"location":"others/admin/SCAVA-Administration/#scava-dashboard-deployment","text":"In order to deploy the Scava Administration, you must to build and copy the output directory to a web server (For instance Apache HTTP Server).","title":"Scava Dashboard Deployment"},{"location":"others/admin/SCAVA-Administration/#using-the-development-profile","text":"Execute the development build using the Angular CLI command line : ng build . Copy everything within the output folder (dist/ by default) to a folder on the server. If you copy the files into a server sub-folder, append the build flag, --base-href and set the appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to like this. or simply you can run: ng build --base-href=/administration/","title":"Using the development profile:"},{"location":"others/admin/SCAVA-Administration/#using-the-production-profile","text":"You can generate an optimized build with additional CLI command line flags: ng build -- prod . Copy everything within the output folder (dist/ by default) to a folder on the server. If you copy the files into a server sub-folder, append the build flag, --base-href and set the appropriately. For example, if the index.html is on the server at /administration/index.html, set the base href to like this. or simply you can run: ng build --base-href=/administration/ .","title":"Using the production profile:"},{"location":"others/architecture/API-Gateway-Component/","text":"The API Gateway component The Scava API Gateway : Provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application). Provide a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services. API Gateway Architecture The API Gateway is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change. The API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the API Gateway will manage the authentication for all services of the platform. Authentication Mechanism JSON Web Tokens The Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io) Authentication Architecture In Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway. Authentication Flow To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Implementation The implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures. https://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul API Gateway Configuration The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters. Server Configuration id : server.port default : 8086 Port of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port. JWT Security Configuration id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. id : apigateway.security.jwt.url default : /login URL Path of the authentication service. id : apigateway.security.jwt.expiration default : 86400 (24H) Port of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port. Routing : Authentication Service Configuration id : zuul.routes.auth-center.path default : /api/authentication/** Relative path of the authentication service. id : zuul.routes.auth-center.url default : NA URL of the authentification server. Example: http://127.0.0.1:8081/ id : zuul.routes.auth-center.sensitiveHeaders default : Cookie,Set-Cookie Specify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers. id : zuul.routes.auth-center.stripPrefix default : false Switch off the stripping of the service-specific prefix from individual routes Routing : Service Configuration id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1 Configuration file example #API Gateway Port server.port=8086 # JWT Configuration apigateway.security.jwt.secret=otherpeopledontknowit apigateway.security.jwt.url=/api/authentication apigateway.security.jwt.expiration=86400 # Rooting Configuration : Authentication Service zuul.routes.auth-center.path=/api/authentication/** zuul.routes.auth-center.url=http://127.0.0.1:8081/ zuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie zuul.routes.auth-center.stripPrefix=false # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2 Control access API The Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including: * \u201cROLE_ADMIN\u201d * \u201cROLE_PROJECT_MANAGER\u201d * \u201cROLE_USER\u201d By the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d. # Filtering private restApi scava.routes.config.adminAccessApi[0]=/api/users scava.routes.config.adminAccessApi[1]=/api/user/** scava.routes.config.projectManagerAccessApi[0]=/administration/projects/create scava.routes.config.projectManagerAccessApi[1]=/administration/projects/import scava.routes.config.projectManagerAccessApi[2]=/administration/analysis/** scava.routes.config.userAccessApi[0]=/administration/projects scava.routes.config.userAccessApi[1]=/administration/projects/p/** scava.routes.config.userAccessApi[2]=/api/users/** scava.routes.config.userAccessApi[3]=/api/account Packaging Form Sources Maven Packaging mvn -Pprod install API Gateway Execution complete an put the \"application.properties\" configuration file in the execute directory. Execute the crossmeter-api-gateway-1.0.0.jar Jar. java -jar scava-api-gateway-1.0.0.jar Client Implementation How to consume a Scava REST services ? \\ This guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.","title":"The API Gateway component"},{"location":"others/architecture/API-Gateway-Component/#the-api-gateway-component","text":"The Scava API Gateway : Provide a centralized access point to all web services implemented by the differents tools involved in the platform (DevOps Dashboard, Workflow Execution Engine, Knowledge Base, Metric Provider, Administration Application). Provide a centralized mechanisms to secuerize Scava web services and manage authentication required to access to this services.","title":"The API Gateway component"},{"location":"others/architecture/API-Gateway-Component/#api-gateway-architecture","text":"The API Gateway is a pattern which come form microserivces echosystem. An API Gateway is a single point of entry (and control) for front end clients, which could be browser based or mobile. The client only has to know the URL of one server, and the backend can be refactored at will with no change. The API Gateway act as a revers web proxy in which can be integrated others functions like load balancing and authentication. In case of the Scava platform, the API Gateway will manage the authentication for all services of the platform.","title":"API Gateway Architecture"},{"location":"others/architecture/API-Gateway-Component/#authentication-mechanism","text":"","title":"Authentication Mechanism"},{"location":"others/architecture/API-Gateway-Component/#json-web-tokens","text":"The Scava API Gateway is secruized using JSON Web Tokens (JWT) mechanism, an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. In authentication, when the user successfully logs in using their credentials, a JSON Web Token will be returned and must be saved locally, instead of the traditional approach of creating a session in the server and returning a cookie.When the client request an access to a protected service ,the server's protected routes will check for a valid JWT in the Authorization header of the request, and if it's present, the user will be allowed to access protected resources. (More about JWT : https://jwt.io)","title":"JSON Web Tokens"},{"location":"others/architecture/API-Gateway-Component/#authentication-architecture","text":"In Scava, the authentification service is a sub component of the Administration Application which centralise Right Management for the whole platform. As for the others services, the authentication service is accessed behind the API Gateway.","title":"Authentication Architecture"},{"location":"others/architecture/API-Gateway-Component/#authentication-flow","text":"To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service.","title":"Authentication Flow"},{"location":"others/architecture/API-Gateway-Component/#implementation","text":"The implementation of the gateway is based on Zuul proxy, a component of the spring-cloud project, an extention of the spring framework dedicated to microservices architectures. https://projects.spring.io/spring-cloud/spring-cloud.html#_router_and_filter_zuul","title":"Implementation"},{"location":"others/architecture/API-Gateway-Component/#api-gateway-configuration","text":"The Scava Gateway can be configured by the intermediary of an external property file (application.properties) to place in the execution directory of the Scava Gateway component. This file allow to configure the routing of requests send to the gateway an some security parameters.","title":"API Gateway Configuration"},{"location":"others/architecture/API-Gateway-Component/#server-configuration","text":"id : server.port default : 8086 Port of the CORSSMINER API server. Each REST request sent to the gateway must be addressed to this port.","title":"Server Configuration"},{"location":"others/architecture/API-Gateway-Component/#jwt-security-configuration","text":"id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. id : apigateway.security.jwt.url default : /login URL Path of the authentication service. id : apigateway.security.jwt.expiration default : 86400 (24H) Port of the CORSSMINER API server. Each request sent to the gateway must be addressed to this port.","title":"JWT Security Configuration"},{"location":"others/architecture/API-Gateway-Component/#routing-authentication-service-configuration","text":"id : zuul.routes.auth-center.path default : /api/authentication/** Relative path of the authentication service. id : zuul.routes.auth-center.url default : NA URL of the authentification server. Example: http://127.0.0.1:8081/ id : zuul.routes.auth-center.sensitiveHeaders default : Cookie,Set-Cookie Specify a list of ignored headers as part of the route configuration which will be not leaking downstream into external servers. id : zuul.routes.auth-center.stripPrefix default : false Switch off the stripping of the service-specific prefix from individual routes","title":"Routing : Authentication Service Configuration"},{"location":"others/architecture/API-Gateway-Component/#routing-service-configuration","text":"id : zuul.routes.**servicename**.path default : NA Relative path of the incoming service which will be redirected. Example : /test1/** id : zuul.routes.**servicename**.url default : NA Redirection URL of the route. Example : http://127.0.0.1:8082/test1","title":"Routing : Service Configuration"},{"location":"others/architecture/API-Gateway-Component/#configuration-file-example","text":"#API Gateway Port server.port=8086 # JWT Configuration apigateway.security.jwt.secret=otherpeopledontknowit apigateway.security.jwt.url=/api/authentication apigateway.security.jwt.expiration=86400 # Rooting Configuration : Authentication Service zuul.routes.auth-center.path=/api/authentication/** zuul.routes.auth-center.url=http://127.0.0.1:8081/ zuul.routes.auth-center.sensitiveHeaders=Cookie,Set-Cookie zuul.routes.auth-center.stripPrefix=false # Rooting Configuration : Test1 Service zuul.routes.test1.path=/test1/** zuul.routes.test1.url=http://127.0.0.1:8082/test1 # Rooting Configuration : Test2 Service zuul.routes.test2.path=/test2/** zuul.routes.test2.url=http://127.0.0.1:8083/test2","title":"Configuration file example"},{"location":"others/architecture/API-Gateway-Component/#control-access-api","text":"The Scava platform comes with public and private APIs to control the access to the REST API using different permission levels. By default, there are three authorization levels which are predefined to get access to all the Scava's APIS, including: * \u201cROLE_ADMIN\u201d * \u201cROLE_PROJECT_MANAGER\u201d * \u201cROLE_USER\u201d By the way, The frontend SCAVA administration comes with a default \u201cadmin\u201d who is mainly the admin user with all authorities access including \u201cROLE_ADMIN\u201d, \u201cROLE_PROJECT_MANAGER\u201d and \u201cROLE_USER\u201d authorizations. His default password is \u201cadmin\u201d. # Filtering private restApi scava.routes.config.adminAccessApi[0]=/api/users scava.routes.config.adminAccessApi[1]=/api/user/** scava.routes.config.projectManagerAccessApi[0]=/administration/projects/create scava.routes.config.projectManagerAccessApi[1]=/administration/projects/import scava.routes.config.projectManagerAccessApi[2]=/administration/analysis/** scava.routes.config.userAccessApi[0]=/administration/projects scava.routes.config.userAccessApi[1]=/administration/projects/p/** scava.routes.config.userAccessApi[2]=/api/users/** scava.routes.config.userAccessApi[3]=/api/account","title":"Control access API"},{"location":"others/architecture/API-Gateway-Component/#packaging-form-sources","text":"Maven Packaging mvn -Pprod install","title":"Packaging Form Sources"},{"location":"others/architecture/API-Gateway-Component/#api-gateway-execution","text":"complete an put the \"application.properties\" configuration file in the execute directory. Execute the crossmeter-api-gateway-1.0.0.jar Jar. java -jar scava-api-gateway-1.0.0.jar","title":"API Gateway Execution"},{"location":"others/architecture/API-Gateway-Component/#client-implementation","text":"How to consume a Scava REST services ? \\ This guideline is dedicated to clients which would like to use the REST Services. It adresses authentication issues.","title":"Client Implementation"},{"location":"others/architecture/Authentication-Component/","text":"Authentication Component The Scava Authentication service: Provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform. Provides user management services, including user registration process, user profile editing and roles based authorization management. Authentication API The Authentication server is a component of The Scava platform which manages the authentication for all services accessible behind the API Gateway. Authenticate User POST /api/authentication Login as a registered user. ### JSON Web Tokens (JWT) JSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are: * Header * Payload * Signature This solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/). ### JWT Authentication Implementation * Users have to login to the authentication service API using their credentials username and password. curl -i -X POST -H \"Content-Type:application/json\" http://localhost:8086/api/authentication -d '{\"username\":\"admin\", \"password\": \"admin\"}' * Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header. * The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway. curl -i -X GET -H \"Content-Type:application/json\" -H \"Authorization:Bearer eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1dGhvcml0aWVzIjpbIlJPTEVfQURNSU4iLCJST0xFX1BST0pFQ1RfTUFOQUdFUiIsIlJPTEVfVVNFUiJdLCJpYXQiOjE1MzE4OTk3NDMsImV4cCI6MTUzMTk4NjE0M30.l-iCJcnae-1mlhMb3_y09HM4HZYFaHxe_JctWi2FRUY\" http://localhost:8086/api/users ## User Management API The Authentication component provides web services for CRUD user account. Register User POST /api/register Register new user. Activate User GET /api/activate Activate the registered user. Update User PUT /api/users Update an existing user. Retrieve Users GET /api/users Get all registered users. Retrieve Login User GET /api/users/{login} Get the \"login\" user. Delete User DELETE /api/users/{login} Delete the \"login\" user. Authentication Server Configuration The Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component. Server Configuration id : server.port default : 8085 Port of the Authentication API server. Each REST request sent to the gateway must be adressed to this port. JWT Security Configuration id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA. Default ADMIN configuration Property Description Default Value scava.administration.username The administrator username admin scava.administration.password The administrator password admin scava.administration.admin-role The admin role ADMIN scava.administration.project-manager-role The project manager role PROJECT_MANAGER scava.administration.project-user-role The user role USER Mongodb Database Configuration Property Description Default Value spring.data.mongodb.uri Url of the MongoDB database server mongodb://localhost:27017 spring.data.mongodb.database Name of the MongoDB database scava Mail Server configuration In order to register new users, you have to configure a mail server. Property Description Default Value spring.mail.host Url of the mail service smtp.gmail.com spring.mail.port Port of the mail service 587 spring.mail.username Login of the mail account spring.mail.password Password of the mail account spring.mail.protocol mail protocole smtp spring.mail.tls - true spring.mail.properties.mail.smtp.auth - true spring.mail.properties.mail.smtp.starttls.enable - true spring.mail.properties.mail.smtp.ssl.trust= - smtp.gmail.com Administration Dashboard Setting id : scava.administration.base-url default : http://localhost:4200 The SCAVA administration base URL to generate the activation account URL. Packaging From Sources Maven Packaging mvn -Pprod install Authentication Server Execution complete an put the \"application.properties\" configuration file in the execution directory. Execute the scava-auth-service-1.0.0.jar Jar. java -jar scava-auth-service-1.0.0.jar","title":"Authentication Component"},{"location":"others/architecture/Authentication-Component/#authentication-component","text":"The Scava Authentication service: Provides a centralized mechanisms to securize Scava's components and manage authentication for all services of the platform. Provides user management services, including user registration process, user profile editing and roles based authorization management.","title":"Authentication Component"},{"location":"others/architecture/Authentication-Component/#authentication-api","text":"The Authentication server is a component of The Scava platform which manages the authentication for all services accessible behind the API Gateway. Authenticate User POST /api/authentication Login as a registered user. ### JSON Web Tokens (JWT) JSON Web Token (JWT) is an open industry standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. It consists of three parts separated by dots (.), which are: * Header * Payload * Signature This solution uses a secure token that holds the information that we want to transmit and other information about our token, basically the user\u2019s **login name** and **authorities**. (Find more about JWT: https://jwt.io/). ### JWT Authentication Implementation * Users have to login to the authentication service API using their credentials username and password. curl -i -X POST -H \"Content-Type:application/json\" http://localhost:8086/api/authentication -d '{\"username\":\"admin\", \"password\": \"admin\"}' * Once the user is authenticated, he will get a JWT token in the HTTP Response Authorization Header. * The generated token will be used by injecting it inside the HTTP Request Authorization Header to get access to the different Scava's components behind the API Gateway. curl -i -X GET -H \"Content-Type:application/json\" -H \"Authorization:Bearer eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImF1dGhvcml0aWVzIjpbIlJPTEVfQURNSU4iLCJST0xFX1BST0pFQ1RfTUFOQUdFUiIsIlJPTEVfVVNFUiJdLCJpYXQiOjE1MzE4OTk3NDMsImV4cCI6MTUzMTk4NjE0M30.l-iCJcnae-1mlhMb3_y09HM4HZYFaHxe_JctWi2FRUY\" http://localhost:8086/api/users ## User Management API The Authentication component provides web services for CRUD user account. Register User POST /api/register Register new user. Activate User GET /api/activate Activate the registered user. Update User PUT /api/users Update an existing user. Retrieve Users GET /api/users Get all registered users. Retrieve Login User GET /api/users/{login} Get the \"login\" user. Delete User DELETE /api/users/{login} Delete the \"login\" user.","title":"Authentication API"},{"location":"others/architecture/Authentication-Component/#authentication-server-configuration","text":"The Authentication server parametrize inside an external property file (application.properties) placed in the same execution directory of the Authentication component.","title":"Authentication Server Configuration"},{"location":"others/architecture/Authentication-Component/#server-configuration","text":"id : server.port default : 8085 Port of the Authentication API server. Each REST request sent to the gateway must be adressed to this port.","title":"Server Configuration"},{"location":"others/architecture/Authentication-Component/#jwt-security-configuration","text":"id : apigateway.security.jwt.secret default : NA Private key pair which allow to sign jwt tokens using RSA.","title":"JWT Security Configuration"},{"location":"others/architecture/Authentication-Component/#default-admin-configuration","text":"Property Description Default Value scava.administration.username The administrator username admin scava.administration.password The administrator password admin scava.administration.admin-role The admin role ADMIN scava.administration.project-manager-role The project manager role PROJECT_MANAGER scava.administration.project-user-role The user role USER","title":"Default ADMIN configuration"},{"location":"others/architecture/Authentication-Component/#mongodb-database-configuration","text":"Property Description Default Value spring.data.mongodb.uri Url of the MongoDB database server mongodb://localhost:27017 spring.data.mongodb.database Name of the MongoDB database scava","title":"Mongodb Database Configuration"},{"location":"others/architecture/Authentication-Component/#mail-server-configuration","text":"In order to register new users, you have to configure a mail server. Property Description Default Value spring.mail.host Url of the mail service smtp.gmail.com spring.mail.port Port of the mail service 587 spring.mail.username Login of the mail account spring.mail.password Password of the mail account spring.mail.protocol mail protocole smtp spring.mail.tls - true spring.mail.properties.mail.smtp.auth - true spring.mail.properties.mail.smtp.starttls.enable - true spring.mail.properties.mail.smtp.ssl.trust= - smtp.gmail.com","title":"Mail Server configuration"},{"location":"others/architecture/Authentication-Component/#administration-dashboard-setting","text":"id : scava.administration.base-url default : http://localhost:4200 The SCAVA administration base URL to generate the activation account URL.","title":"Administration Dashboard Setting"},{"location":"others/architecture/Authentication-Component/#packaging-from-sources","text":"Maven Packaging mvn -Pprod install","title":"Packaging From Sources"},{"location":"others/architecture/Authentication-Component/#authentication-server-execution","text":"complete an put the \"application.properties\" configuration file in the execution directory. Execute the scava-auth-service-1.0.0.jar Jar. java -jar scava-auth-service-1.0.0.jar","title":"Authentication Server Execution"},{"location":"others/deploy/Docker-Ossmeter/","text":"Docker Ossmeter This page lists information about the Ossmeter docker image. It should be noted that the generated image uses the old Ossmeter binaries and will not be updated -- all new development will go into the Crossminer repository. All images are stored on the Crossminer Docker-hub account . The Docker image is composed of 4 services: oss-web: corresponds to the service of OSSMETER platform website. oss-app: service running api server and the orchestrator of OSSMETER slave instances. oss-slave: service corresponding to the OSSMETER slaves responsible for the analysis of software projects. There can be several slaves serving the same master for load balancing. oss-db: service responsible for the the storage of OSSMETER data. Uses a MongoDB image. The database comes pre-populated with a project and a user. The loaded dump comes from md2manoppello's repo . Login information: user: demo@crossminer.org password: demo18 Custom quality is in the user object (demo@crossminer.org) stored in the users collection of users db. It resembles the demo quality model . Running the Ossmeter docker image The easiest way to build the full stack is to run the docker-compose file: $ docker-compose up This command will download the images and run them. The application is then available on localhost:9000 . Building the Ossmeter docker image Two containers actually need to be built. They can be built individually. oss-platform Build the image from the oss-platform directory: $ docker build -t bbaldassari/ossmeter-platform . Sending build context to Docker daemon 3.072kB Step 1/5 : FROM openjdk:8-jdk oss-web Build the image from the oss-web directory: $ docker build -t bbaldassari/ossmeter-web . Sending build context to Docker daemon 3.072kB Step 1/7 : FROM openjdk:8-jre-alpine Continuous integration We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the #ci channel in Slack.","title":"Docker Ossmeter"},{"location":"others/deploy/Docker-Ossmeter/#docker-ossmeter","text":"This page lists information about the Ossmeter docker image. It should be noted that the generated image uses the old Ossmeter binaries and will not be updated -- all new development will go into the Crossminer repository. All images are stored on the Crossminer Docker-hub account . The Docker image is composed of 4 services: oss-web: corresponds to the service of OSSMETER platform website. oss-app: service running api server and the orchestrator of OSSMETER slave instances. oss-slave: service corresponding to the OSSMETER slaves responsible for the analysis of software projects. There can be several slaves serving the same master for load balancing. oss-db: service responsible for the the storage of OSSMETER data. Uses a MongoDB image. The database comes pre-populated with a project and a user. The loaded dump comes from md2manoppello's repo . Login information: user: demo@crossminer.org password: demo18 Custom quality is in the user object (demo@crossminer.org) stored in the users collection of users db. It resembles the demo quality model .","title":"Docker Ossmeter"},{"location":"others/deploy/Docker-Ossmeter/#running-the-ossmeter-docker-image","text":"The easiest way to build the full stack is to run the docker-compose file: $ docker-compose up This command will download the images and run them. The application is then available on localhost:9000 .","title":"Running the Ossmeter docker image"},{"location":"others/deploy/Docker-Ossmeter/#building-the-ossmeter-docker-image","text":"Two containers actually need to be built. They can be built individually.","title":"Building the Ossmeter docker image"},{"location":"others/deploy/Docker-Ossmeter/#oss-platform","text":"Build the image from the oss-platform directory: $ docker build -t bbaldassari/ossmeter-platform . Sending build context to Docker daemon 3.072kB Step 1/5 : FROM openjdk:8-jdk","title":"oss-platform"},{"location":"others/deploy/Docker-Ossmeter/#oss-web","text":"Build the image from the oss-web directory: $ docker build -t bbaldassari/ossmeter-web . Sending build context to Docker daemon 3.072kB Step 1/7 : FROM openjdk:8-jre-alpine","title":"oss-web"},{"location":"others/deploy/Docker-Ossmeter/#continuous-integration","text":"We use Codefresh for the CI of our docker images. The latest demo instance of generated docker images can be found in the #ci channel in Slack.","title":"Continuous integration"},{"location":"others/deploy/Platform-configuration/","text":"When starting the platform, you can pass a configuration file to control the behaviour of the platform: ./eclipse -slave -config myconfiguration.properties The configuration file is a typical Java properties file. The properties that can be configured are: identifier=<your name> The identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID. If you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers. log.type=console|file|rolling You can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify file or rolling , you must complete the log.file.path or log.rolling.path property as well. If the property is not specified, it will default to the console logger. log.file.path=<path> The path to the file to store the log. E.g. log.file.path=/tmp/lovelylog.log log.rolling.path=<path> The path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify log.rolling.path=/tmp/mylovelylog.log , it will store files like so: /tmp/mylovelylog.log.2014-12-17-00 and /tmp/mylovelylog.log.2014-12-17-12 . maven_executable=<path> The path to where Maven is installed. E.g. maven_executable=/usr/bin/mvn storage_path=<path> The path to where files should be stored. E.g. storage_path=/mnt/ossmeter/ mongo_hosts A comma-separated list of the hosts and ports in a replica set. E.g. ua002:27017,ua009:27017,ua019:27017,ua020:27017","title":"Platform configuration"},{"location":"others/deploy/Platform-configuration/#identifierltyour-namegt","text":"The identifier of the node. If not specified, the platform will attempt to use the node's hostname, and if it cannot resolve the hostname, it will generated a random UUID. If you plan to multiple instances of the platform on the same machine, you should definitely specify different node identifiers.","title":"identifier=&lt;your name&gt;"},{"location":"others/deploy/Platform-configuration/#logtypeconsolefilerolling","text":"You can specify whether to log output to the console (Log4J's ConsoleAppender), to a particular file without a size limit (Log4J's FileAppender), or to a different file per day (Log4J's DailyRollingFileAppender). If you specify file or rolling , you must complete the log.file.path or log.rolling.path property as well. If the property is not specified, it will default to the console logger.","title":"log.type=console|file|rolling"},{"location":"others/deploy/Platform-configuration/#logfilepathltpathgt","text":"The path to the file to store the log. E.g. log.file.path=/tmp/lovelylog.log","title":"log.file.path=&lt;path&gt;"},{"location":"others/deploy/Platform-configuration/#logrollingpathltpathgt","text":"The path to the file to store the log. This is a Log4J DailyRollingFileAppender that will create separate logs for each 12 hours of the day. The date stamp will be appended to the path provided. E.g.: if you specify log.rolling.path=/tmp/mylovelylog.log , it will store files like so: /tmp/mylovelylog.log.2014-12-17-00 and /tmp/mylovelylog.log.2014-12-17-12 .","title":"log.rolling.path=&lt;path&gt;"},{"location":"others/deploy/Platform-configuration/#maven_executableltpathgt","text":"The path to where Maven is installed. E.g. maven_executable=/usr/bin/mvn","title":"maven_executable=&lt;path&gt;"},{"location":"others/deploy/Platform-configuration/#storage_pathltpathgt","text":"The path to where files should be stored. E.g. storage_path=/mnt/ossmeter/","title":"storage_path=&lt;path&gt;"},{"location":"others/deploy/Platform-configuration/#mongo_hosts","text":"A comma-separated list of the hosts and ports in a replica set. E.g. ua002:27017,ua009:27017,ua019:27017,ua020:27017","title":"mongo_hosts"},{"location":"others/deploy/Running-the-platform/","text":"Running the platform This is a quick start guide to get the OSSMETER platform running from source. Although these instructions may apply to other versions of Eclipse, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package). A step-by-step video guide is also available at https://youtu.be/3Ry4KKfNdYg Start MongoDB You can download MongoDB from the MongoDb website . Instructions for starting mongo can be found in the MongoDB manual . For example: mongod --dbpath /data/db --port 27017 Get the Code Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev Setup Eclipse Open Eclipse and import all projects from the top level directory of the Scava code ( File -> Import -> Existing projects into workspace ), and wait for all the projects to compile without errors. Validate and Run the Platform Open org.ossmeter.platform.osgi/ossmeterfromfeature.product * Click the Validate... icon in the top right of the product configuration editor (the icon is a piece of paper with a tick) * If things do not validate, there's something wrong -- get in touch :) Problems related to org.eclipse.e4.core.di aren't critical. * Then, click the Export an Eclipse product on the left of the Validate... button. Uncheck the Generate p2 repository checkbox, select a destination directory and validate. After a while, the OSSMETER platform will be generated in the selected directory. * The platform can then be run using the generated eclipse binary; it accepts the following arguments: * -apiServer : Starts up the client API on localhost:8182 * -worker ${id-worker} : Spawns a thread that analyses registered projects * To get a full platform running, first launch a master thread, then a slave, and finally the API server. If you are developing code for the Scava platform, be sure to check out the Contributing . Run the api-gateway Right click on scava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java Then click on Run As -> Java Application Run the authentication service Right click on scava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java Then click on Run As -> Java Application Run the administration dashboard Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart). The following instructions show how to run the dashboard web app: * Enter the administration/scava-administration/ directory within the scava repository. * Run the web app on port 4200 using angular-cli: ng serve * Navigate to http://localhost:4200/","title":"Running the platform"},{"location":"others/deploy/Running-the-platform/#running-the-platform","text":"This is a quick start guide to get the OSSMETER platform running from source. Although these instructions may apply to other versions of Eclipse, they were tested under Eclipse Neon.3 with plug-in development support (Eclipse IDE for RCP Developers package). A step-by-step video guide is also available at https://youtu.be/3Ry4KKfNdYg","title":"Running the platform"},{"location":"others/deploy/Running-the-platform/#start-mongodb","text":"You can download MongoDB from the MongoDb website . Instructions for starting mongo can be found in the MongoDB manual . For example: mongod --dbpath /data/db --port 27017","title":"Start MongoDB"},{"location":"others/deploy/Running-the-platform/#get-the-code","text":"Get the latest version of the code, and checkout the dev branch. Please don't commit to the master branch: see the Development Guidelines : If you are using Linux / OS X : git clone https://github.com/crossminer/scava.git scava cd scava git checkout dev If you are using Windows you need to do things differently due to Windows' long file name limit. In the Git shell: mkdir scava cd scava git init git config core.longpaths true git add remote origin https://github.com/crossminer/scava.git git fetch git checkout dev","title":"Get the Code"},{"location":"others/deploy/Running-the-platform/#setup-eclipse","text":"Open Eclipse and import all projects from the top level directory of the Scava code ( File -> Import -> Existing projects into workspace ), and wait for all the projects to compile without errors.","title":"Setup Eclipse"},{"location":"others/deploy/Running-the-platform/#validate-and-run-the-platform","text":"Open org.ossmeter.platform.osgi/ossmeterfromfeature.product * Click the Validate... icon in the top right of the product configuration editor (the icon is a piece of paper with a tick) * If things do not validate, there's something wrong -- get in touch :) Problems related to org.eclipse.e4.core.di aren't critical. * Then, click the Export an Eclipse product on the left of the Validate... button. Uncheck the Generate p2 repository checkbox, select a destination directory and validate. After a while, the OSSMETER platform will be generated in the selected directory. * The platform can then be run using the generated eclipse binary; it accepts the following arguments: * -apiServer : Starts up the client API on localhost:8182 * -worker ${id-worker} : Spawns a thread that analyses registered projects * To get a full platform running, first launch a master thread, then a slave, and finally the API server. If you are developing code for the Scava platform, be sure to check out the Contributing .","title":"Validate and Run the Platform"},{"location":"others/deploy/Running-the-platform/#run-the-api-gateway","text":"Right click on scava-api-gateway/src/main/java/org.eclipse.scava.apigateway/ApiGatewayApplication.java Then click on Run As -> Java Application","title":"Run the api-gateway"},{"location":"others/deploy/Running-the-platform/#run-the-authentication-service","text":"Right click on scava-auth-service/src/main/java/org.eclipse.scava.authservice/AuthServiceApplication.java Then click on Run As -> Java Application","title":"Run the authentication service"},{"location":"others/deploy/Running-the-platform/#run-the-administration-dashboard","text":"Scava Administration is a single page web application based on Angular 6 Framework. To get started with Angular, it's better to install Angular CLI tool to make application development more quicker and easier (Find more here: https://angular.io/guide/quickstart). The following instructions show how to run the dashboard web app: * Enter the administration/scava-administration/ directory within the scava repository. * Run the web app on port 4200 using angular-cli: ng serve * Navigate to http://localhost:4200/","title":"Run the administration dashboard"},{"location":"others/development/Component-Naming/","text":"As consequence of our status of project hosted by the eclipse foundation, we have to follow a specific naming schema for all components implemented in context of SCAVA project. In this section, \"component\" means big funcional componen of the SCAVA project. Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration Project Naming For Eclipse Plugin org.eclipse.scava.{componentname} For Maven Project : the name of the project if the ArtifactId If your component is composed of one project : {component-name} If your component is composed of several sub projects : {sub-component-name} ``` ## Source Code Namsespace All sources must be nemspaces by : org.eclipse.scava.{componentname} ## Maven Ids For the Maven projects: * If your component is composed of one project : Group Id : org.eclipse.scava ArtifactId : {component-name} * If your component is composed of several sub projects : Group Id : org.eclipse.scava.{component-name} ArtifactId : {sub-component-name} ```","title":"Component Naming"},{"location":"others/development/Component-Naming/#project-naming","text":"For Eclipse Plugin org.eclipse.scava.{componentname} For Maven Project : the name of the project if the ArtifactId If your component is composed of one project : {component-name} If your component is composed of several sub projects : {sub-component-name} ``` ## Source Code Namsespace All sources must be nemspaces by : org.eclipse.scava.{componentname} ## Maven Ids For the Maven projects: * If your component is composed of one project : Group Id : org.eclipse.scava ArtifactId : {component-name} * If your component is composed of several sub projects : Group Id : org.eclipse.scava.{component-name} ArtifactId : {sub-component-name} ```","title":"Project Naming"},{"location":"others/development/Contributing/","text":"Contributing Subcategories SCAVA Repository Organisation Guideline describing how the SCAVA code repository is organised and how to add a new component in this repository. How to name SCAVA components? Guideline describing naming constraints for a new scava component (component name, java namespace, maven artefact id and group id, etc. How to name SCAVA REST services? This guideline provide naming rules for each REST services routes implemented by the SCAVA platform. How to manage Licensing Guideline describing licensing requirements for SCAVA components. Technical Guidelines REST API Each implemented REST services must be documented (see /users directory): REST API DOCUMENTATION How to configure the SCAVA Gateway in order to integrate a new REST service Customers access SCAVA services through the SCAVA API Gateway. This guideline present how to configure the API Gateway to integrate new remote service provider. How to consume a SCAVA REST services This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues. How to implement Restlet services Guideline which describe how to integrate and implement a REST service in SCAVA platform using the RESTLET framework. DATA ACCESS How to access MongoDB database using PONGO Guideline which describe how to the access to MongoDB database using the PONGO framework. How to extend the SCAVA data model Guideline which describe ways to extend the SCAVA Data Model, stored in a MongoDb database and based on the PONGO framework. OSGI How to integrate OSGI service plugin in SCAVA Architecture Todo How to communicate between OSGI plugin using JMS Todo","title":"Contributing"},{"location":"others/development/Contributing/#contributing","text":"","title":"Contributing"},{"location":"others/development/Contributing/#subcategories","text":"SCAVA Repository Organisation Guideline describing how the SCAVA code repository is organised and how to add a new component in this repository. How to name SCAVA components? Guideline describing naming constraints for a new scava component (component name, java namespace, maven artefact id and group id, etc. How to name SCAVA REST services? This guideline provide naming rules for each REST services routes implemented by the SCAVA platform. How to manage Licensing Guideline describing licensing requirements for SCAVA components.","title":"Subcategories"},{"location":"others/development/Contributing/#technical-guidelines","text":"","title":"Technical Guidelines"},{"location":"others/development/Contributing/#rest-api","text":"Each implemented REST services must be documented (see /users directory): REST API DOCUMENTATION How to configure the SCAVA Gateway in order to integrate a new REST service Customers access SCAVA services through the SCAVA API Gateway. This guideline present how to configure the API Gateway to integrate new remote service provider. How to consume a SCAVA REST services This guideline is dedicated to clients which would like to used SCAVA REST Services.It adress authentication issues. How to implement Restlet services Guideline which describe how to integrate and implement a REST service in SCAVA platform using the RESTLET framework.","title":"REST API"},{"location":"others/development/Contributing/#data-access","text":"How to access MongoDB database using PONGO Guideline which describe how to the access to MongoDB database using the PONGO framework. How to extend the SCAVA data model Guideline which describe ways to extend the SCAVA Data Model, stored in a MongoDb database and based on the PONGO framework.","title":"DATA ACCESS"},{"location":"others/development/Contributing/#osgi","text":"How to integrate OSGI service plugin in SCAVA Architecture Todo How to communicate between OSGI plugin using JMS Todo","title":"OSGI"},{"location":"others/development/Development-Guidelines/","text":"Development Guidelines Introduction This document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development. Process overview The development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system. It is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests. The different elements shown below are individually described in the remaining of the section. Figure 1. Overview of the CROSSMINER development process and tools Source Code Repository Different branches will be created in the repository. In particular, the master branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The dev branch contains the most recent code integrated by partners before release. Other branches will be created for the different features/components, which will be eventually merged in the dev branch. The name of each branch contains the id of the issue it is supposed to implement. All source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository. Code: https://github.com/crossminer/scava Product documentation: https://github.com/crossminer/scava-docs Product deployment: https://github.com/crossminer/scava-deployment Since all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details). By taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3. Figure 2: Branching model Tests Each branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below. Figure 2: Explanatory master and branches BRANCH: Set of branches TEST: Set of tests INTGRATION_TEST < TEST UNIT_TEST < TEST function test: BRANCH -> TEST test(master) -> test(cool-feature) \u22c3 {itu, \u2026 itz} Continuous Integration Server Continuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus reduce cost and time . A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all unit tests . Integration tests are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities: Allow developers to write unit tests that can be executed automatically Perform automated tests against newly written code Show a list of tests that have passed and failed Quality analysis on source code Perform all the necessary actions to create a fully functioning build of the software when all tests have passed The currently used CI server is available at http://ci5.castalia.camp:8080/ Development and Production environments According to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform. Naming and change conventions The repository will contain the code of the high-level components shown in Table 1 Components Corresponding folder in repository Leader DevOps Dashboard BIT Workflow Diagram Editor /crossflow YORK Administration Web Application /administration SFT IDE /eclipse-based-ide FEA API Gateway /api-gateway SFT DevOps Backend BIT Knowledge Base /knowledge-base UDA Project Analyser /metric-platform SFT Data Collector /metric-platform SFT Project Scheduler /metric-platform SFT Metric Providers /metric-platform YORK, CWI, AUEB, EHU Data Storage /metric-platform SFT Web-based dashboards /web-dashboards BIT Table 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes Figure 2: Scava components architecture For each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus: when developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons; when a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR. The partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder: readme.md : this \ufb01le will contain the entry-point to all documentation for the component. Jenkinsfile : in order to compile the component, run tests, etc on the CI server. test : this is a folder containing the unit and/or integration tests. As a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base The following items are typical practices when applying CI: * Maintain a code repository * Automate the build * Make the build self-testing * Everyone commits to the baseline every day * Every commit (to baseline) should be built * Keep the build fast * Test in a clone of the production environment * Make it easy to get the latest deliverables * Everyone can see the results of the latest build * Automate deployment Communication and collaboration means The development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of: Slack channel: http://crossminer.slack.com Eclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev GitHub repository for documentation: https://github.com/crossminer/scava-docs/ Remark I recall that we are running a research and innovative project. Reviewers that will evaluate our work will not be impressed by the fact that 100% of our tests succeed, that we perfectly adhere to a detailed code convention, etc. Of course, the community will do!!! Thus, we have to find a reasonably compromise: to have all the deliverables accepted !!! to develop in a way, which is preparatory to achieve our \u201cdreams\u201d \u201cBetter have a low test coverage than a part of our tests fail\u201d [cit. Boris] :-)","title":"Development Guidelines"},{"location":"others/development/Development-Guidelines/#development-guidelines","text":"","title":"Development Guidelines"},{"location":"others/development/Development-Guidelines/#introduction","text":"This document presents the guidelines about the processes and tools supporting the development of the Scava platform. In particular, Sec 2 presents an overview of the main development workflow by highlighting the main tools of the supporting development infrastructure. Section 3 mentions peculiar conventions that have to be adopted to write source code and to perform changes. Section 4 makes a list of tools supporting communication and collaboration among the different partners. Section 5 gives some important remarks related to the Scava project and its development.","title":"Introduction"},{"location":"others/development/Development-Guidelines/#process-overview","text":"The development of the Scava components and of the integrated platform should follow the process shown in the following figure. Essentially, the development relies on the availability of a source code repository (to enable collaborative development) and of a Continuous Integration Server (to do the building and integration on behalf of the developers). Different deployment environments can be considered for deploying the built system. It is important to remark that Continuous Integration (CI) requires that the different developers have self-testing code. This is code that tests itself to ensure that it is working as expected, and these tests are often called unit tests. When all the unit tests pass after the code is integrated, developers will get a green build. This indicates that they have verified that their changes are successfully integrated together, and the code is working as expected by the tests. The different elements shown below are individually described in the remaining of the section. Figure 1. Overview of the CROSSMINER development process and tools","title":"Process overview"},{"location":"others/development/Development-Guidelines/#source-code-repository","text":"Different branches will be created in the repository. In particular, the master branch is the main branch and represents the released state of the product. It is always clean, builds and runs all the tests successfully. The dev branch contains the most recent code integrated by partners before release. Other branches will be created for the different features/components, which will be eventually merged in the dev branch. The name of each branch contains the id of the issue it is supposed to implement. All source code, binary code and con\ufb01guration needed to build a working implementation of the system will reside in a GitHub repository. Code: https://github.com/crossminer/scava Product documentation: https://github.com/crossminer/scava-docs Product deployment: https://github.com/crossminer/scava-deployment Since all source code will be stored in a single GitHub repository, each developer will have a working copy of the entire project. To simplify collaboration, each component will be contained in one subdirectory within the source structure (see Sec. 3 for details). By taking inspiration from https://datasift.github.io/gitflow/IntroducingGitFlow.html the branching model that will be followed will resemble that shown in Fig. 2 without making use of the release and hotfixes branches. Details about the change conventions, pull requests, etc. are given in Sec. 3. Figure 2: Branching model","title":"Source Code Repository"},{"location":"others/development/Development-Guidelines/#tests","text":"Each branch must have the corresponding unit tests. The dev branch must have integration tests and the unit tests of all the features that have been already merged as shown below. Figure 2: Explanatory master and branches BRANCH: Set of branches TEST: Set of tests INTGRATION_TEST < TEST UNIT_TEST < TEST function test: BRANCH -> TEST test(master) -> test(cool-feature) \u22c3 {itu, \u2026 itz}","title":"Tests"},{"location":"others/development/Development-Guidelines/#continuous-integration-server","text":"Continuous integration (CI) involves integrating early and often, so as to avoid the pitfalls of \"integration hell\". The practice aims to reduce rework and thus reduce cost and time . A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all unit tests . Integration tests are usually run automatically on a CI server when it detects a new commit. In particular, a CI server provides developers with at least the following functionalities: Allow developers to write unit tests that can be executed automatically Perform automated tests against newly written code Show a list of tests that have passed and failed Quality analysis on source code Perform all the necessary actions to create a fully functioning build of the software when all tests have passed The currently used CI server is available at http://ci5.castalia.camp:8080/","title":"Continuous Integration Server"},{"location":"others/development/Development-Guidelines/#development-and-production-environments","text":"According to the DoW we have not promised the availability of a public installation of the CROSSMINER platform (i.e., a production environment). However, in order to implement the different use cases, use case partners are provided with docker images enabling the local installation of the whole CROSSMINER platform.","title":"Development and Production environments"},{"location":"others/development/Development-Guidelines/#naming-and-change-conventions","text":"The repository will contain the code of the high-level components shown in Table 1 Components Corresponding folder in repository Leader DevOps Dashboard BIT Workflow Diagram Editor /crossflow YORK Administration Web Application /administration SFT IDE /eclipse-based-ide FEA API Gateway /api-gateway SFT DevOps Backend BIT Knowledge Base /knowledge-base UDA Project Analyser /metric-platform SFT Data Collector /metric-platform SFT Project Scheduler /metric-platform SFT Metric Providers /metric-platform YORK, CWI, AUEB, EHU Data Storage /metric-platform SFT Web-based dashboards /web-dashboards BIT Table 1: Leaders and Contributors of the CROSSMINER Deployment diagram nodes Figure 2: Scava components architecture For each component a corresponding folder is available in the repository. Changes must have a CR (i.e. issue) entry for traceability. In particular, the requirements and objectives of each Scava components will be detailed in the relevant deliverables and they will be added in the form of items in the provided Bug Tracking infrastructure. In this way progress on the implementation of each requirement can be tracked. Thus: when developers want to operate changes they have to create corresponding issues in the bug tracking system for traceability reasons; when a developer wants to perform changes on a component led by an organization, which is different than that the developer belongs to, a pull request (PR) has to be done. Such a PR will be managed by one of the members of the organization leading the component affected by the changes in the PR. The partner in charge of managing the development of a component will be free to organize the content of the corresponding subdirectory in any way as long as the following standard \ufb01les/directories are contained in the component folder: readme.md : this \ufb01le will contain the entry-point to all documentation for the component. Jenkinsfile : in order to compile the component, run tests, etc on the CI server. test : this is a folder containing the unit and/or integration tests. As a reference, please have a look at https://github.com/crossminer/scava/tree/master/knowledge-base The following items are typical practices when applying CI: * Maintain a code repository * Automate the build * Make the build self-testing * Everyone commits to the baseline every day * Every commit (to baseline) should be built * Keep the build fast * Test in a clone of the production environment * Make it easy to get the latest deliverables * Everyone can see the results of the latest build * Automate deployment","title":"Naming and change conventions"},{"location":"others/development/Development-Guidelines/#communication-and-collaboration-means","text":"The development activities can be done by exploiting different communication and collaboration means. In particular, currently the consortium members have the availability of: Slack channel: http://crossminer.slack.com Eclipse Mailing lists: https://accounts.eclipse.org/mailing-list/scava-dev GitHub repository for documentation: https://github.com/crossminer/scava-docs/","title":"Communication and collaboration means"},{"location":"others/development/Development-Guidelines/#remark","text":"I recall that we are running a research and innovative project. Reviewers that will evaluate our work will not be impressed by the fact that 100% of our tests succeed, that we perfectly adhere to a detailed code convention, etc. Of course, the community will do!!! Thus, we have to find a reasonably compromise: to have all the deliverables accepted !!! to develop in a way, which is preparatory to achieve our \u201cdreams\u201d \u201cBetter have a low test coverage than a part of our tests fail\u201d [cit. Boris] :-)","title":"Remark"},{"location":"others/development/Extend-MongoDB-Data-Model/","text":"Extending MongoDB data model When to use ? In this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order the data layer of Scava platform during the evolution. Context The Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class. Each MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model. The current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins : The Platform data model (org.ossmeter.repository.model) : Contains data related to project administration , metric execution process and authentification system. Source Code Repositroy managers (org.ossmeter.repository.model.'project delta manager') : Contains configuration informations related to source codes managemeny tools. Metric Providers data models (in each metric provider plugins) : Each metric provider implementation contains his once data model. You need to Extend an Existing Data Model ? The first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same. 1. Locate the *.emf file of this data model A presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model. Ex : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf) 2. Update the Data Model description A data model description file contains a statical description of a MongoDb document. @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . attr String stackTrace; } ... If we would like to add one more attribute to the element ProjectError , we could add it this way : @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . . . attr String stackTrace; + attr String TestAttribute; } ... You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines 3. Generate the Java Class using the Pongo Tool. Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Replace the existing Java class by the new generated java class. More information about Pongo : https://github.com/kolovos/pongo/wiki You need to Create a new Data Model ? The second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model. In this case, we invite you to create a new plugin which will contain your data model. 1. Create a new Eclipse Plug-In Create a new Eclipse Plug-In Project ( In Eclipse Toolbar : File > New > Plug-In Project Name of the project : org.scava. mycomponent .repository.model Disable the generation of an Activator Class / contribution to the ui Edit the MANIFEST.MF file In Dependency : add a dependency to the org.eclipse.core.runtime plugin In Dependency : add a dependency to the com.googlecode.pongo.runtime plugin In Dependency : add a dependency to the org.apache.commons.lang3 plugin In Extentions : reference an extension point named com.googlecode.pongo.runtime.osgi In source directory Create a package named org.scava. mycomponent .repository.model In this package create an emf file named mycomponent.emf A presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file : ```package org.scava.mycomponent.repository.model; @db class MyComponent { .... } ``` You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines 2. Generate the Java Class using the Pongo Tool. Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Add this class in your org.scava. mycomponent .repository.model package More information about Pongo : https://github.com/kolovos/pongo/wiki Comment Here we learnt ways to modify model in the Scava platform. To know more about the access of data with the Pongo APIs link here .","title":"Extending MongoDB data model"},{"location":"others/development/Extend-MongoDB-Data-Model/#extending-mongodb-data-model","text":"","title":"Extending MongoDB data model"},{"location":"others/development/Extend-MongoDB-Data-Model/#when-to-use","text":"In this guideline, we describe ways to extend the Scava data model with the existing architecture of Scava. These guidelines are needed to keep order the data layer of Scava platform during the evolution.","title":"When to use ?"},{"location":"others/development/Extend-MongoDB-Data-Model/#context","text":"The Scava platform use MongoBb as database. The access to the database is managed using the Pongo , a framework which manage the mapping between MongoDb documents and Java class. Each MongoDb document is mapped on a Java class which which extend the Pongo class.This Java class are generated form an emf file which describe the data model. The current data model is composed of multiple documents which are mapped to several Pongo classes. This Java classes are organized in plugins : The Platform data model (org.ossmeter.repository.model) : Contains data related to project administration , metric execution process and authentification system. Source Code Repositroy managers (org.ossmeter.repository.model.'project delta manager') : Contains configuration informations related to source codes managemeny tools. Metric Providers data models (in each metric provider plugins) : Each metric provider implementation contains his once data model.","title":"Context"},{"location":"others/development/Extend-MongoDB-Data-Model/#you-need-to-extend-an-existing-data-model","text":"The first way would be to make changes directly in the existing model. This option is to be used carefully as it may affect the rest of the platform modules. Therefore, it must be well checked, such that the rest of the part of the platform remains the same.","title":"You need to Extend an Existing Data Model ?"},{"location":"others/development/Extend-MongoDB-Data-Model/#1-locate-the-emf-file-of-this-data-model","text":"A presented previously, the Pongo Java class are generated form and EMF file which describe the data model. The file can be found in the implementaion plugin of each data model. Ex : the platform data model Definition File can be find on the org.ossmeter.repository.model plugin (/src/org/ossmeter/repository/model/ossmeter.emf)","title":"1. Locate the *.emf file of this data model"},{"location":"others/development/Extend-MongoDB-Data-Model/#2-update-the-data-model-description","text":"A data model description file contains a statical description of a MongoDb document. @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . attr String stackTrace; } ... If we would like to add one more attribute to the element ProjectError , we could add it this way : @db class ProjectRepository extends NamedElement { val Project[*] projects; val Role[*] roles; . val ProjectError[*] errors; } @customize class ProjectError { attr Date date; . . . attr String stackTrace; + attr String TestAttribute; } ... You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines","title":"2. Update the Data  Model description"},{"location":"others/development/Extend-MongoDB-Data-Model/#3-generate-the-java-class-using-the-pongo-tool","text":"Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Replace the existing Java class by the new generated java class. More information about Pongo : https://github.com/kolovos/pongo/wiki","title":"3. Generate the Java Class using the Pongo Tool."},{"location":"others/development/Extend-MongoDB-Data-Model/#you-need-to-create-a-new-data-model","text":"The second way is to evolve the data model by building a new model/ database/ collection in Mongodb. This pongo model is separate from the existing model with separate database and thus avoids issues of breaking the existing model. In this case, we invite you to create a new plugin which will contain your data model.","title":"You need to Create a new Data Model ?"},{"location":"others/development/Extend-MongoDB-Data-Model/#1-create-a-new-eclipse-plug-in","text":"Create a new Eclipse Plug-In Project ( In Eclipse Toolbar : File > New > Plug-In Project Name of the project : org.scava. mycomponent .repository.model Disable the generation of an Activator Class / contribution to the ui Edit the MANIFEST.MF file In Dependency : add a dependency to the org.eclipse.core.runtime plugin In Dependency : add a dependency to the com.googlecode.pongo.runtime plugin In Dependency : add a dependency to the org.apache.commons.lang3 plugin In Extentions : reference an extension point named com.googlecode.pongo.runtime.osgi In source directory Create a package named org.scava. mycomponent .repository.model In this package create an emf file named mycomponent.emf A presented previously, the Pongo Java class are generated form and EMF file which describe the data model.Define your data model in this file : ```package org.scava.mycomponent.repository.model; @db class MyComponent { .... } ``` You can find more information about the data model description syntax at this url : https://github.com/kolovos/pongo/wiki/Model-Design-Guidelines","title":"1. Create a new Eclipse Plug-In"},{"location":"others/development/Extend-MongoDB-Data-Model/#2-generate-the-java-class-using-the-pongo-tool","text":"Download the Pongo tool : https://github.com/kolovos/pongo/releases Run the Pongo generator from the command line as follows: java -jar pongo.jar youremffile.emf Add this class in your org.scava. mycomponent .repository.model package More information about Pongo : https://github.com/kolovos/pongo/wiki","title":"2. Generate the Java Class using the Pongo Tool."},{"location":"others/development/Extend-MongoDB-Data-Model/#comment","text":"Here we learnt ways to modify model in the Scava platform. To know more about the access of data with the Pongo APIs link here .","title":"Comment"},{"location":"others/development/How-To-Develop-Metric-Provider/","text":"How to Develop a Metric Provider In this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a transient metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a historical metric provider, will count the total number of commits over time. We'll go through the steps above for each metric provider. Pre-requisites Eclipse The Emfatic plug-in should be installed in your Eclipse The Pongo plug-in should be installed in your Eclipse The OSSMETER source code should be in your workspace The Transient Metric Provider This metric provider will store a complete history of the commits in the version control system(s) used by a project. 0. Setup Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list. 1. The data model We define the data model using the Emfatic language. In your newly created plug-in, create a package called org.ossmeter.metricprovider.trans.commits.model . In that package create an empty file called commits.emf . In this file, we will define our data model. First of all, we need to state the name of the package. package org.ossmeter.metricprovider.trans.commits.model; This is used by the Pongo code generator - the generated classes will be put in this package. We then define the database for our model: @db(qualifiedCollectionNames=\"true\") class Commits { val Repository[*] repositories; val Commit[*] commits; } The @db annotation tells Pongo that this will be the container database for the data. Adding the qualifiedCollectionNames=true property will prepend the database name to all Mongo collections. The Commits class above says that we want a database with two collections, name repositories and commits . If qualifiedCollectionNames is set to true , the collections will be named Commits.repositories and Commits.commits . We now define the schema of the Commits.repositories collection: ~~~java class Repository { @searchable attr String url; attr String repoType; attr String revision; attr int totalNumberOfCommits; ref CommitData[*] commits; } This class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection. The `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time. Each commit is represented in the `Commits.commits` collection by the following model: ~~~~java class Commit { @searchable attr Date date; attr String identifier; attr String message; attr String author; } For each commit, we store its date , identifier (revision ID), the commit message , and the author . We also create an index on the date to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated findByDate(String date) or findOneByDate(String date) methods). Now we need to use Pongo to generate code from this model. Right-click on the commits.emf file and select Pongo > Generate Pongos and plugin.xml . You should see some Java classes appear in your package. Now that we have our data model, we can implement the metric provider. 2. The metric provider Create a Java class called org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider This class should extend AbstractTransientMetricProvider and specify Commits for the generic argument: public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits> The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. adapt(DB db) This method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows: @Override public Commits adapt(DB db) { return new Commits(db); } The next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider: @Override public String getShortIdentifier() { // This may be deprecated very soon return \"transient-commits\"; } @Override public String getFriendlyName() { return \"Commit History\"; } @Override public String getSummaryInformation() { return \"The commit history of the project.\"; } The next method allows you to declare whether the metric provider is applicable to a given project: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } Our metric applies to any project that has at least one VCS repository. Finally, we have the measure(...) method that performs the actual metric calculation: @Override public void measure(Project project, ProjectDelta delta, Commits db) { for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) { Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl()); if (repo == null) { repo = new Repository(); repo.setUrl(repoDelta.getRepository().getUrl()); db.getRepositories().add(repo); } for (VcsCommit commit : repoDelta.getCommits()) { Commit c = new Commit(); c.setDate(commit.getJavaDate()); c.setMessage(commit.getMessage()); c.setAuthor(commit.getAuthor()); c.setIdentifier(commit.getRevision()); repo.getCommits().add(c); db.getCommits().add(c); } } db.getCommits().sync(); db.getRepositories().sync(); } The above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new Commit object is created for each commit in the delta. 3. Make the metric provider discoverable Metric providers are registered with the OSSMETER platform using extension points : Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsTransientMetricProvider' class Now everything is ready for the metric to be executed :) The Historic Metric Provider This metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value. 0. Setup Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list. 1. The data model In your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model: package org.ossmeter.metricprovider.historic.commits.model; class HistoricCommits { attr int numberOfCommits; } The data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed. 2. The metric provider Create a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider This class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)): public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. First of all, complete the typical information-related methods: @Override public String getShortIdentifier() { return \"historicalcommits\"; } @Override public String getFriendlyName() { return \"Historical commits\"; } @Override public String getSummaryInformation() { return \"...\"; } Now complete the standard appliesTo method: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } We now need to specify a dependency on the transient metric provider that we just implemented. @Override public List<String> getIdentifiersOfUses() { return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName()); } This tells the platform that we need access to the CommitsTransientMetricProvider database. The platform will assign this to the uses field that is available to the historical metric provider, as you'll see in the measure method: @Override public Pongo measure(Project project) { Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0)); int commits = (int) transDb.getCommits().size(); HistoricCommits hist = new HistoricCommits(); hist.setNumberOfCommits(commits); return hist; } First of all, we get hold of the database of the transient commits metric provider, and simply count the size of the commits collection. We save this in an instance of the HistoricCommits Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project. 3. Make the metric provider discoverable This process is the same as the transient metric provider: Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class Now everything is ready for both metrics to be executed :) But first. Let's specify how we want this historical metric to be visualised. 4. Define a MetVis visualisation specification MetVis is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis web page has numerous examples of this. Create a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider: { \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\", \"vis\" : [ { \"id\" : \"historicalcommits\", \"name\" : \"Commits over time\", \"description\" : \"This metric shows when the projects commits occurred\", \"type\" : \"LineChart\", \"datatable\" : { \"cols\" : [ { \"name\" : \"Date\", \"field\" : \"$__date\" }, { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" } ] }, \"x\" : \"Date\", \"y\" : \"Commits\" } ] } The metricId field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use $__date . The final two fields ( x and y ) are references to column names in the datatable specification. The type states that the data should be plotted as a line chart. You can test your MetVis specifications on the MetVis playpen . 5. Make the visualisation specification discoverable As with metric providers, visualisation specifications are registered using extension points. Add the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.visualisation.metric On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'historicalcommits.json' file Good job. Running the metric providers See Running from Source Homework Adapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.","title":"How to Develop a Metric Provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#how-to-develop-a-metric-provider","text":"In this tutorial we will implement two metric providers related to the commits that occur in a version control system. The first, a transient metric provider, will keep track of the dates, times, and messages of all commits in the repositories. The second, a historical metric provider, will count the total number of commits over time. We'll go through the steps above for each metric provider.","title":"How to Develop a Metric Provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#pre-requisites","text":"Eclipse The Emfatic plug-in should be installed in your Eclipse The Pongo plug-in should be installed in your Eclipse The OSSMETER source code should be in your workspace","title":"Pre-requisites"},{"location":"others/development/How-To-Develop-Metric-Provider/#the-transient-metric-provider","text":"This metric provider will store a complete history of the commits in the version control system(s) used by a project.","title":"The Transient Metric Provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#0-setup","text":"Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.","title":"0. Setup"},{"location":"others/development/How-To-Develop-Metric-Provider/#1-the-data-model","text":"We define the data model using the Emfatic language. In your newly created plug-in, create a package called org.ossmeter.metricprovider.trans.commits.model . In that package create an empty file called commits.emf . In this file, we will define our data model. First of all, we need to state the name of the package. package org.ossmeter.metricprovider.trans.commits.model; This is used by the Pongo code generator - the generated classes will be put in this package. We then define the database for our model: @db(qualifiedCollectionNames=\"true\") class Commits { val Repository[*] repositories; val Commit[*] commits; } The @db annotation tells Pongo that this will be the container database for the data. Adding the qualifiedCollectionNames=true property will prepend the database name to all Mongo collections. The Commits class above says that we want a database with two collections, name repositories and commits . If qualifiedCollectionNames is set to true , the collections will be named Commits.repositories and Commits.commits . We now define the schema of the Commits.repositories collection: ~~~java class Repository { @searchable attr String url; attr String repoType; attr String revision; attr int totalNumberOfCommits; ref CommitData[*] commits; } This class is used to store information related to the commits in the project's VCS repositories. The collection will contain one entry per VCS repository. Repositories are identified by a `url`, and also have a `repoType` (e.g. Git or SVN), the latest `revision`, the `totalNumberOfCommits`, and a reference to all of the `commits` in the repository, which are stored in the `Commits.commits` collection. The `@searchable` annotation will make Pongo generate two utility search methods on the collection: `findByUrl(String url) : Iterable<Repository>` and `findOneByUrl(String url) : Repository`. An index will also be create on this field to improve look up time. Each commit is represented in the `Commits.commits` collection by the following model: ~~~~java class Commit { @searchable attr Date date; attr String identifier; attr String message; attr String author; } For each commit, we store its date , identifier (revision ID), the commit message , and the author . We also create an index on the date to allow us to quickly discover the set of commits that occurred on a given date (using the autogenerated findByDate(String date) or findOneByDate(String date) methods). Now we need to use Pongo to generate code from this model. Right-click on the commits.emf file and select Pongo > Generate Pongos and plugin.xml . You should see some Java classes appear in your package. Now that we have our data model, we can implement the metric provider.","title":"1. The data model"},{"location":"others/development/How-To-Develop-Metric-Provider/#2-the-metric-provider","text":"Create a Java class called org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider This class should extend AbstractTransientMetricProvider and specify Commits for the generic argument: public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider<Commits> The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn.","title":"2. The metric provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#adaptdb-db","text":"This method returns the Pongo database that the metric provider will use to store its data. This is boilerplate, unfortunately, we can't auto-generate it. Implement as follows: @Override public Commits adapt(DB db) { return new Commits(db); } The next thing we want to do is fill in useful information that helps users understand the purpose of the metric provider: @Override public String getShortIdentifier() { // This may be deprecated very soon return \"transient-commits\"; } @Override public String getFriendlyName() { return \"Commit History\"; } @Override public String getSummaryInformation() { return \"The commit history of the project.\"; } The next method allows you to declare whether the metric provider is applicable to a given project: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } Our metric applies to any project that has at least one VCS repository. Finally, we have the measure(...) method that performs the actual metric calculation: @Override public void measure(Project project, ProjectDelta delta, Commits db) { for (VcsRepositoryDelta repoDelta : delta.getVcsDelta().getRepoDeltas()) { Repository repo = db.getRepositories().findOneByUrl(repoDelta.getRepository().getUrl()); if (repo == null) { repo = new Repository(); repo.setUrl(repoDelta.getRepository().getUrl()); db.getRepositories().add(repo); } for (VcsCommit commit : repoDelta.getCommits()) { Commit c = new Commit(); c.setDate(commit.getJavaDate()); c.setMessage(commit.getMessage()); c.setAuthor(commit.getAuthor()); c.setIdentifier(commit.getRevision()); repo.getCommits().add(c); db.getCommits().add(c); } } db.getCommits().sync(); db.getRepositories().sync(); } The above method iterates through the delta computed by the platform. The delta consists of the commits that occurred in each of the project's VCS repositories for the date being analysed. A new Commit object is created for each commit in the delta.","title":"adapt(DB db)"},{"location":"others/development/How-To-Develop-Metric-Provider/#3-make-the-metric-provider-discoverable","text":"Metric providers are registered with the OSSMETER platform using extension points : Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsTransientMetricProvider' class Now everything is ready for the metric to be executed :)","title":"3. Make the metric provider discoverable"},{"location":"others/development/How-To-Develop-Metric-Provider/#the-historic-metric-provider","text":"This metric provider will keep track of the total number of commits of the project over time. For each date of the project, the metric counts how many commits have been made and stores the value.","title":"The Historic Metric Provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#0-setup_1","text":"Create a new Plugin project in Eclipse. Go to File > New > Project... and select 'Plug-in project' Give the project an appropriate name. The OSSMETER naming convention is: Transient metrics: org.ossmeter.metricprovider.trans.(metric name) Historical metrics: org.ossmeter.metricprovider.historical.(metric name) Click Next If the \"This plug-in will make contributions to the UI\" checkbox is ticked, uncheck it Click Finish Open up the plugin.xml file in your new project. Open the 'Dependencies' tab and add 'com.googlecode.pongo.runtime', 'org.ossmeter.platform', and 'org.ossmeter.repository.model' to the dependency list.","title":"0. Setup"},{"location":"others/development/How-To-Develop-Metric-Provider/#1-the-data-model_1","text":"In your newly created plug-in, create a package called org.ossmeter.metricprovider.historic.commits.model. In that package create an empty file called historiccommits.emf. In this file, we will define our data model: package org.ossmeter.metricprovider.historic.commits.model; class HistoricCommits { attr int numberOfCommits; } The data model for the historic metric is much simpler. No Pongo annotations are used because, unlike transient metrics, the platform is responsible for storing the historic data. The data model defined here will be stored against the date that the analysis is performed.","title":"1. The data model"},{"location":"others/development/How-To-Develop-Metric-Provider/#2-the-metric-provider_1","text":"Create a Java class called org.ossmeter.metricprovider.historical.commits.CommitsHistoricalMetricProvider This class should extend AbstractHistoricalMetricProvider (there are no generic arguments (yet!)): public class org.ossmeter.metricprovider.trans.commits.CommitsTransientMetricProvider extends AbstractTransientMetricProvider The generic argument states that the metric provider stores objects conforming to the Commits data model. Note: You do not need to extend AbstractTransientMetricProvider and can implement ITransientMetricProvider instead should you wish to. There are a number of methods that need implementing. We will discuss each in turn. First of all, complete the typical information-related methods: @Override public String getShortIdentifier() { return \"historicalcommits\"; } @Override public String getFriendlyName() { return \"Historical commits\"; } @Override public String getSummaryInformation() { return \"...\"; } Now complete the standard appliesTo method: @Override public boolean appliesTo(Project project) { return project.getVcsRepositories().size() > 0; } We now need to specify a dependency on the transient metric provider that we just implemented. @Override public List<String> getIdentifiersOfUses() { return Arrays.asList(CommitsTransientMetricProvider.class.getCanonicalName()); } This tells the platform that we need access to the CommitsTransientMetricProvider database. The platform will assign this to the uses field that is available to the historical metric provider, as you'll see in the measure method: @Override public Pongo measure(Project project) { Commits transDb = (Commits) getDbOfMetricProvider(project, (ITransientMetricProvider) uses.get(0)); int commits = (int) transDb.getCommits().size(); HistoricCommits hist = new HistoricCommits(); hist.setNumberOfCommits(commits); return hist; } First of all, we get hold of the database of the transient commits metric provider, and simply count the size of the commits collection. We save this in an instance of the HistoricCommits Pongo defined above. This object is returned by the method and the platform stores it in the database along with the date that is currently being analysed for the project.","title":"2. The metric provider"},{"location":"others/development/How-To-Develop-Metric-Provider/#3-make-the-metric-provider-discoverable_1","text":"This process is the same as the transient metric provider: Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.metricprovider On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'CommitsHistoricalMetricProvider' class Now everything is ready for both metrics to be executed :) But first. Let's specify how we want this historical metric to be visualised.","title":"3. Make the metric provider discoverable"},{"location":"others/development/How-To-Develop-Metric-Provider/#4-define-a-metvis-visualisation-specification","text":"MetVis is a JSON-based specification language and visualisation engine for metrics. You specify how the Pongo data model should be transformed into something that can be plotted on a chart. The MetVis web page has numerous examples of this. Create a folder in your historical metric project called 'vis'. In the 'vis' folder create a file called 'historicalcommits.json'. Here is the MetVis specification for the historical commits metric provider: { \"metricid\" : \"org.ossmeter.metricprovider.historic.commits.CommitsHistoricalMetricProvider\", \"vis\" : [ { \"id\" : \"historicalcommits\", \"name\" : \"Commits over time\", \"description\" : \"This metric shows when the projects commits occurred\", \"type\" : \"LineChart\", \"datatable\" : { \"cols\" : [ { \"name\" : \"Date\", \"field\" : \"$__date\" }, { \"name\" : \"Commits\", \"field\" : \"$numberOfCommits\" } ] }, \"x\" : \"Date\", \"y\" : \"Commits\" } ] } The metricId field tells the platform which metric provider the specification visualises. A metric provider can have multiple visualisations. In this case, we just define one, which plots the date on the X-axis and the number of commits on the Y-axis. Fields in the Pongo data model are references using the $-sign. To access the date field of a historical metric, use $__date . The final two fields ( x and y ) are references to column names in the datatable specification. The type states that the data should be plotted as a line chart. You can test your MetVis specifications on the MetVis playpen .","title":"4. Define a MetVis visualisation specification"},{"location":"others/development/How-To-Develop-Metric-Provider/#5-make-the-visualisation-specification-discoverable","text":"As with metric providers, visualisation specifications are registered using extension points. Add the 'org.ossmeter.platform.visualisation' plugin as a dependency on your project Open up the plugin.xml file and select the 'Extensions' tab Click the Add button and from the list select org.ossmeter.platform.visualisation.metric On the right hand side of the editor, you will now see a text box with the label 'provider' and a 'Browse...' button. Click the 'Browse...' button and select the 'historicalcommits.json' file Good job.","title":"5. Make the visualisation specification discoverable"},{"location":"others/development/How-To-Develop-Metric-Provider/#running-the-metric-providers","text":"See Running from Source","title":"Running the metric providers"},{"location":"others/development/How-To-Develop-Metric-Provider/#homework","text":"Adapt the historical commits metric provider so that it stores the commits for each repository separately (in the above, it sums them all up). Write the MetVis specification - each series should be the commits for separate repositories.","title":"Homework"},{"location":"others/development/Implementing-Restlet-Service/","text":"Implementing RESTLET services When to use this guideline ? This guideline present how to create a new REST service using the RESTLET framework in the Scava platform. Context Scava project manages REST services with the RESTLET framework. The usage of Restlet framework has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin : org.eclipse.crossmeter.platform.client.api. The RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in. You want to access to create a new REST Service ? 1. Create a new Route To register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service. Naming the Route The routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service : Naming-Scava-REST-Services.html Register the Route The org.scava.platform.services plug-in contained the class PlatformRoute.java responsible for declaring routes. package org.scava.platform.services; import org.restlet.Application; import org.restlet.Restlet; import org.restlet.routing.Router; public class PlatformRoute extends Application { @Override public Restlet createInboundRoot() { Router router = new Router(getContext()); router.attach(\"/\", PingResource.class); router.attach(\"/search\", SearchProjectResource.class); ... router.attach(\"/projects/p/{projectid}\", ProjectResource.class); router.attach(\"/raw/metrics\", RawMetricListResource.class); ... return router; } } Route Example : router.attach(\"/raw/metrics\", RawMetricListResource.class); \"/raw/metrics\" : Represent the route URL. \"RawMetricListResource.class\" : Represent the class where the service to be implemented for this path. A route can contained some parameters. In this case, parameters are identified by a name with curly brackets {} . 2. Implement the Service A service implementation is a Java class which extend the ServerResource class provided by the RESTLET framework. To create a new service create a new Class : * Named \" ServiceName \" + Resource. Ex : ProjectCreationResource.java * On a namespace based on the route. Ex : org.scava.platform.services.administration for platform administration services. * Who extend the org.restlet.resource.ServerResource class. GET Service To implement a service of type GET, create a new method : * Based on the following signature : public final Representation represent() * Add the @Get(\"json\") annotation @Get(\"json\") public final Representation represent() { // Initialise Response Header Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\"); if (responseHeaders == null) { responseHeaders = new Series(Header.class); getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders); } responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\")); responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\")); // Get Route parameter if required {projectid} String projectId = (String) getRequest().getAttributes().get(\"projectid\"); try { .... // Provide Result getResponse().setStatus(Status.SUCCESS_OK); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } You can also extend the AbstractApiResource , a service class provided by the platform and dedicate to services who request a connection to MongoDb database instead of the ServerResource . In this case you will have to implement the doRepresent() method. public class RawMetricListResource extends AbstractApiResource { public Representation doRepresent() { ObjectNode res = mapper.createObjectNode(); ArrayNode metrics = mapper.createArrayNode(); res.put(\"metrics\", metrics); ... return Util.createJsonRepresentation(res); } } POST Service To implement a service of type POST, crate a new method : * Based on the following signature : public Representation myServiceName (Representation entity) * Add the @Post annotation @Post public Representation myServiceName(Representation entity) { try { // Read Json Datas JsonNode json = mapper.readTree(entity.getText()); ... // Provide Result getResponse().setStatus(Status.SUCCESS_CREATED); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } DELETE Service To do .... 3. Document the Service The REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to allow an easy integration, this contract must be documented : 4. Test the Service To do .... Comment","title":"Implementing RESTLET services"},{"location":"others/development/Implementing-Restlet-Service/#implementing-restlet-services","text":"","title":"Implementing RESTLET services"},{"location":"others/development/Implementing-Restlet-Service/#when-to-use-this-guideline","text":"This guideline present how to create a new REST service using the RESTLET framework in the Scava platform.","title":"When to use this guideline ?"},{"location":"others/development/Implementing-Restlet-Service/#context","text":"Scava project manages REST services with the RESTLET framework. The usage of Restlet framework has been inherited from the OSSMETER platform. The RESTLET framework is integrated in the platform in a single OSGI plugin : org.eclipse.crossmeter.platform.client.api. The RESTLET framework used an embedded web server. In order to avoid to multiply the number of deployed web servers, we plan to centralize all REST service implementation in the same plug-in.","title":"Context"},{"location":"others/development/Implementing-Restlet-Service/#you-want-to-access-to-create-a-new-rest-service","text":"","title":"You want to access to create a new REST Service ?"},{"location":"others/development/Implementing-Restlet-Service/#1-create-a-new-route","text":"To register a new RESTLET service, the first step is to define the route (base url which allow to access to this service) and make the link between this route an the implementation of the service.","title":"1. Create a new Route"},{"location":"others/development/Implementing-Restlet-Service/#naming-the-route","text":"The routes (Base URL) of services provided by the platform is normalize. Please refer to this guideline to know ho to define the route of the new service : Naming-Scava-REST-Services.html","title":"Naming the Route"},{"location":"others/development/Implementing-Restlet-Service/#register-the-route","text":"The org.scava.platform.services plug-in contained the class PlatformRoute.java responsible for declaring routes. package org.scava.platform.services; import org.restlet.Application; import org.restlet.Restlet; import org.restlet.routing.Router; public class PlatformRoute extends Application { @Override public Restlet createInboundRoot() { Router router = new Router(getContext()); router.attach(\"/\", PingResource.class); router.attach(\"/search\", SearchProjectResource.class); ... router.attach(\"/projects/p/{projectid}\", ProjectResource.class); router.attach(\"/raw/metrics\", RawMetricListResource.class); ... return router; } } Route Example : router.attach(\"/raw/metrics\", RawMetricListResource.class); \"/raw/metrics\" : Represent the route URL. \"RawMetricListResource.class\" : Represent the class where the service to be implemented for this path. A route can contained some parameters. In this case, parameters are identified by a name with curly brackets {} .","title":"Register the Route"},{"location":"others/development/Implementing-Restlet-Service/#2-implement-the-service","text":"A service implementation is a Java class which extend the ServerResource class provided by the RESTLET framework. To create a new service create a new Class : * Named \" ServiceName \" + Resource. Ex : ProjectCreationResource.java * On a namespace based on the route. Ex : org.scava.platform.services.administration for platform administration services. * Who extend the org.restlet.resource.ServerResource class.","title":"2. Implement the Service"},{"location":"others/development/Implementing-Restlet-Service/#get-service","text":"To implement a service of type GET, create a new method : * Based on the following signature : public final Representation represent() * Add the @Get(\"json\") annotation @Get(\"json\") public final Representation represent() { // Initialise Response Header Series<Header> responseHeaders = (Series<Header>) getResponse().getAttributes().get(\"org.restlet.http.headers\"); if (responseHeaders == null) { responseHeaders = new Series(Header.class); getResponse().getAttributes().put(\"org.restlet.http.headers\", responseHeaders); } responseHeaders.add(new Header(\"Access-Control-Allow-Origin\", \"*\")); responseHeaders.add(new Header(\"Access-Control-Allow-Methods\", \"GET\")); // Get Route parameter if required {projectid} String projectId = (String) getRequest().getAttributes().get(\"projectid\"); try { .... // Provide Result getResponse().setStatus(Status.SUCCESS_OK); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } } You can also extend the AbstractApiResource , a service class provided by the platform and dedicate to services who request a connection to MongoDb database instead of the ServerResource . In this case you will have to implement the doRepresent() method. public class RawMetricListResource extends AbstractApiResource { public Representation doRepresent() { ObjectNode res = mapper.createObjectNode(); ArrayNode metrics = mapper.createArrayNode(); res.put(\"metrics\", metrics); ... return Util.createJsonRepresentation(res); } }","title":"GET Service"},{"location":"others/development/Implementing-Restlet-Service/#post-service","text":"To implement a service of type POST, crate a new method : * Based on the following signature : public Representation myServiceName (Representation entity) * Add the @Post annotation @Post public Representation myServiceName(Representation entity) { try { // Read Json Datas JsonNode json = mapper.readTree(entity.getText()); ... // Provide Result getResponse().setStatus(Status.SUCCESS_CREATED); return new StringRepresentation(...); } catch (IOException e) { StringRepresentation rep = new StringRepresentation(\"{\\\"status\\\":\\\"error\\\", \\\"message\\\" : \\\"\"+e.getMessage()+\"\\\"}\"); rep.setMediaType(MediaType.APPLICATION_JSON); getResponse().setStatus(Status.CLIENT_ERROR_BAD_REQUEST); return rep; } }","title":"POST Service"},{"location":"others/development/Implementing-Restlet-Service/#delete-service","text":"To do ....","title":"DELETE Service"},{"location":"others/development/Implementing-Restlet-Service/#3-document-the-service","text":"The REST services are the main integration points between platform components or between the platform and external clients. This services are the implementation of a contract between the service provider and his consumers. In order to allow an easy integration, this contract must be documented :","title":"3. Document the Service"},{"location":"others/development/Implementing-Restlet-Service/#4-test-the-service","text":"To do ....","title":"4. Test the Service"},{"location":"others/development/Implementing-Restlet-Service/#comment","text":"","title":"Comment"},{"location":"others/development/Licensing/","text":"Licencing for Scava Content The Scava project is licensed under Eclipse Public License - v 2.0 license. As consequence of our status of project hosted by the eclipse foundation, all Scava components must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header. \"Eclipse Public License\" licensing file The text below must be integrated to the root folder of your project on a text file name \"LICENSE\". Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership. Source File header All sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...). Optionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization. Example of Java license header file /******************************************************************************* * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"} * This program and the accompanying materials are made * available under the terms of the Eclipse Public License 2.0 * which is available at https://www.eclipse.org/legal/epl-2.0/ * * SPDX-License-Identifier: EPL-2.0 ******************************************************************************/ Comment n Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files. This plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.","title":"Licencing for Scava"},{"location":"others/development/Licensing/#licencing-for-scava","text":"","title":"Licencing for Scava"},{"location":"others/development/Licensing/#content","text":"The Scava project is licensed under Eclipse Public License - v 2.0 license. As consequence of our status of project hosted by the eclipse foundation, all Scava components must contained licensing information from the first commit. In order to be compliant with the Eclipse foundation requirements, an \"Eclipse Public License - v 2.0\" license file must be added on root folder of the component project and all sources files of the project must have a license Header.","title":"Content"},{"location":"others/development/Licensing/#eclipse-public-license-licensing-file","text":"The text below must be integrated to the root folder of your project on a text file name \"LICENSE\". Eclipse Public License - v 2.0 THE ACCOMPANYING PROGRAM IS PROVIDED UNDER THE TERMS OF THIS ECLIPSE PUBLIC LICENSE (\"AGREEMENT\"). ANY USE, REPRODUCTION OR DISTRIBUTION OF THE PROGRAM CONSTITUTES RECIPIENT'S ACCEPTANCE OF THIS AGREEMENT. 1. DEFINITIONS \"Contribution\" means: a) in the case of the initial Contributor, the initial content Distributed under this Agreement, and b) in the case of each subsequent Contributor: i) changes to the Program, and ii) additions to the Program; where such changes and/or additions to the Program originate from and are Distributed by that particular Contributor. A Contribution \"originates\" from a Contributor if it was added to the Program by such Contributor itself or anyone acting on such Contributor's behalf. Contributions do not include changes or additions to the Program that are not Modified Works. \"Contributor\" means any person or entity that Distributes the Program. \"Licensed Patents\" mean patent claims licensable by a Contributor which are necessarily infringed by the use or sale of its Contribution alone or when combined with the Program. \"Program\" means the Contributions Distributed in accordance with this Agreement. \"Recipient\" means anyone who receives the Program under this Agreement or any Secondary License (as applicable), including Contributors. \"Derivative Works\" shall mean any work, whether in Source Code or other form, that is based on (or derived from) the Program and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. \"Modified Works\" shall mean any work in Source Code or other form that results from an addition to, deletion from, or modification of the contents of the Program, including, for purposes of clarity any new file in Source Code form that contains any contents of the Program. Modified Works shall not include works that contain only declarations, interfaces, types, classes, structures, or files of the Program solely in each case in order to link to, bind by name, or subclass the Program or Modified Works thereof. \"Distribute\" means the acts of a) distributing or b) making available in any manner that enables the transfer of a copy. \"Source Code\" means the form of a Program preferred for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Secondary License\" means either the GNU General Public License, Version 2.0, or any later versions of that license, including any exceptions or additional permissions as identified by the initial Contributor. 2. GRANT OF RIGHTS a) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, Distribute and sublicense the Contribution of such Contributor, if any, and such Derivative Works. b) Subject to the terms of this Agreement, each Contributor hereby grants Recipient a non-exclusive, worldwide, royalty-free patent license under Licensed Patents to make, use, sell, offer to sell, import and otherwise transfer the Contribution of such Contributor, if any, in Source Code or other form. This patent license shall apply to the combination of the Contribution and the Program if, at the time the Contribution is added by the Contributor, such addition of the Contribution causes such combination to be covered by the Licensed Patents. The patent license shall not apply to any other combinations which include the Contribution. No hardware per se is licensed hereunder. c) Recipient understands that although each Contributor grants the licenses to its Contributions set forth herein, no assurances are provided by any Contributor that the Program does not infringe the patent or other intellectual property rights of any other entity. Each Contributor disclaims any liability to Recipient for claims brought by any other entity based on infringement of intellectual property rights or otherwise. As a condition to exercising the rights and licenses granted hereunder, each Recipient hereby assumes sole responsibility to secure any other intellectual property rights needed, if any. For example, if a third party patent license is required to allow Recipient to Distribute the Program, it is Recipient's responsibility to acquire that license before distributing the Program. d) Each Contributor represents that to its knowledge it has sufficient copyright rights in its Contribution, if any, to grant the copyright license set forth in this Agreement. e) Notwithstanding the terms of any Secondary License, no Contributor makes additional grants to any Recipient (other than those set forth in this Agreement) as a result of such Recipient's receipt of the Program under the terms of a Secondary License (if permitted under the terms of Section 3). 3. REQUIREMENTS 3.1 If a Contributor Distributes the Program in any form, then: a) the Program must also be made available as Source Code, in accordance with section 3.2, and the Contributor must accompany the Program with a statement that the Source Code for the Program is available under this Agreement, and informs Recipients how to obtain it in a reasonable manner on or through a medium customarily used for software exchange; and b) the Contributor may Distribute the Program under a license different than this Agreement, provided that such license: i) effectively disclaims on behalf of all other Contributors all warranties and conditions, express and implied, including warranties or conditions of title and non-infringement, and implied warranties or conditions of merchantability and fitness for a particular purpose; ii) effectively excludes on behalf of all other Contributors all liability for damages, including direct, indirect, special, incidental and consequential damages, such as lost profits; iii) does not attempt to limit or alter the recipients' rights in the Source Code under section 3.2; and iv) requires any subsequent distribution of the Program by any party to be under a license that satisfies the requirements of this section 3. 3.2 When the Program is Distributed as Source Code: a) it must be made available under this Agreement, or if the Program (i) is combined with other material in a separate file or files made available under a Secondary License, and (ii) the initial Contributor attached to the Source Code the notice described in Exhibit A of this Agreement, then the Program may be made available under the terms of such Secondary Licenses, and b) a copy of this Agreement must be included with each copy of the Program. 3.3 Contributors may not remove or alter any copyright, patent, trademark, attribution notices, disclaimers of warranty, or limitations of liability (\"notices\") contained within the Program from any copy of the Program which they Distribute, provided that Contributors may add their own appropriate notices. 4. COMMERCIAL DISTRIBUTION Commercial distributors of software may accept certain responsibilities with respect to end users, business partners and the like. While this license is intended to facilitate the commercial use of the Program, the Contributor who includes the Program in a commercial product offering should do so in a manner which does not create potential liability for other Contributors. Therefore, if a Contributor includes the Program in a commercial product offering, such Contributor (\"Commercial Contributor\") hereby agrees to defend and indemnify every other Contributor (\"Indemnified Contributor\") against any losses, damages and costs (collectively \"Losses\") arising from claims, lawsuits and other legal actions brought by a third party against the Indemnified Contributor to the extent caused by the acts or omissions of such Commercial Contributor in connection with its distribution of the Program in a commercial product offering. The obligations in this section do not apply to any claims or Losses relating to any actual or alleged intellectual property infringement. In order to qualify, an Indemnified Contributor must: a) promptly notify the Commercial Contributor in writing of such claim, and b) allow the Commercial Contributor to control, and cooperate with the Commercial Contributor in, the defense and any related settlement negotiations. The Indemnified Contributor may participate in any such claim at its own expense. For example, a Contributor might include the Program in a commercial product offering, Product X. That Contributor is then a Commercial Contributor. If that Commercial Contributor then makes performance claims, or offers warranties related to Product X, those performance claims and warranties are such Commercial Contributor's responsibility alone. Under this section, the Commercial Contributor would have to defend claims against the other Contributors related to those performance claims and warranties, and if a court requires any other Contributor to pay any damages as a result, the Commercial Contributor must pay those damages. 5. NO WARRANTY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, THE PROGRAM IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Each Recipient is solely responsible for determining the appropriateness of using and distributing the Program and assumes all risks associated with its exercise of rights under this Agreement, including but not limited to the risks and costs of program errors, compliance with applicable laws, damage to or loss of data, programs or equipment, and unavailability or interruption of operations. 6. DISCLAIMER OF LIABILITY EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT, AND TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEITHER RECIPIENT NOR ANY CONTRIBUTORS SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OR DISTRIBUTION OF THE PROGRAM OR THE EXERCISE OF ANY RIGHTS GRANTED HEREUNDER, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 7. GENERAL If any provision of this Agreement is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this Agreement, and without further action by the parties hereto, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable. If Recipient institutes patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Program itself (excluding combinations of the Program with other software or hardware) infringes such Recipient's patent(s), then such Recipient's rights granted under Section 2(b) shall terminate as of the date such litigation is filed. All Recipient's rights under this Agreement shall terminate if it fails to comply with any of the material terms or conditions of this Agreement and does not cure such failure in a reasonable period of time after becoming aware of such noncompliance. If all Recipient's rights under this Agreement terminate, Recipient agrees to cease use and distribution of the Program as soon as reasonably practicable. However, Recipient's obligations under this Agreement and any licenses granted by Recipient relating to the Program shall continue and survive. Everyone is permitted to copy and distribute copies of this Agreement, but in order to avoid inconsistency the Agreement is copyrighted and may only be modified in the following manner. The Agreement Steward reserves the right to publish new versions (including revisions) of this Agreement from time to time. No one other than the Agreement Steward has the right to modify this Agreement. The Eclipse Foundation is the initial Agreement Steward. The Eclipse Foundation may assign the responsibility to serve as the Agreement Steward to a suitable separate entity. Each new version of the Agreement will be given a distinguishing version number. The Program (including Contributions) may always be Distributed subject to the version of the Agreement under which it was received. In addition, after a new version of the Agreement is published, Contributor may elect to Distribute the Program (including its Contributions) under the new version. Except as expressly stated in Sections 2(a) and 2(b) above, Recipient receives no rights or licenses to the intellectual property of any Contributor under this Agreement, whether expressly, by implication, estoppel or otherwise. All rights in the Program not expressly granted under this Agreement are reserved. Nothing in this Agreement is intended to be enforceable by any entity that is not a Contributor or Recipient. No third-party beneficiary rights are created under this Agreement. Exhibit A - Form of Secondary Licenses Notice \"This Source Code may also be made available under the following Secondary Licenses when the conditions for such availability set forth in the Eclipse Public License, v. 2.0 are satisfied: {name license(s), version(s), and exceptions or additional permissions here}.\" Simply including a copy of this Agreement, including this Exhibit A is not sufficient to license the Source Code under Secondary Licenses. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership.","title":"\"Eclipse Public License\" licensing file"},{"location":"others/development/Licensing/#source-file-header","text":"All sources files of each Scava component projects must have a license Header. This file must be fill with the name of organization of the partner who have implemented the file ( example : Softeam,University of York ...). Optionally an \"And Others\" following the name of the main contributor can be added to indicate that this file is a result of a collaboration between several organization. Example of Java license header file /******************************************************************************* * Copyright (c) 2018 {Name of the partner} {optional: \"And Others\"} * This program and the accompanying materials are made * available under the terms of the Eclipse Public License 2.0 * which is available at https://www.eclipse.org/legal/epl-2.0/ * * SPDX-License-Identifier: EPL-2.0 ******************************************************************************/","title":"Source File header"},{"location":"others/development/Licensing/#comment","text":"n Eclipse plugin : the Copyright Wizard plulgin could help you to manage licensing files. This plug-in adds a wizard allowing to apply a copyright header comment to a selection of files in projects. The same text can be applyied on different types of files (java, xml...), the comment format being adapted in function of the content type.","title":"Comment"},{"location":"others/development/Naming-Scava-REST-Services/","text":"Naming Scava REST services When to use this guideline ? This guideline present how to define the route of a new REST service provided by the Scava platform. Context The REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of platform services , we need to used a common naming schema for all REST services provided by the platform. How to name a REST service ? /{ componentid }/{ categoryname }/{ servicename } / componentid / : Name of the Architectural component which provide the service / categoryname / (Optional) : optional category of the service / servicename / : Name of the rest service Component Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration Comment","title":"Naming Scava REST services"},{"location":"others/development/Naming-Scava-REST-Services/#naming-scava-rest-services","text":"","title":"Naming Scava REST services"},{"location":"others/development/Naming-Scava-REST-Services/#when-to-use-this-guideline","text":"This guideline present how to define the route of a new REST service provided by the Scava platform.","title":"When to use this guideline ?"},{"location":"others/development/Naming-Scava-REST-Services/#context","text":"The REST services are the main integration points between platform components or between the platform and external clients. In order to provide an unified view of platform services , we need to used a common naming schema for all REST services provided by the platform.","title":"Context"},{"location":"others/development/Naming-Scava-REST-Services/#how-to-name-a-rest-service","text":"/{ componentid }/{ categoryname }/{ servicename } / componentid / : Name of the Architectural component which provide the service / categoryname / (Optional) : optional category of the service / servicename / : Name of the rest service","title":"How to name a REST service ?"},{"location":"others/development/Naming-Scava-REST-Services/#component","text":"Component ComponentId DevOps Dashboard dashboard Workflow Execution Engine workflow Knowledge Base knowledgebase Metric Provider metricprovider Administration administration","title":"Component"},{"location":"others/development/Naming-Scava-REST-Services/#comment","text":"","title":"Comment"},{"location":"others/development/Repository-Organisation/","text":"The SCAVA code repository is organized by functional components with one package for each of this components. General organisation metric-platform platform : Core projects of the metric platform platform-extensions : Extensions of the metric platform metric-providers : Metric Providers implementations projects factoids : Factoids implementations projects tests : Test projects related to the metric-platform web-dashboards : DevOps Dashboard and DevOpsDashboard components knowledge-base : Knowledge Base implementation Workflow Execution Engine : Workflow Execution Engine implementation eclipse-based-ide : SCAVA Eclipse plugin api-gateway : API Gateway implementation. administration : Administration component implementation mingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project. Comments","title":"Repository Organisation"},{"location":"others/development/Repository-Organisation/#general-organisation","text":"metric-platform platform : Core projects of the metric platform platform-extensions : Extensions of the metric platform metric-providers : Metric Providers implementations projects factoids : Factoids implementations projects tests : Test projects related to the metric-platform web-dashboards : DevOps Dashboard and DevOpsDashboard components knowledge-base : Knowledge Base implementation Workflow Execution Engine : Workflow Execution Engine implementation eclipse-based-ide : SCAVA Eclipse plugin api-gateway : API Gateway implementation. administration : Administration component implementation mingration : Temporary directory which contains sources which are currently required to run the platform but that will be replaced during the project.","title":"General organisation"},{"location":"others/development/Repository-Organisation/#comments","text":"","title":"Comments"},{"location":"others/development/Testing-Guidelines/","text":"Knowledge Base This builds needs some configuration to run successfully. In the application.properties files: * /knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties * /knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties Edit the following parameters: lucene.index.folder=/tmp/scava_lucene/ egit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57 To generate the GitHub access token you need to go to your own GitHub account and create a new one. Once it's done simply restart the tests, they should pass.","title":"Testing Guidelines"},{"location":"others/development/Testing-Guidelines/#knowledge-base","text":"This builds needs some configuration to run successfully. In the application.properties files: * /knowledge-base/org.eclipse.scava.knowledgebase/src/main/resources/application.properties * /knowledge-base/org.eclipse.scava.knowledgebase/src/test/resources/application.properties Edit the following parameters: lucene.index.folder=/tmp/scava_lucene/ egit.github.token=3f5accc2f8f4cXXXXXXXXX73b201692fc1df57 To generate the GitHub access token you need to go to your own GitHub account and create a new one. Once it's done simply restart the tests, they should pass.","title":"Knowledge Base"},{"location":"others/users/Consuming-REST-Services/","text":"Consuming REST services When to use ? This guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications. REST API Reference The reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]]. API Gateway The Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway. All web service request form clients have to go through the gateway. The api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform. Platform Authentication The CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io). 1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. 1. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Authentication in Java Retrieve a Web Tokens from authentication service private String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException { // Authentication Service URI URL url = new URL(\"http://localhost:8086/api/authentication\"); // AUthentication Request HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setDoOutput(true); connection.setRequestMethod(\"POST\"); connection.setRequestProperty(\"Content-Type\", \"application/json\"); String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\"; OutputStream os = connection.getOutputStream(); os.write(input.getBytes()); os.flush(); if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) { throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode()); } connection.disconnect(); // A JWT Token is return in the Header of the response return connection.getHeaderField(\"Authorization\"); } REST Service Call curl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication Service Consumption To consume a REST service provided by the integrated platform, the client must include the Token in the header of his request. ```java // Service URL URL url = new URL(\"http://localhost:8086/api/users\"); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setRequestMethod(\"GET\"); // Add Token to the request header connection.setRequestProperty(\"Authorization\",token); ``` Comment","title":"Consuming REST services"},{"location":"others/users/Consuming-REST-Services/#consuming-rest-services","text":"","title":"Consuming REST services"},{"location":"others/users/Consuming-REST-Services/#when-to-use","text":"This guideline describes general way of consuming REST services of Scava. Its basically for the use of Scava rest APIs in other tools/applications.","title":"When to use ?"},{"location":"others/users/Consuming-REST-Services/#rest-api-reference","text":"The reference guide presenting all REST services implemented by Scava is available [[right here |Development Guidelignes]].","title":"REST API Reference"},{"location":"others/users/Consuming-REST-Services/#api-gateway","text":"The Scava integrated platform provide a centralized access point to all web services implemented by the different tools involved in the platform : the Scava API Gateway. All web service request form clients have to go through the gateway. The api gateway is in charge to redirect the client request to the right service provider. The gateway also manage authentication mechanism for all services provided by the integrated platform.","title":"API Gateway"},{"location":"others/users/Consuming-REST-Services/#platform-authentication","text":"The CROSSMIER API Gateway is secruized using JSON Web Tokens (JWT https://jwt.io). 1. To obtain an access to a specific service, the client must authenticate with the authentication service.If the authentication success,he recived a web token that should be include in the header of all of his future requests. 1. When the client request a specific service, the api gateway valivate the token from the authentication service. If the token is valide, the api gateway transmite the request to the related service. Authentication in Java Retrieve a Web Tokens from authentication service private String getAuthToken(String login,String password) throws MalformedURLException, IOException, ProtocolException { // Authentication Service URI URL url = new URL(\"http://localhost:8086/api/authentication\"); // AUthentication Request HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setDoOutput(true); connection.setRequestMethod(\"POST\"); connection.setRequestProperty(\"Content-Type\", \"application/json\"); String input = \"{\\\"username\\\":\\\"\"+login+\"\\\",\\\"password\\\":\\\"\"+password+\"\\\"}\"; OutputStream os = connection.getOutputStream(); os.write(input.getBytes()); os.flush(); if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) { throw new RuntimeException(\"Failed : HTTP error code : \"+ connection.getResponseCode()); } connection.disconnect(); // A JWT Token is return in the Header of the response return connection.getHeaderField(\"Authorization\"); } REST Service Call curl -d '{\"username\":\"admin\", \"password\":\"admin\"}' -H \"Content-Type: application/json\" -X POST http://localhost:8086/api/authentication","title":"Platform Authentication"},{"location":"others/users/Consuming-REST-Services/#service-consumption","text":"To consume a REST service provided by the integrated platform, the client must include the Token in the header of his request. ```java // Service URL URL url = new URL(\"http://localhost:8086/api/users\"); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); connection.setRequestMethod(\"GET\"); // Add Token to the request header connection.setRequestProperty(\"Authorization\",token); ```","title":"Service Consumption"},{"location":"others/users/Consuming-REST-Services/#comment","text":"","title":"Comment"},{"location":"others/users/REST-API-Documentation/","text":"REST API Documentation Project Administration API The Administration API provides services related to platform administration, user management and registration of projects to be analyse by the platform. Base URL : \"/administration/\" Project Management Manage registration of projects analysed by the platform. Projects List GET /administration/projects/{page} Retrieve a list of monitored projects. Project Search GET /administration/projects/search Retrieve a list of monitored projects base on a search query. Project Info GET /administration/projects/{projectId} Retrieve the metadata of a specific project. Import Project POST /administration/projects/import Import a new project to be analyzed by the platform. Take as input the URL of the repository of the project to import. Register Project POST /administration/projects/register Register a new project to be analyzed by the platform. Take as input the definition of project to analyse. Project Analysis Analyse projects registered on the platform. Create Analysis Task POST /administration/task/create Create a new analysis task. Update Analysis Task PUT /administration/task/update Update an analysis task. Start Analysis Task POST /administration/task/start Start an analysis task. Stop Analysis Task POST /administration/task/stop Stop an analysis task. Reset Analysis Task POST /administration/task/reset Reset an analysis task. Delete Analysis Task DELETE /administration/task/delete/{analysisTaskId} Delete an analysis task. Analysis Tasks List GET /analysis/tasks Retrieve a list of analysis tasks. Analysis Task GET /analysis/task/{analysistaskid} Retrieve an analysis task by Id. Analysis Tasks List By Project GET /analysis/tasks/project/{projectid} Retrieve a list of analysis tasks by project. Analysis Tasks Status By Project GET /analysis/tasks/status/project/{projectid} Retrieve a gloabl status of the analysis tasks by project. Workers List GET /analysis/workers Retrieve a list of workers. Metric Providers List GET /analysis/metricproviders Retrieve a list of Metric Providers. Promote Analysis Task DELETE /analysis/task/promote/{analysisTaskId} Push up an analysis task in Workers Queue. Demote Analysis Task DELETE /analysis/task/demote/{analysisTaskId} Push down an analysis task in Workers Queue. Push Analysis Task On Worker DELETE /analysis/task/pushOnWorker/{analysisTaskId}/w/{workerId} Force the execution of an analysis task by a worker. Metrics Provider API Access to Mertics collected by the Metrics Provider component. List of Metrics GET /metricprovider/metrics Retrieve a list of the visualisable metric providers supported by the plat- form. It includes information about how the metric provider should be visualised. List of Raw Metrics GET /metricprovider/raw/metrics Retrieve a list of the all of the metric providers supported by the plat- form. List of Metric Visualisation GET /metricprovider/projects/p/{projectId}/m/{metricId} Retrieve the visualisation of a specific project metric provider. Visuali- sations are defined in the MetVis language. List of Factoids GET /metricprovider/factoids Retrieve a list of factoids supported by the platform. List of Project Factoids GET /metricprovider/projects/p/{projectId}/f Retrieve the data of a specific factoid for a project. Metric Raw Data GET /metricprovider/raw/projects/p/{projectId}/m/{metricId} Retrieve the raw data of a project\u2019s metric provider. This is essentially a JSON dump of the metric provider\u2019s MongoDB collection. Workflow Execution Engine API TODO Knowledge Base API List of analyzed artifacts GET /api/artifacts Retrieve a list of monitored artifacts. List of similar artifacts GET /api/recommendation/similar/p/{id}/m/{sim_method}/n/{num} This resource is used to retrieve projects that are similar to a given project. DevOps Dashboard API","title":"REST API Documentation"},{"location":"others/users/REST-API-Documentation/#rest-api-documentation","text":"","title":"REST API Documentation"},{"location":"others/users/REST-API-Documentation/#project-administration-api","text":"The Administration API provides services related to platform administration, user management and registration of projects to be analyse by the platform. Base URL : \"/administration/\"","title":"Project Administration API"},{"location":"others/users/REST-API-Documentation/#project-management","text":"Manage registration of projects analysed by the platform. Projects List GET /administration/projects/{page} Retrieve a list of monitored projects. Project Search GET /administration/projects/search Retrieve a list of monitored projects base on a search query. Project Info GET /administration/projects/{projectId} Retrieve the metadata of a specific project. Import Project POST /administration/projects/import Import a new project to be analyzed by the platform. Take as input the URL of the repository of the project to import. Register Project POST /administration/projects/register Register a new project to be analyzed by the platform. Take as input the definition of project to analyse.","title":"Project Management"},{"location":"others/users/REST-API-Documentation/#project-analysis","text":"Analyse projects registered on the platform. Create Analysis Task POST /administration/task/create Create a new analysis task. Update Analysis Task PUT /administration/task/update Update an analysis task. Start Analysis Task POST /administration/task/start Start an analysis task. Stop Analysis Task POST /administration/task/stop Stop an analysis task. Reset Analysis Task POST /administration/task/reset Reset an analysis task. Delete Analysis Task DELETE /administration/task/delete/{analysisTaskId} Delete an analysis task. Analysis Tasks List GET /analysis/tasks Retrieve a list of analysis tasks. Analysis Task GET /analysis/task/{analysistaskid} Retrieve an analysis task by Id. Analysis Tasks List By Project GET /analysis/tasks/project/{projectid} Retrieve a list of analysis tasks by project. Analysis Tasks Status By Project GET /analysis/tasks/status/project/{projectid} Retrieve a gloabl status of the analysis tasks by project. Workers List GET /analysis/workers Retrieve a list of workers. Metric Providers List GET /analysis/metricproviders Retrieve a list of Metric Providers. Promote Analysis Task DELETE /analysis/task/promote/{analysisTaskId} Push up an analysis task in Workers Queue. Demote Analysis Task DELETE /analysis/task/demote/{analysisTaskId} Push down an analysis task in Workers Queue. Push Analysis Task On Worker DELETE /analysis/task/pushOnWorker/{analysisTaskId}/w/{workerId} Force the execution of an analysis task by a worker.","title":"Project Analysis"},{"location":"others/users/REST-API-Documentation/#metrics-provider-api","text":"Access to Mertics collected by the Metrics Provider component. List of Metrics GET /metricprovider/metrics Retrieve a list of the visualisable metric providers supported by the plat- form. It includes information about how the metric provider should be visualised. List of Raw Metrics GET /metricprovider/raw/metrics Retrieve a list of the all of the metric providers supported by the plat- form. List of Metric Visualisation GET /metricprovider/projects/p/{projectId}/m/{metricId} Retrieve the visualisation of a specific project metric provider. Visuali- sations are defined in the MetVis language. List of Factoids GET /metricprovider/factoids Retrieve a list of factoids supported by the platform. List of Project Factoids GET /metricprovider/projects/p/{projectId}/f Retrieve the data of a specific factoid for a project. Metric Raw Data GET /metricprovider/raw/projects/p/{projectId}/m/{metricId} Retrieve the raw data of a project\u2019s metric provider. This is essentially a JSON dump of the metric provider\u2019s MongoDB collection.","title":"Metrics Provider API"},{"location":"others/users/REST-API-Documentation/#workflow-execution-engine-api","text":"TODO","title":"Workflow Execution Engine API"},{"location":"others/users/REST-API-Documentation/#knowledge-base-api","text":"List of analyzed artifacts GET /api/artifacts Retrieve a list of monitored artifacts. List of similar artifacts GET /api/recommendation/similar/p/{id}/m/{sim_method}/n/{num} This resource is used to retrieve projects that are similar to a given project.","title":"Knowledge Base API"},{"location":"others/users/REST-API-Documentation/#devops-dashboard-api","text":"","title":"DevOps Dashboard API"},{"location":"others/users/REST-API-Generation/","text":"REST API Generation REST API Tutorial files Install First of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install. Execute the following steps. Clone repository: https://github.com/patrickneubauer/crossminer-workflow Import projects from the cloned repo to empty Eclipse workspace: Import root via Maven > Existing Maven Projects , then the rest via General > Existing projects into Workspace without checking Search for nested projects . Install new software packages: Install Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling [web page] Install Epsilon (Stable): http://download.eclipse.org/epsilon/updates/ [update site] Update Epsilon (Interim): http://download.eclipse.org/epsilon/interim/ [update site] Install Emfatic: http://download.eclipse.org/emfatic/update/ [update site] Note: I didn't see the the feature until Group items by category was unchecked. Install GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ [update site] Import the given json projects ( org.eclipse.epsilon.emc.json and org.eclipse.epsilon.emc.json.dt ) into the workspace via General > Existing projects into Workspace . Now right-click on org.eclipse.crossmeter.workflow project and select Maven > Update Project.... Copy the give M2M_Environment_oxygen.launch (for Windows 10) to org.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator and overwrite the old one. Refresh project org.eclipse.crossmeter.workflow.restmule.generator in Eclipse. Before running the .launch file, right-click on it, Run as > Run configurations.... Here go to Plug-ins tab and click on Add Required Plug-ins . Now you can run the .launch file TBA:How? In the Runtime Eclipse import the org.eclipse.crossmeter.workflow.restmule.generator project as Maven Project. Run generateFromOAS.launch as you can see in the videos https://youtu.be/BJXuozHJPeg. Now, you should see it generating the project. For further steps, watch Patrick's videos . github client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg github client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI github client generation example \u2014 https://youtu.be/ltSNnSZRETA kafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc kafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4 kafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA MDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk Example I've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following: /square/{number} . You can replace the {number} with any number. The repsonse for this request is a JSON object, which looks like this: { \"squared\": {squaredValue} } It contains only one field, named squared , which has the value of the square of the given number. From the specification the generator will provide us a Java Project which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines: ITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings). int number = 7; //The number to be squared. IData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server. NumberValue numberValue; //NumberValue is the class which represents the object received from the server numberValue = squared.observe().blockingSingle(); //Get the actually received object from the response. System.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer. Everything, including the IEntityApi interface and the NumberValue class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through Java to the objects/field inside of the received JSON object. And once again, they are all generated from the OpenAPI specification. So, to generate the mentioned Java Project , you have to put your OpenAPI specification file into the schemas folder inside the org.eclipse.crossmeter.workflow.restmule.generator project. I'll call mine as TestAPI.json , and it's made up of the following: { \"swagger\": \"2.0\", \"schemes\": [ \"http\" ], \"host\": \"localhost:8080\", \"basePath\": \"/\", \"info\": { \"description\": \"This is a test API, only for demonstration of the generator.\", \"termsOfService\": \"\", \"title\": \"TestAPI\", \"version\": \"v1\" }, \"consumes\": [ \"application/json\" ], \"produces\": [ \"application/json\" ], \"securityDefinitions\": { }, \"paths\": { \"/square/{number}\": { \"get\": { \"description\": \"Square a number.\", \"parameters\": [ { \"description\": \"The number to be squared\", \"in\": \"path\", \"name\": \"number\", \"required\": true, \"type\": \"integer\" } ], \"responses\": { \"200\": { \"description\": \"OK\", \"schema\": { \"$ref\": \"#/definitions/NumberValue\" } } } } } }, \"definitions\": { \"NumberValue\": { \"properties\": { \"squared\": { \"type\": \"integer\" } }, \"type\": \"object\" } } } You can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own. Then we have to set the generator to use our new schema. To do this, open the build.xml in the generator project and modify the api and json.model.file property to testapi and schemas/TestAPI.json , respectively. <!--API Variables --> <property name=\"api\" value=\"testapi\" /> <property name=\"json.model.file\" value=\"schemas/TestAPI.json\" /> We are almost done, but we have to take one more small step. We have to provide an .eol file in the epsilon/util/fix/ folder in the generator project. Its name must be the same as the value of the api property in the previous step, so for this example we use the testapi.eol . The content of this file: import \"../restmule.eol\"; var api = RestMule!API.all.first(); // RATE LIMITS var search = new RestMule!RatePolicyScope; search.scope = \"Search\"; var entity = new RestMule!RatePolicyScope; entity.scope = \"Entity\"; // RATE POLICY var policy = new RestMule!RatePolicy; policy.scopes.add(entity); policy.scopes.add(search); var reset = new RestMule!ResponseHeader; var resetInt = new RestMule!TInteger; resetInt.label = \"X-RateLimit-Reset\"; reset.type = resetInt; policy.reset = reset; var limit = new RestMule!ResponseHeader; var limitInt = new RestMule!TInteger; limitInt.label = \"X-RateLimit-Limit\"; limit.type = limitInt; policy.limit = limit; var remaining = new RestMule!ResponseHeader; var remainingInt = new RestMule!TInteger; remainingInt.label = \"X-RateLimit-Remaining\"; remaining.type = remainingInt; policy.remaining = remaining; api.ratePolicy = policy; // PAGINATION var pagination= new RestMule!PaginationPolicy; pagination.start = 1; pagination.max = 10; pagination.increment = 1; pagination.maxPerIteration = 100; var perIteration = new RestMule!Query; perIteration.description = \"Items per page\"; perIteration.required = false; var type = new RestMule!TInteger; type.label = \"per_page\"; type.name = type.label; perIteration.type = type; pagination.perIteration = perIteration; var page = new RestMule!Query; page.description = \"Page identifier\"; page.required = false; var type1 = new RestMule!TInteger; type1.label = \"page\"; type1.name = type1.label; page.type = type1; pagination.page = page; var link = new RestMule!ResponseHeader; link.description = \"Page links\"; var format = new RestMule!TFormattedString; format.label = \"Link\"; format.name = format.label; link.type = format; pagination.links = link; api.pagination = pagination; // WRAPPER var wrapper = new RestMule!Wrapper; wrapper.name = \"Wrapper\"; var items = new RestMule!ListType; items.label = \"items\"; wrapper.items = items; wrapper.totalLabel= \"total_count\"; wrapper.incompleteLabel = \"incomplete_results\"; api.pageWrapper = wrapper; // ADD RATE & WRAPPER TO REQUESTS (FIXME) for (r in RestMule!Request.all){ if (r.parent.path.startsWith(\"/search\")){ r.scope = search; } else { r.scope = entity; } r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader))); for (resp in r.responses.select(s|s.responseType <> null)){ resp.unwrap(); } } /* ////////// * OPERATIONS *////////// operation RestMule!ObjectType hasWrapper() : Boolean { var wrapper = RestMule!Wrapper.all.first; var lists = self.listFields.collect(a|a.label); return (not lists.isEmpty()) and lists .includes(wrapper.items.label); } operation RestMule!Response unwrap() : RestMule!ObjectType{ if (self.responseType.instanceOf(ObjectType)){ if (self.responseType.hasWrapper()){ (\"Unwrapping : \"+ self.responseType.name).println; var wrapper = RestMule!Wrapper.all.first; var name = self.responseType.name.println; self.responseType = self.responseType.println.listFields .select(b| b.label == wrapper.items.label).first.elements.first; self.responseType.name = name; self.pageWrapped = true; self.responseType.description = \"UNWRAPPED: \" + self.responseType.description; } } } After this, we can run the generator and it will generate our Java Project from the specification. In the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.","title":"REST API Generation"},{"location":"others/users/REST-API-Generation/#rest-api-generation","text":"REST API Tutorial files","title":"REST API Generation"},{"location":"others/users/REST-API-Generation/#install","text":"First of all, I installed a new copy of Eclipse (Committers), just to make sure I start with a clean install. Execute the following steps. Clone repository: https://github.com/patrickneubauer/crossminer-workflow Import projects from the cloned repo to empty Eclipse workspace: Import root via Maven > Existing Maven Projects , then the rest via General > Existing projects into Workspace without checking Search for nested projects . Install new software packages: Install Graphical Modeling Framework (GMF): http://marketplace.eclipse.org/content/graphical-modeling-framework-gmf-tooling [web page] Install Epsilon (Stable): http://download.eclipse.org/epsilon/updates/ [update site] Update Epsilon (Interim): http://download.eclipse.org/epsilon/interim/ [update site] Install Emfatic: http://download.eclipse.org/emfatic/update/ [update site] Note: I didn't see the the feature until Group items by category was unchecked. Install GMF tooling: http://download.eclipse.org/modeling/gmp/gmf-tooling/updates/releases/ [update site] Import the given json projects ( org.eclipse.epsilon.emc.json and org.eclipse.epsilon.emc.json.dt ) into the workspace via General > Existing projects into Workspace . Now right-click on org.eclipse.crossmeter.workflow project and select Maven > Update Project.... Copy the give M2M_Environment_oxygen.launch (for Windows 10) to org.eclipse.crossmeter.workflow\\org.eclipse.crossmeter.workflow.restmule.generator and overwrite the old one. Refresh project org.eclipse.crossmeter.workflow.restmule.generator in Eclipse. Before running the .launch file, right-click on it, Run as > Run configurations.... Here go to Plug-ins tab and click on Add Required Plug-ins . Now you can run the .launch file TBA:How? In the Runtime Eclipse import the org.eclipse.crossmeter.workflow.restmule.generator project as Maven Project. Run generateFromOAS.launch as you can see in the videos https://youtu.be/BJXuozHJPeg. Now, you should see it generating the project. For further steps, watch Patrick's videos . github client generation and execution example \u2014 https://youtu.be/BJXuozHJPeg github client generation example 2 \u2014 https://youtu.be/CIebrgRE4zI github client generation example \u2014 https://youtu.be/ltSNnSZRETA kafka twitter default example \u2014 https://youtu.be/3XZMMQyURVc kafka twitter workflow example 2 \u2014 https://youtu.be/Udqd-gaH4O4 kafka twitter workflow example \u2014 https://youtu.be/DB03Rtfa5ZA MDEPopularityExample \u2014 https://youtu.be/PBeuOaqHngk","title":"Install"},{"location":"others/users/REST-API-Generation/#example","text":"I've made a simple example for the generator. I've created an OpenAPI specification and then built it with the generator. You will see a minimalistic configuration: it contains a single path specification, which requires one number input parameter and then gives back the parameters square. The number input parameter is provided via the path, so it looks like the following: /square/{number} . You can replace the {number} with any number. The repsonse for this request is a JSON object, which looks like this: { \"squared\": {squaredValue} } It contains only one field, named squared , which has the value of the square of the given number. From the specification the generator will provide us a Java Project which will implement the use of the specified API. It will provide us an interface to easily access the functionalities of the API. To use this interface, after the import of the proper dependencies, we only need to write a few lines: ITestAPIApi api = TestAPIApi.createDefault(); //create an instance of the api interface (with default settings). int number = 7; //The number to be squared. IData<NumberValue> squared = api.getSquareNumberValueByNumber(number); //the actual use of the API descibed in the specification. It sends a request to the server. NumberValue numberValue; //NumberValue is the class which represents the object received from the server numberValue = squared.observe().blockingSingle(); //Get the actually received object from the response. System.out.println(\"Squared:\" + numberValue.getSquared()); //Print the received answer. Everything, including the IEntityApi interface and the NumberValue class is generated by the generator. The former is a complete set of the functions provided by the API and the latter is an example of the container classes, which has the purpose of letting access through Java to the objects/field inside of the received JSON object. And once again, they are all generated from the OpenAPI specification. So, to generate the mentioned Java Project , you have to put your OpenAPI specification file into the schemas folder inside the org.eclipse.crossmeter.workflow.restmule.generator project. I'll call mine as TestAPI.json , and it's made up of the following: { \"swagger\": \"2.0\", \"schemes\": [ \"http\" ], \"host\": \"localhost:8080\", \"basePath\": \"/\", \"info\": { \"description\": \"This is a test API, only for demonstration of the generator.\", \"termsOfService\": \"\", \"title\": \"TestAPI\", \"version\": \"v1\" }, \"consumes\": [ \"application/json\" ], \"produces\": [ \"application/json\" ], \"securityDefinitions\": { }, \"paths\": { \"/square/{number}\": { \"get\": { \"description\": \"Square a number.\", \"parameters\": [ { \"description\": \"The number to be squared\", \"in\": \"path\", \"name\": \"number\", \"required\": true, \"type\": \"integer\" } ], \"responses\": { \"200\": { \"description\": \"OK\", \"schema\": { \"$ref\": \"#/definitions/NumberValue\" } } } } } }, \"definitions\": { \"NumberValue\": { \"properties\": { \"squared\": { \"type\": \"integer\" } }, \"type\": \"object\" } } } You can see here the path and the NumberValue object. They are well specified in this file, so the generator can create our project by on its own. Then we have to set the generator to use our new schema. To do this, open the build.xml in the generator project and modify the api and json.model.file property to testapi and schemas/TestAPI.json , respectively. <!--API Variables --> <property name=\"api\" value=\"testapi\" /> <property name=\"json.model.file\" value=\"schemas/TestAPI.json\" /> We are almost done, but we have to take one more small step. We have to provide an .eol file in the epsilon/util/fix/ folder in the generator project. Its name must be the same as the value of the api property in the previous step, so for this example we use the testapi.eol . The content of this file: import \"../restmule.eol\"; var api = RestMule!API.all.first(); // RATE LIMITS var search = new RestMule!RatePolicyScope; search.scope = \"Search\"; var entity = new RestMule!RatePolicyScope; entity.scope = \"Entity\"; // RATE POLICY var policy = new RestMule!RatePolicy; policy.scopes.add(entity); policy.scopes.add(search); var reset = new RestMule!ResponseHeader; var resetInt = new RestMule!TInteger; resetInt.label = \"X-RateLimit-Reset\"; reset.type = resetInt; policy.reset = reset; var limit = new RestMule!ResponseHeader; var limitInt = new RestMule!TInteger; limitInt.label = \"X-RateLimit-Limit\"; limit.type = limitInt; policy.limit = limit; var remaining = new RestMule!ResponseHeader; var remainingInt = new RestMule!TInteger; remainingInt.label = \"X-RateLimit-Remaining\"; remaining.type = remainingInt; policy.remaining = remaining; api.ratePolicy = policy; // PAGINATION var pagination= new RestMule!PaginationPolicy; pagination.start = 1; pagination.max = 10; pagination.increment = 1; pagination.maxPerIteration = 100; var perIteration = new RestMule!Query; perIteration.description = \"Items per page\"; perIteration.required = false; var type = new RestMule!TInteger; type.label = \"per_page\"; type.name = type.label; perIteration.type = type; pagination.perIteration = perIteration; var page = new RestMule!Query; page.description = \"Page identifier\"; page.required = false; var type1 = new RestMule!TInteger; type1.label = \"page\"; type1.name = type1.label; page.type = type1; pagination.page = page; var link = new RestMule!ResponseHeader; link.description = \"Page links\"; var format = new RestMule!TFormattedString; format.label = \"Link\"; format.name = format.label; link.type = format; pagination.links = link; api.pagination = pagination; // WRAPPER var wrapper = new RestMule!Wrapper; wrapper.name = \"Wrapper\"; var items = new RestMule!ListType; items.label = \"items\"; wrapper.items = items; wrapper.totalLabel= \"total_count\"; wrapper.incompleteLabel = \"incomplete_results\"; api.pageWrapper = wrapper; // ADD RATE & WRAPPER TO REQUESTS (FIXME) for (r in RestMule!Request.all){ if (r.parent.path.startsWith(\"/search\")){ r.scope = search; } else { r.scope = entity; } r.parameters.removeAll(r.parameters.select(p|p.instanceOf(RequestHeader))); for (resp in r.responses.select(s|s.responseType <> null)){ resp.unwrap(); } } /* ////////// * OPERATIONS *////////// operation RestMule!ObjectType hasWrapper() : Boolean { var wrapper = RestMule!Wrapper.all.first; var lists = self.listFields.collect(a|a.label); return (not lists.isEmpty()) and lists .includes(wrapper.items.label); } operation RestMule!Response unwrap() : RestMule!ObjectType{ if (self.responseType.instanceOf(ObjectType)){ if (self.responseType.hasWrapper()){ (\"Unwrapping : \"+ self.responseType.name).println; var wrapper = RestMule!Wrapper.all.first; var name = self.responseType.name.println; self.responseType = self.responseType.println.listFields .select(b| b.label == wrapper.items.label).first.elements.first; self.responseType.name = name; self.pageWrapped = true; self.responseType.description = \"UNWRAPPED: \" + self.responseType.description; } } } After this, we can run the generator and it will generate our Java Project from the specification. In the attached zip you can find all of the releated projects and files. There is a spring-boot server, which can serve the request, a java client, which uses the generated project, an eclipse plug-in project which also uses the generated project, the generated project and the additional files for the generator. And there is one more project, which provides the proper dependecies for the plug-in project.","title":"Example"},{"location":"others/users/Running-Scava-in-Eclipse/","text":"Running Scava in Eclipse This page gives general guidelines for running Ossmeter platform in Eclipse The screeshot below will be used as a reference : (Eclipse version used in screenshot : Eclipse Java EE IDE for Web developers Luna 2 (4.4.2) ) We compile with Ossmeterfromfeature.product file in org.ossmeter.platform.osgi (highlighted above) In the file we put program arguements : -apiServer , -master , -slave . More information on https://github.com/crossminer/crossminer/wiki/Running-the-platform We must validate with the button on corner top right (highlighted above). This will add the required packages. If the required packages are still missing, go to Run Configurations -> Plug-ins and click on Add Required Plugins . You could also check the general information , if its the same (as in the screenshot below). These were some of the things for running the platform. If there is any issue still, do not hesitate to contact us.","title":"Running Scava in Eclipse"},{"location":"others/users/Running-Scava-in-Eclipse/#running-scava-in-eclipse","text":"This page gives general guidelines for running Ossmeter platform in Eclipse The screeshot below will be used as a reference : (Eclipse version used in screenshot : Eclipse Java EE IDE for Web developers Luna 2 (4.4.2) ) We compile with Ossmeterfromfeature.product file in org.ossmeter.platform.osgi (highlighted above) In the file we put program arguements : -apiServer , -master , -slave . More information on https://github.com/crossminer/crossminer/wiki/Running-the-platform We must validate with the button on corner top right (highlighted above). This will add the required packages. If the required packages are still missing, go to Run Configurations -> Plug-ins and click on Add Required Plugins . You could also check the general information , if its the same (as in the screenshot below). These were some of the things for running the platform. If there is any issue still, do not hesitate to contact us.","title":"Running Scava in Eclipse"},{"location":"others/users/Scava-Metrics/","text":"Metrics computed by Scava Metric-platform OSGi metrics are defined in: https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.osgi/src/OSGi.rsc List of metrics: allOSGiBundleDependencies -- Retrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies). allOSGiPackageDependencies -- Retrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). allOSGiDynamicImportedPackages -- Retrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file. numberOSGiPackageDependencies -- Retrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). numberOSGiBundleDependencies -- Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies). unusedOSGiImportedPackages -- Retrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell). ratioUnusedOSGiImportedPackages -- Retrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages. ratioUsedOSGiImportedPackages -- Retrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code. numberOSGiSplitImportedPackages -- Retrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest. numberOSGiSplitExportedPackages -- Retrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest.. unversionedOSGiRequiredBundles -- Retrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header. ratioUnversionedOSGiRequiredBundles -- Retrieves the ratio of unversioned OSGi required bundles. unversionedOSGiImportedPackages -- Retrieves the set of unversioned OSGi imported packages (declared in the Import-Package header). If returned value != {} there is a smell in the Manifest. ratioUnversionedOSGiImportedPackages -- Retrieves the ratio of unversioned OSGi imported packages. unversionedOSGiExportedPackages -- Retrieves the set of unversioned OSGi exported packages (declared in the Export-Package header). If returned value != {} there is a smell in the Manifest. ratioUnversionedOSGiExportedPackages -- Retrieves the ratio of unversioned OSGi exported packages. Maven metrics are defined in https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.maven/src/Maven.rsc List of metrics: allMavenDependencies -- Retrieves all the Maven dependencies. allOptionalMavenDependencies -- Retrieves all the optional Maven dependencies. numberMavenDependencies -- Retrieves the number of Maven dependencies. numberUniqueMavenDependencies -- Retrieves the number of unique Maven dependencies. ratioOptionalMavenDependencies -- Retrieves the ratio of optional Maven dependencies. isUsingTycho -- Checks if the current project is a Tycho project. IDE plugin All metrics are defined in metrics_definition_ide_plugin.docx scava-lib-usage -- Level of using CROSSMINER library change function. scava-search-usage -- Level of using CROSSMINER search function. scava-search-success -- Level of successfull free text serach using CROSSMINER plug-in. modifictaion-rate -- Rate of changes applied to document. gui-usage-rate -- Rate of activly using the GUI of Eclipse. testing-rate -- Count of executing tests. working-time -- Average working time for Java source files. file-access-rate -- Average count of Java source files open and brought to top. Natural Language Processing All metrics are defined in metrics_definition_nlp.docx . Issue tracking metrics: Number of bugs: It indicates how many bugs, through the time, are in total all the issue tracking systems related to a project. Number of comments: This metric indicates, through the time, the evolution in the quantity of comments for all the bugs related to a project. Emotions: This metric expresses, grosso modo, the emotions found in the issue tracker for a given project. Number new bugs: It indicates how many bugs are created every delta through a lapse of time. Number of new users: How many users are new in the bug tracking system. Open time: This metric shows the average open time of an issue. Number of patches: It indicates the number of patches located in the issue tracking system. Number of request and replies: From all the comments in the issue tracking system, how many of them are replies and how many are requests. Response time: It presents the average time to reply a comment in an issue tracker. Sentiments: Which sentiments are found in a issue tracking system for a project. Severity: The severity level of the bugs found in the issue tracker. This metric classifies a bug report into 1 of 7 severity levels such as Blocker, Critical, Major, Normal, Minor, Trivial or Enhancement. Status: It indicates the issues status, like open or closed. Topics: Using clustering methods, this metric indicates the topics that are discussed through the time. Unasnwered bugs: It indicates how many issues do not have any reply. Users: How many users are found in a issue tracker system. Newsgroup metrics Number of articles; It determines the number of articles found in a newsgroup. Emotions: Which emotions are found in the newgroup. Number of threads: How many threads a newsgroup contains. In other words, how many different branches have been created since the first email was sent to the newsgroup. Number of new users: How many users make use of the newsgroup. Number of request and replies: From the total number of emails belonging to the newsgroup, how many of them are request and how many are replies. Response time: The average time to reply a message. Sentiments: The sentiments found in the newsgroup. Severity: The severity level of the messages exchanges in the newsgroup. The severity is measured in terms of how severe an issue is. Number of new threads: How many threads are created per delta. Topics: Using the clustering algorithm, we determine which are the most frequent topics discussed in the newsgroup. Number of unanswered topics: How many messages haven\u2019t been answered. Number of users: Number of users that make use of the newsgroup. Forums metrics Number of posts: For every topic (i.e. forum thread), how many posts in total exist. Emotions: Which are the emotions located in the forum. Number of topics: How many forum threads contain the forum. Number of request and replies: From all the posts in the forum, how many are request and how many are reply. Sentiments: Which are the sentiments that can be found in a forum. Severity: In the case the forums are used to express issues, we can determine their severity. Number of new topics: Number of new forum threads created. Topics: Number of content topics, i.e. subjects, are discussed in the forum. Number of unanswered topics: Number of forums threads which do not have a reply. Configuration analysis metrics Metrics of Puppet are defined in detail in Deliverable 4.2 . Puppet Design Metrics: numberOfMultifacetedSmells -- The number of Multifaceted Abstraction smells numberOfUnnecessarySmells -- The number of Unnecessary Abstraction smells numberOfImperativeSmells -- The number of Imperative Abstraction smells numberOfMissAbSmells -- The number of Missing Abstraction smells numberOfInsufficientSmells -- The number of Insufficient Modularization smells numberOfUnstructuredSmells -- The number of Unstructured Module smells numberOfTightSmells -- The number of Tightly-coupled Module smells numberOfBrokenSmells -- The number of Broken Hierarchy smells numberOfMissingDepSmells -- The number of Missing Dependency smells numberOfHairballSmells -- The number of Hairball Structure smells numberOfDeficientSmells -- The number of Deficient Encapsulation smells numberOfWeakenSmells -- The number of Weaken Modularity smells cumulativeNumberOfDesignSmells -- The number of design smells Puppet Implementation Metrics: numberOfMissingDefaultCaseSmells -- Smell exists when a default case is missing in a case or selector statement. numberOfInconsistentNamingConventionSmells -- Smell exists when the used naming convention deviates from the recommended naming convention. numberOfComplexExpressionSmells -- Smell exists when a program contains a difficult to understand complex expression. numberOfDuplicateEntitySmells -- Smell exists duplicate parameters are present in the configuration code. numberOfMisplacedAttributeSmells -- Smell exists when attribute placement within a resource or a class has not followed a recommended order. numberOfImproperAlignmentSmells -- Smell exists when the code is not properly aligned or tabulation characters are used numberOfInvalidPropertyValueSmells -- Smell exists when an invalid value of a property or attribute is used. numberOfIncompleteTasksSmells -- Smell exists when the code has \u201cfixme\u201d and \u201ctodo\u201d tags indicating incomplete tasks. numberOfDeprecatedStatementUsageSmells -- Smell exists when the configuration code uses one of the deprecated statements. numberOfImproperQuoteUsageSmells -- Smell exists when single and double quotes are not used properly. numberOfLongStatementSmells -- Smell exists when the code contains long statements. numberOfIncompleteConditionalSmells -- Smell exists when an \u201cif..elseif\u201d construct used without a terminating \u201celse\u201d clause. numberOfUnguardedVariableSmells -- Smell exists when a variable is not enclosed in braces when being interpolated in a string. Docker Metrics are based on the following rules . Docker Metrics: numberOfUpgradeSmells -- Smell exists when apt-get or apk commands are used problematically. numberOfPinVersionSmells -- Smell exists when version of packages is not explicitly declared. numberOfUntaggedImageSmells -- Smell exists when version of images is not explicitly declared. numberOfSudoSmells -- Smell exists when sudo command and root user are used inappropriately. numberOfCopySmells -- Smell exists when COPY instruction is not used properly. numberOfFromSmells -- Smell exists when FROM instruction is not used properly. numberOfCmdSmells -- Smell exists when CMD and ENTRYPOINT instruction is not used properly. numberOfAddSmells -- Smell exists when ADD instruction is not used properly. numberOfMeaninglessCommandsSmells -- Smell exists when there are commands that makes no sense running them in a Docker container. numberOfInvalidPortsSmells -- Smell exists when invalid UNIX ports range is used. numberOfShellSmells -- Smell exists when SHELL instruction is not used when it should. All of the above metrics have their corresponding allOfSmells metrics (e.g. numberOfMultifacetedSmells -> allOfMultifacetedSmells) that lists the actual smells. Pattern and Anti-pattern Metrics: puppetPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a puppet file. puppetAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a puppet file. dockerPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a Dockerfile. dockerAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a Dockerfile. Components metrics: allDockerLibraries: Retrieves all the libraries that are installed with the use of apt-get install or apk install in Dockerfiles numberOfDockerLibraries: Retrieves the number of the above libraries allDockerImages: Retrieves all the images that are used with the use of FROM instruction in Dockerfiles numberOfDockerImages: Retrieves the number of the above images New versions Metrics: newVersionFound: The metric will list the new versions (if any) of libraries that are used in the project.","title":"Metrics computed by Scava"},{"location":"others/users/Scava-Metrics/#metrics-computed-by-scava","text":"","title":"Metrics computed by Scava"},{"location":"others/users/Scava-Metrics/#metric-platform","text":"OSGi metrics are defined in: https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.osgi/src/OSGi.rsc List of metrics: allOSGiBundleDependencies -- Retrieves all the OSGi bunlde dependencies (i.e. Require-Bundle dependencies). allOSGiPackageDependencies -- Retrieves all the OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). allOSGiDynamicImportedPackages -- Retrieves all the OSGi dynamically imported packages. If returned value != {} a smell exists in the Manifest file. numberOSGiPackageDependencies -- Retrieves the number of OSGi package dependencies (i.e. Import-Package and DynamicImport-Package dependencies). numberOSGiBundleDependencies -- Retrieves the number of OSGi bunlde dependencies (i.e. Require-Bundle dependencies). unusedOSGiImportedPackages -- Retrieves the set of unused OSGi imported packages. If set != {} then developers are importing more packages than needed (smell). ratioUnusedOSGiImportedPackages -- Retrieves the ratio of unused OSGi imported packages with regards to the whole set of imported and dynamically imported OSGi packages. ratioUsedOSGiImportedPackages -- Retrieves the ratio of used imported packages. If ratio == 0.0 all imported packages have been used in the project code. numberOSGiSplitImportedPackages -- Retrieves the number of split imported packages. If returned value > 0 there is a smell in the Manifest. numberOSGiSplitExportedPackages -- Retrieves the number of split exported packages. If returned value > 0 there is a smell in the Manifest.. unversionedOSGiRequiredBundles -- Retrieves the set of unversioned OSGi required bundles (declared in the Require-Bundle header. ratioUnversionedOSGiRequiredBundles -- Retrieves the ratio of unversioned OSGi required bundles. unversionedOSGiImportedPackages -- Retrieves the set of unversioned OSGi imported packages (declared in the Import-Package header). If returned value != {} there is a smell in the Manifest. ratioUnversionedOSGiImportedPackages -- Retrieves the ratio of unversioned OSGi imported packages. unversionedOSGiExportedPackages -- Retrieves the set of unversioned OSGi exported packages (declared in the Export-Package header). If returned value != {} there is a smell in the Manifest. ratioUnversionedOSGiExportedPackages -- Retrieves the ratio of unversioned OSGi exported packages. Maven metrics are defined in https://github.com/crossminer/scava/blob/master/metric-platform/metric-providers/org.eclipse.scava.metricprovider.trans.rascal.dependency.maven/src/Maven.rsc List of metrics: allMavenDependencies -- Retrieves all the Maven dependencies. allOptionalMavenDependencies -- Retrieves all the optional Maven dependencies. numberMavenDependencies -- Retrieves the number of Maven dependencies. numberUniqueMavenDependencies -- Retrieves the number of unique Maven dependencies. ratioOptionalMavenDependencies -- Retrieves the ratio of optional Maven dependencies. isUsingTycho -- Checks if the current project is a Tycho project.","title":"Metric-platform"},{"location":"others/users/Scava-Metrics/#ide-plugin","text":"All metrics are defined in metrics_definition_ide_plugin.docx scava-lib-usage -- Level of using CROSSMINER library change function. scava-search-usage -- Level of using CROSSMINER search function. scava-search-success -- Level of successfull free text serach using CROSSMINER plug-in. modifictaion-rate -- Rate of changes applied to document. gui-usage-rate -- Rate of activly using the GUI of Eclipse. testing-rate -- Count of executing tests. working-time -- Average working time for Java source files. file-access-rate -- Average count of Java source files open and brought to top.","title":"IDE plugin"},{"location":"others/users/Scava-Metrics/#natural-language-processing","text":"All metrics are defined in metrics_definition_nlp.docx . Issue tracking metrics: Number of bugs: It indicates how many bugs, through the time, are in total all the issue tracking systems related to a project. Number of comments: This metric indicates, through the time, the evolution in the quantity of comments for all the bugs related to a project. Emotions: This metric expresses, grosso modo, the emotions found in the issue tracker for a given project. Number new bugs: It indicates how many bugs are created every delta through a lapse of time. Number of new users: How many users are new in the bug tracking system. Open time: This metric shows the average open time of an issue. Number of patches: It indicates the number of patches located in the issue tracking system. Number of request and replies: From all the comments in the issue tracking system, how many of them are replies and how many are requests. Response time: It presents the average time to reply a comment in an issue tracker. Sentiments: Which sentiments are found in a issue tracking system for a project. Severity: The severity level of the bugs found in the issue tracker. This metric classifies a bug report into 1 of 7 severity levels such as Blocker, Critical, Major, Normal, Minor, Trivial or Enhancement. Status: It indicates the issues status, like open or closed. Topics: Using clustering methods, this metric indicates the topics that are discussed through the time. Unasnwered bugs: It indicates how many issues do not have any reply. Users: How many users are found in a issue tracker system. Newsgroup metrics Number of articles; It determines the number of articles found in a newsgroup. Emotions: Which emotions are found in the newgroup. Number of threads: How many threads a newsgroup contains. In other words, how many different branches have been created since the first email was sent to the newsgroup. Number of new users: How many users make use of the newsgroup. Number of request and replies: From the total number of emails belonging to the newsgroup, how many of them are request and how many are replies. Response time: The average time to reply a message. Sentiments: The sentiments found in the newsgroup. Severity: The severity level of the messages exchanges in the newsgroup. The severity is measured in terms of how severe an issue is. Number of new threads: How many threads are created per delta. Topics: Using the clustering algorithm, we determine which are the most frequent topics discussed in the newsgroup. Number of unanswered topics: How many messages haven\u2019t been answered. Number of users: Number of users that make use of the newsgroup. Forums metrics Number of posts: For every topic (i.e. forum thread), how many posts in total exist. Emotions: Which are the emotions located in the forum. Number of topics: How many forum threads contain the forum. Number of request and replies: From all the posts in the forum, how many are request and how many are reply. Sentiments: Which are the sentiments that can be found in a forum. Severity: In the case the forums are used to express issues, we can determine their severity. Number of new topics: Number of new forum threads created. Topics: Number of content topics, i.e. subjects, are discussed in the forum. Number of unanswered topics: Number of forums threads which do not have a reply.","title":"Natural Language Processing"},{"location":"others/users/Scava-Metrics/#configuration-analysis-metrics","text":"Metrics of Puppet are defined in detail in Deliverable 4.2 . Puppet Design Metrics: numberOfMultifacetedSmells -- The number of Multifaceted Abstraction smells numberOfUnnecessarySmells -- The number of Unnecessary Abstraction smells numberOfImperativeSmells -- The number of Imperative Abstraction smells numberOfMissAbSmells -- The number of Missing Abstraction smells numberOfInsufficientSmells -- The number of Insufficient Modularization smells numberOfUnstructuredSmells -- The number of Unstructured Module smells numberOfTightSmells -- The number of Tightly-coupled Module smells numberOfBrokenSmells -- The number of Broken Hierarchy smells numberOfMissingDepSmells -- The number of Missing Dependency smells numberOfHairballSmells -- The number of Hairball Structure smells numberOfDeficientSmells -- The number of Deficient Encapsulation smells numberOfWeakenSmells -- The number of Weaken Modularity smells cumulativeNumberOfDesignSmells -- The number of design smells Puppet Implementation Metrics: numberOfMissingDefaultCaseSmells -- Smell exists when a default case is missing in a case or selector statement. numberOfInconsistentNamingConventionSmells -- Smell exists when the used naming convention deviates from the recommended naming convention. numberOfComplexExpressionSmells -- Smell exists when a program contains a difficult to understand complex expression. numberOfDuplicateEntitySmells -- Smell exists duplicate parameters are present in the configuration code. numberOfMisplacedAttributeSmells -- Smell exists when attribute placement within a resource or a class has not followed a recommended order. numberOfImproperAlignmentSmells -- Smell exists when the code is not properly aligned or tabulation characters are used numberOfInvalidPropertyValueSmells -- Smell exists when an invalid value of a property or attribute is used. numberOfIncompleteTasksSmells -- Smell exists when the code has \u201cfixme\u201d and \u201ctodo\u201d tags indicating incomplete tasks. numberOfDeprecatedStatementUsageSmells -- Smell exists when the configuration code uses one of the deprecated statements. numberOfImproperQuoteUsageSmells -- Smell exists when single and double quotes are not used properly. numberOfLongStatementSmells -- Smell exists when the code contains long statements. numberOfIncompleteConditionalSmells -- Smell exists when an \u201cif..elseif\u201d construct used without a terminating \u201celse\u201d clause. numberOfUnguardedVariableSmells -- Smell exists when a variable is not enclosed in braces when being interpolated in a string. Docker Metrics are based on the following rules . Docker Metrics: numberOfUpgradeSmells -- Smell exists when apt-get or apk commands are used problematically. numberOfPinVersionSmells -- Smell exists when version of packages is not explicitly declared. numberOfUntaggedImageSmells -- Smell exists when version of images is not explicitly declared. numberOfSudoSmells -- Smell exists when sudo command and root user are used inappropriately. numberOfCopySmells -- Smell exists when COPY instruction is not used properly. numberOfFromSmells -- Smell exists when FROM instruction is not used properly. numberOfCmdSmells -- Smell exists when CMD and ENTRYPOINT instruction is not used properly. numberOfAddSmells -- Smell exists when ADD instruction is not used properly. numberOfMeaninglessCommandsSmells -- Smell exists when there are commands that makes no sense running them in a Docker container. numberOfInvalidPortsSmells -- Smell exists when invalid UNIX ports range is used. numberOfShellSmells -- Smell exists when SHELL instruction is not used when it should. All of the above metrics have their corresponding allOfSmells metrics (e.g. numberOfMultifacetedSmells -> allOfMultifacetedSmells) that lists the actual smells. Pattern and Anti-pattern Metrics: puppetPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a puppet file. puppetAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a puppet file. dockerPatternIntroduction: The metric will list the smells that removed between two consecutive versions of a Dockerfile. dockerAntipatternIntroduction: The metric will list the smells that introduced between two consecutive versions of a Dockerfile. Components metrics: allDockerLibraries: Retrieves all the libraries that are installed with the use of apt-get install or apk install in Dockerfiles numberOfDockerLibraries: Retrieves the number of the above libraries allDockerImages: Retrieves all the images that are used with the use of FROM instruction in Dockerfiles numberOfDockerImages: Retrieves the number of the above images New versions Metrics: newVersionFound: The metric will list the new versions (if any) of libraries that are used in the project.","title":"Configuration analysis metrics"},{"location":"others/users/Scava-Resources/","text":"Accessing Scava generated data Knowledge-base API description: http:// :8080/swagger-ui.html Get value of a metric for a single project: http:// :8182/projects/p/ /m/ List of metrics: http:// :8182/metrics List of factoids: http:// :8182/factoids","title":"Accessing Scava generated data"},{"location":"others/users/Scava-Resources/#accessing-scava-generated-data","text":"Knowledge-base API description: http:// :8080/swagger-ui.html Get value of a metric for a single project: http:// :8182/projects/p/ /m/ List of metrics: http:// :8182/metrics List of factoids: http:// :8182/factoids","title":"Accessing Scava generated data"},{"location":"user-guide/","text":"SCAVA User Guide The SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin. Quick Start Guide [TODO] Overivew Required Platform Administration and Project Analysis The SCAVA Administration application provide services to analyse open source software projects repository and provide several general administration feature including user managements services and platform configuration servicies. (Softeam) Visualisation Dashboard [TODO] Overivew Required Eclipse Plugin [TODO] Overivew Required Workflow Engine [TODO] Overivew Required Metrics Reference Guide [TODO] Overivew Required","title":"SCAVA User Guide"},{"location":"user-guide/#scava-user-guide","text":"The SCAVA user guide provide general instruction to analyse open sources repository using the platform, visualise the collected data using the visualisation dashboard and access to analysis service provided by the platform by the intermediary of the eclipse plugin.","title":"SCAVA User Guide"},{"location":"user-guide/#quick-start-guide","text":"[TODO] Overivew Required","title":"Quick Start Guide"},{"location":"user-guide/#platform-administration-and-project-analysis","text":"The SCAVA Administration application provide services to analyse open source software projects repository and provide several general administration feature including user managements services and platform configuration servicies. (Softeam)","title":"Platform Administration and Project Analysis"},{"location":"user-guide/#visualisation-dashboard","text":"[TODO] Overivew Required","title":"Visualisation Dashboard"},{"location":"user-guide/#eclipse-plugin","text":"[TODO] Overivew Required","title":"Eclipse Plugin"},{"location":"user-guide/#workflow-engine","text":"[TODO] Overivew Required","title":"Workflow Engine"},{"location":"user-guide/#metrics-reference-guide","text":"[TODO] Overivew Required","title":"Metrics Reference Guide"},{"location":"user-guide/administration/","text":"Platform Administration User Guide (Softeam)","title":"Platform Administration User Guide (Softeam)"},{"location":"user-guide/administration/#platform-administration-user-guide-softeam","text":"","title":"Platform Administration User Guide (Softeam)"},{"location":"user-guide/dashboard/","text":"Visualisation Dashboard","title":"Visualisation Dashboard"},{"location":"user-guide/dashboard/#visualisation-dashboard","text":"","title":"Visualisation Dashboard"},{"location":"user-guide/metrics/","text":"Metrics Reference Guide This guide contains the historic and trans metric providers for for bug trackers and newsgroups (including eclipse forum). Historic Metric Providers Historic metrics maintain a record of various heuristics associated with a specific open source project over its lifetime. They typically depend on the results from one or more transient metrics. Bug Trackers The following Historic Metric Providers are associated with Issue trackers org.eclipse.scava.metricprovider.historic.bugs.bugs Short name : historic.bugs.bugs Friendly name : Number of bugs per day per bug tracker This metric computes the number of bugs per day for each bug tracker seperately. It also computes additional information such as average comments per bug, average comments per user, average requests and/or replies per user and bug. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata , org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsBugsHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackerData> numberOfBugs int averageCommentsPerBug float averageRequestsPerBug float averageRepliesPerBug float averageCommentsPerUser float averageRequestsPerUser float averageRepliesPerUser float Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfBugs org.eclipse.scava.metricprovider.historic.bugs.comments Short name : historic.bugs.comments Friendly name : Number of bug comments per day per bug tracker This metric computes the number of bug comments submitted by the community (users) per day for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.comments Returns : BugsCommentsHistoricMetric which contains: Variable Type Bugs List<DailyBugData> numberOfComments int cumulativeNumberOfComments int Additional Information : DailyBugData : String bugTrackerID int numberOfComments int cumulativeNumberOfComments org.eclipse.scava.metricprovider.historic.bugs.emotions Short name : historic.bugs.emotions Friendly name : Number of emotions per day per bug tracker This metric computes the emotional dimensions present in bug comments submitted by the community (users) per day for each bug tracker. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise). Depends-on : org.eclipse.scava.metricprovider.trans.bugs.emotions Returns : BugsEmotionsHistoricMetric which contains: Variable Type bugData List<BugData> Dimensions List<Dimensions> Additional Information : BugData : String bugTrackerID int numberOfComments int cumulativeNumberOfComments Dimensions : String bugTrackerId String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfComments int cumulativeNumberOfComments float percentage float cumulativePercentage org.eclipse.scava.metricprovider.historic.bugs.newbugs Short name : historic.bugs.newbugs Friendly name : Number of new bugs per day per bug tracker This metric computes the number of new bugs reported by the community (users) per day for each bug tracker. A small number of bug reports can indicate either a bug-free, robust project or a project with a small/inactive user community. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.newbugs Returns : BugsNewBugsHistoricMetric which contains: Variable Type dailyBugData List<DailyBugData> numberOfBugs int cumulativeNumberOfBugs int Additional Information : DailyBugData : String bugTrackerId int numberOfBugs int cumulativeNumberOfBugs org.eclipse.scava.metricprovider.historic.bugs.newusers Short name : historic.bugs.newusers Friendly name : Number of new users per day per bug tracker This metric computes the number of new users per day for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsNewUsersHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackerData> numberOfNewUsers int cumulativeNumberOfNewUsers int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfNewUsers int cumulativeNumberOfNewUsers org.eclipse.scava.metricprovider.historic.bugs.opentime Short name : historic.bugs.opentime Friendly name : Average duration to close an open bug This metric computes the average duration between creating and closing bugs. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : OpenTimeHistoricMetricProvider which contains: Variable Type avgBugOpenTime String avgBugOpenTimeInDays double bugsConsidered int org.eclipse.scava.metricprovider.historic.bugs.patches Short name : historic.bugs.patches Friendly name : Number of bug patches per day This class computes the number of bug patches per day, for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.patches Returns : PatchesHistoricMetricProvider which contains: Variable Type numberOfPatches int cumulativeNumberOfPatches int bugs List<DailyBugData> Additional Information : DailyBugData : String bugTrackerId int numberOfPatches int cumulativeNumberOfPatches org.eclipse.scava.metricprovider.historic.bugs.requestsreplies Short name : historic.bugs.requestsreplies Friendly name : Number of request and replies in bug comments per bug tracker This metric computes the number of requests and replies realting to comments posted to bugs by the community (users) per day for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsRequestsRepliesHistoricMetric which contains: Variable Type Bugs List<DailyBugTrackerData> numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies org.eclipse.scava.metricprovider.historic.bugs.requestsreplies.average Short name : historic.bugs.requestsreplies.average Friendly name : Average number of requests and replies in bug comments per bug tracker This metric computes the average number of bug comments considered as request and reply for each bug tracker per day. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsRequestsRepliesHistoricMetric which contains: Variable Type bugs List<DailyBugTrackerData> numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies org.eclipse.scava.metricprovider.historic.bugs.responsetime Short name : historic.bugs.responsetime Friendly name : Average response time to open bugs per bug tracker This metric computes the average time in which the community (users) responds to open bugs per day for each bug tracker seperately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsResponseTimeHistoricMetric which contains: Variable Type bugTrackerId String avgResponseTimeFormatted String cumulativeAvgResponseTimeFormatted String avgResponseTime float cumulativeAvgResponseTime float bugsConsidered int cumulativeBugsConsidered int org.eclipse.scava.metricprovider.historic.bugs.sentiment Short name : historic.bugs.sentiment Friendly name : Overall sentiment per bug tracker This metric computes the overall sentiment per bug tracker up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score for each bug contributes equally, regardless of it's size. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSentimentHistoricMetric which contains: Variable Type overallAverageSentiment float overallSentimentAtThreadBeggining float overallSentimentAtThreadEnd float Additional Information : The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. org.eclipse.scava.metricprovider.historic.bugs.severity Short name : historic.bugs.severity Friendly name : Number of bugs per severity level per bug tracker This metric computes the number of severity levels for bugs submitted by the community (users) every day for each bug tracker. Specifically, it calculates the number and percentage of bugs that have been categorised into 1 of 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification Returns : BugsSeveritiesHistoricMetric which contains: Variable Type bugData List<BugData> severityLevels List<ServerityLevel> Additional Information : BugData : String bugTrackerId int numberOfBugs SeverityLevel : String bugTrackerId String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs int percentage org.eclipse.scava.metricprovider.historic.bugs.severitybugstatus Short name : historic.bugs.severitybugstatus Friendly name : Number of each bug status per bug severity level This metric computes the total number and percentage of each bug status per severity level, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate) and 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs int numberOfWontFixBugs int numberOfWorksForMeBugs int numberOfNonResolvedClosedBugs int numberOfInvalidBugs int numberOfFixedBugs int numberOfDuplicateBugs float percentageOfResolvedClosedBugs float percentageOfWontFixBugs float percentageOfWorksForMeBugs float percentageOfNonResolvedClosedBugs float percentageOfInvalidBugs float percentageOfFixedBugs float percentageOfDuplicateBugs org.eclipse.scava.metricprovider.historic.bugs.severityresponsetime Short name : historic.bugs.severityresponsetime Friendly name : Average response time to bugs per severity level per day This metric computes the average time in which the community (users) responds to open bugs per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) String avgResponseTimeFormatted int numberOfBugs long avgResponseTime org.eclipse.scava.metricprovider.historic.bugs.severitysentiment Short name : historic.bugs.severitysentiment Friendly name : Average sentiment per bugs severity level per day This metric computes for each bug severity level, the average sentiment, sentiment at the begining and end of bug comments posted by the community (users) every day for each bug tracker. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). There are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs float averageSentiment float sentimentAtThreadBeggining float sentimentAtThreadEnd The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. org.eclipse.scava.metricprovider.historic.bugs.status Short name : historic.bugs.status Friendly name : Number of bugs per bug status per day This metric computes the total number of bugs that corresponds to each bug status, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate). Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsStatusHistoricMetric which contains: Variable Type numberOfBugs long numberOfResolvedClosedBugs int numberOfWontFixBugs int numberOfWorksForMeBugs int numberOfNonResolvedClosedBugs int numberOfInvalidBugs int numberOfFixedBugs int numberOfDuplicateBugs int org.eclipse.scava.metricprovider.historic.bugs.topics Short name : historic.bugs.topics Friendly name : Labels of topics in bug comments per bug tracker This metric computes the labels of topics (thematic clusters) in bug comments submitted by the community (users), per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.topics Returns : BugsTopicsHistoricMetric which contains: Variable Type bugTopics List<BugTopic> Additional Information : SeverityLevel : String bugTrackerId String label float numberOfDocuments org.eclipse.scava.metricprovider.historic.bugs.unansweredbugs Short name : historic.bugs.unansweredbugs Friendly name : Number of unanswered bugs per day This metric computes the number of unanswered bugs per day. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsUnansweredBugsHistoricMetric which contains: Variable Type numberOfUnansweredBugs int org.eclipse.scava.metricprovider.historic.bugs.users Short name : historic.bugs.users Friendly name : Number of users, active and inactive per day per bug tracker This metric computes the number of users, number of active and inactive users per day for each bug tracker separately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsUsersHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackingData> numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers int Additional Information : DailyBugTrackingData : String bugTrackerId int numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers News Groups and Forums The following Historic Metric Providers are associated with newsgroups. org.eclipse.scava.metricprovider.historic.newsgroups.articles Short name : historic.newsgroups.articles Friendly name : Number of articles per day per news group This metric computes the number of articles submitted by the community (users) per day for each newsgroup separately Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.articles Returns : NewsgroupsArticlesHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfArticles int cummulativeNumberOfArticles org.eclipse.scava.metricprovider.historic.newsgroups.emotions Short name : historic.newsgroups.emotions Friendly name : Number of emotions per day per newsgroup This metric computes the emotional dimensions present in newsgroup comments submitted by the community (users) per day for each newsgroup. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise). Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.emotions Returns : NewsgroupsEmotionsHistoricMetric which contains: Variable Type newsgroupsData List<Newsgroups> emotionDimension List<Emotion> Additional Information : NewsgroupsData : String newsgroupName int numberOfArticles int cummulativeNumberOfArticles EmotionDimension : String newsgroupName String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfArticles int cumulativeNumberOfArticles float percentage float cumulativePercentage org.eclipse.scava.metricprovider.historic.newsgroups.newthreads Short name : historic.newsgroups.newthreads Friendly name : Number of new threads per day per newsgroup This metric computes the number of new threads submitted by the community (users) per day for each newsgroup separately Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : NewsgroupsNewThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfNewThreads int cummulativeNumberOfNewThreads org.eclipse.scava.metricprovider.historic.newsgroups.newusers Short name : historic.newsgroups.newusers Friendly name : Number of new users per day per newsgroup This metric computes the number of new users per day for each newsgroup seperately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsNewUsersHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfNewUsers int cummulativeNumberOfNewUsers org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies Short name : historic.newsgroups.requestsreplies Friendly name : Number of requests and replies in articles per day This metric computes the number of requests and replies in newsgroup articles submitted by the community (users) per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsRequestsRepliesHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies.average Short name : historic.newsgroups.requestsreplies.average Friendly name : Average number of articles, requests and replies per day This metric computes the average number of newsgroup articles, including the number of requests and replies within the newsgroup articles per day. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsRequestsRepliesHistoricMetric which contains: Variable Type averageArticlesPerDay float averageRequestsPerDay float averageRepliesPerDay float org.eclipse.scava.metricprovider.historic.newsgroups.responsetime Short name : historic.newsgroups.responsetime Friendly name : Average response time to threads per day per newsgroup This metric computes the average time in which the community responds to open threads per day for each newsgroup separately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsResponseTimeHistoricMetric which contains: Variable Type newsgroupName String avgResponseTime long avgResponseTimeFormatted String threadsConsidered int cumulativeAvgResponseTimeFormatted String cumulativeThreadsConsidered int org.eclipse.scava.metricprovider.historic.newsgroups.sentiment Short name : historic.newsgroups.sentiment Friendly name : Overall sentiment of newsgroup articles This metric computes the overall sentiment per newsgroup repository up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score of each thread contributes equally, irrespective of its size. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.sentiment Returns : SentimentClassificationTransMetric which contains: Variable Type overallAverageSentiment float overallSentimentAtThreadBegining float overallSentimentAtThreadEnd float The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. org.eclipse.scava.metricprovider.historic.newsgroups.severity Short name : historic.newsgroups.severity Friendly name : Number of each severity level in newsgroup threads per day This metric computes the number of each severity levels in threads submitted every day, per newsgroup. There are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification Returns : NewsgroupsSeveritiesHistoricMetric which contains: Variable Type newsgroupData List<Newsgroups> severityLevel List<SeverityLevel> Additional Information : NewsgroupData : String newsgroupName int numberThreads SeverityLevel : String newsgroupName String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads float percentage org.eclipse.scava.metricprovider.historic.newsgroups.severityresponsetime Short name : historic.newsgroups.severityresponsetime Friendly name : Average response time to threads per severity level per day This metric computes the average time in which the community (users) responds to open threads per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Average response time to threads per severity level This metric computes the average response time for newsgroup threads submitted every day, based on their severity levels. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsSeverityResponseTimeHistoricMetric which contains: Variable Type severityLevel List<SeverityLevel> Additional Information : SeverityLevel : String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads long avgResponseTime String avgResponseTimeFormatted org.eclipse.scava.metricprovider.historic.newsgroups.severitysentiment Short name : historic.newsgroups.severitysentiment Friendly name : Average sentiment in threads per severity level per day This metric computes the average sentiment, the sentiment at the beginning of threads and the sentiment at the end of threads; for each severity level in newsgroup threads submitted every day. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). Note: there are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.newsgroups.sentiment Returns : NewsgroupsSeveritySentimentHistoricMetric which contains: Variable Type severityLevel List<SeverityLevel> Additional Information : SeverityLevel : String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads float avgSentiment float avgSentimentThreadBeginning float avgSentimentThreadEnd The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. org.eclipse.scava.metricprovider.historic.newsgroups.threads Short name : historic.newsgroups.threads Friendly name : Number of threads per day per newsgroup This metric computes the number of threads per day for each newsgroup separately. The metric also computes average values for articles per thread, requests per thread, replies per thread, articles per user, requests per user and replies per user. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfThreads float averageArticlesPerThread float averageRequestsPerThread float averageRepliesPerThread float averageArticlesPerUser float averageRequestsPerUser float averageRepliesPerUser org.eclipse.scava.metricprovider.historic.newsgroups.topics Short name : historic.newsgroups.topics Friendly name : Labels of newsgroup topics per newsgroup This metric computes the labels of topics (thematic clusters) in articles submitted by the community (users), for each newsgroup seperately. Depends-on : org.eclipse.scava.metricprovider.trans.topics Returns : NewsgroupTopicsHistoricMetric which contains: Variable Type newsgroupTopic List<NewsgrpTopic> Additional Information : NewsgroupTopic : String newsgroupName String label int numberOfDocuments org.eclipse.scava.metricprovider.historic.newsgroups.unansweredthreads Short name : historic.newsgroups.unansweredthreads Friendly name : Number of unanswered threads per day per newsgroup This metric computes the number of unanswered threads per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsUnansweredThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfUnansweredThreads org.eclipse.scava.metricprovider.historic.newsgroups.users Short name : historic.newsgroups.users Friendly name : Number of users, active and inactive per day per newsgroup This metric computes the number of users, including active and inactive users per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsUsersHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers Transient Metric Providers Transient metrics are used to calculate heuristics that are associated with a particular period in time, i.e. a single day. Transient Metrics are stored temporarily within the knowledge base and their output is passed as parameters in the calculation of other transient and historic metrics. Depending on the complexity, a transient metric can depend on the output from other tools, other transient metircs or have no dependencies at all. Bug Trackers The following Transient Metric Providers are associated with Issue trackers. org.eclipse.scava.metricprovider.trans.bugs.activeusers Short name : trans.bugs.activeusers Friendly name : Number of users with new bug comment in the last 15 days This metric computes the number of users that submitted new bug comments in the last 15 days, for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsActiveUsersTransMetric which contains: Variable Type bugs List<BugsData> users List<User> Additional Information : BugData : String bugTrackerId int activeUsers int inactiveUsers int previousUsers int users int days User : String bugTrackerId String userId String lastActivityDate int comments int requests int replies org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Short name : trans.bugs.bugmetadata Friendly name : Bug header metadata This metric computes various metadata in bug header, i.e. priority, status, operation system and resolution. Other values computed by this metric includes average sentiment, content class and requests/replies. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification , org.eclipse.scava.metricprovider.trans.sentimentclassification , org.eclipse.scava.metricprovider.trans.detectingcode Returns : BugsBugMetadataTransMetric which contains: Variable Type BugData List<BugData> CommentData List<CommentData> Additional Information : BugData : String bugTrackerId String bugId String status String resolution String operatingSystem String priority String creationTime String lastClosedTime String startSentiment String endSentiment float averageSentiment CommentData : String bugTrackerId String bugId String commentId String creationTime String creator String contentClass String requestReplyPrediction org.eclipse.scava.metricprovider.trans.bugs.comments Short name : trans.bugs.comments Friendly name : Number of bug comments This metric computes the number of bug comments, per bug tracker. Depends-on : None Returns : BugsCommentsTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> Additional Information : BugTrackerData: String bugTrackerId int numberOfComments int cumulativeNumberOfComments org.eclipse.scava.metricprovider.trans.bugs.contentclasses Short name : trans.bugs.contentclasses Friendly name : Content classes in bug comments This metric computes the frequency and percentage of content Classes in bug comments, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsContentClassesTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> contentClasses List<ContentClass> Additional Information : BugTrackerData : String bugTrackerId int numberOfComments ContentClass : String bugTrackerId String classLabel int numberOfComments float percentage org.eclipse.scava.metricprovider.trans.bugs.dailyrequestsreplies Short name : trans.bugs.dailyrequestsreplies Friendly name : Number of bug comments, requests and replies per day This metric computes the number of bug comments, including those regarded as requests and replies each day, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsDailyRequestsRepliesTransMetric which contains: Variable Type dayComments List<DayComments> Additional Information : DayComments : String name String bugTrackerId int numberOfComments int numberOfRequests int numberOfReplies float percentageOfComments float percentageOfRequests float percentageOfReplies org.eclipse.scava.metricprovider.trans.bugs.emotions Short name : trans.bugs.emotions Friendly name : Emotions in bug comments This metric computes the emotional dimensions in bug comments, per bug tracker. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.emotionclassification Returns : BugsEmotionsTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> dimensions List<EmotionDimension> Additional Information : BugTrackerData : String bugTrackerId int numberOfComments int cumulativeNumberOfComments EmotionDimension : String bugTrackerId String emotionLabel ( anger , fear , joy , sadness , love , surprise`) int numberOfComments int cumulativeNumberOfComments float percentage float cumulativePercentage org.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies Short name : trans.bugs.hourlyrequestsreplies Friendly name : Number of bug comments, requests and replies per hour This metric computes the number of bug comments, including those regarded as requests and replies, every hour of the day, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsHourlyRequestsRepliesTransMetric which contains: Variable Type hourComments List<HourComments> Additional Information : HourComments : type bugTrackerId type hour type numberOfComments type numberOfRequests type numberOfReplies type percentageOfComments type percentageOfRequests type percentageOfReplies org.eclipse.scava.metricprovider.trans.bugs.newbugs Short name : trans.bugs.newbugs Friendly name : Number of new bugs This metric computes the number of new bugs over time, per bug tracker. Depends-on : None Returns : BugsNewBugsTransMetric which contains: Variable Type bugTrackerData type Additional Information : BugTrackerData : String bugTrackerId int numberOfBugs int cumulativeNumberOfBugs org.eclipse.scava.metricprovider.trans.bugs.patches Short name : trans.bugs.patches Friendly name : Number of patches per bug This metric computes the number of patches submitted by the community (users) for each bug. Depends-on : None Returns : PatchesTransMetricProvider which contains: Variable Type bugTrackerData type Additional Information : BugTrackerData : String bugTrackerId int numberOfPatches int cumulativeNumberOfPatches org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Short name : trans.bugs.requestreplies Friendly name : Bug statistics (answered?, response time) This metric computes for each bug, whether it was answered. If so, it computes the time taken to respond. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsRequestsRepliesTransMetric which contains: Variable Type bugs List<BugStatistics> Additional Information : BugStatistics : String bugTrackerId String bugId boolean firstRequest boolean answered long responseDurationSec String responseDate News Groups and Forums The following Transient Metric Providers are associated with newsgroups. org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Short name : trans.newsgroups.activeusers Friendly name : Number of users with new comment in the last 15 days This metric computes the number of users that submitted news comments in the last 15 days, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsActiveUsersTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> user List<User> Additional Information : NewsgroupData : String newsgroupName int activeUsers int inactiveUsers int previousUsers int users int days User : String newsgroupName String userId String lastActiveDate int articles int requests int replies org.eclipse.scava.metricprovider.trans.newsgroups.articles Short name : trans.newsgroups.articles Friendly name : Number of articles per newsgroup This metric computes the number of articles, per newsgroup. Depends-on : None Returns : NewsgroupsArticlesTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> Additional Information : NewsgroupData : String newsgroupName int numberOfArticles int cumulativeNumberOfArticles org.eclipse.scava.metricprovider.trans.newsgroups.contentclasses Short name : trans.newsgroups.contentclasses Friendly name : Content classes in newsgroup articles This metric computes the content classes in newgroup articles, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : NewsgroupsContentClassesTransMetric which contains: Variable Type newsGroupData List<NewsgroupData> contentClass List< ContentClass> Additional Information : NewsGroupData: String newsgroupName int numberOfArticles ContentClass: String newsgroupName String classLabel int numberOfArticles float percentage org.eclipse.scava.metricprovider.trans.newsgroups.dailyrequestsreplies Short name : trans.newsgroups.dailyrequestsreplies Friendly name : Number of articles, requests and replies per day This metric computes the number of articles, including those regarded as requests and replies for each day of the week, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsDailyRequestsRepliesTransMetric which contains: Variable Type dailyArticles List<DailyArticles> Additional Information : DailyArticles : String name int numberOfArticles int numberOfRequests int numberOfReplies float percentageOfArticles float percentageOfRequests float percentageOfReplies org.eclipse.scava.metricprovider.trans.newsgroups.emotions Short name : trans.newsgroups.emotions Friendly name : Emotions in newsgroup articles This metric computes the emotional dimensions in newsgroup articles, per newsgroup. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.emotionclassification Returns : NewsgroupsEmotionsTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> emotionDimension List<EmotionDimension> Additional Information : NewsgroupData : String newsgroupName int numberOfArticles int cumulativeNumberOfArticles EmotionDimension : String newsgroupName String emotionLabel ( anger , fear , joy , sadness , love , surprise`) int numberOfArticles int cumulativeNumberOfArticles float percentage float cumulativePercentage org.eclipse.scava.metricprovider.trans.newsgroups.hourlyrequestsreplies Short name : trans.newsgroups.hourlyrequestsreplies Friendly name : Number of articles, requests and replies per hour This metric computes the number of articles, including those regarded as requests and replies for each hour of the day, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsHourlyRequestsRepliesTransMetric which contains: Variable Type hourArticles List<HourArticles> Additional Information : HourArticles : String hour int numberOfArticles int numberOfRequests int numberOfReplies float percentageOfArticles float percentageOfRequests float percentageOfReplies org.eclipse.scava.metricprovider.trans.newsgroups.sentiment Short name : trans.newsgroups.sentiment Friendly name : Average sentiment in newsgroup threads The metric computes the average sentiment, including sentiment at the beginning and end of each thread, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.sentimentclassification Returns : NewsgroupsSentimentTransMetric which contains: Variable Type threadStatistics List<ThreadStatistics> Additional Information : ThreadStatistics : String newsgroupName int threadId float averageSentiment String startSentiment String endSentiment The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment. org.eclipse.scava.metricprovider.trans.newsgroups.threads Short name : trans.newsgroups.threads Friendly name : Assigns newsgroup articles to threads This metric holds information for assigning newsgroup articles to threads. The threading algorithm is executed from scratch every time. Depends-on : None , Returns : NewsgroupsThreadsTransMetric which contains: Variable Type articleData List<ArticleData> threadData List<ThreadData> newsgroupData List<NewsgroupData> currentDate List<CurrentDate> Additional Information : ArticleData : String newsgroupName int articleNumber String articlesId String date String from String subject String contentClass String references ThreadData : int threadId NewsgroupData : String newsgroupName int threads int previousThreads CurrentDate : String date org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Short name : trans.newsgroups.threadsrequestsreplies Friendly name : Thread statistics (answered?, response time) The metric computes for each thread whether it is answered. If so, it computes the response time. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsSentimentTransMetric which contains: Variable Type threadStatistics List<ThreadStatistics> Additional Information : ThreadStatistics : String newsgroupName int threadId boolean firstRequest boolean answered long responseDurationSec String responseDate Natrual Language Processing The following Transient Metric Providers are associated with Natural Language Processing . org.eclipse.scava.metricprovider.trans.detectingcode Short name : trans.detectingcode Friendly name : Distinguishes between code and natural language This metric determines the parts of a bug comment or a newsgroup article that contains code or natural language. Depends-on : org.eclipse.scava.metricprovider.trans.plaintextprocessing Returns : DetectingCodeTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerCommentDetectingCode> newsgroupArticles List<NewsgroupArticleDetectingCode> forumPosts List<ForumPostsDetectingCode> Additional Information : BugTrackerCommentDetectingCode : String bugTrackerId String bugId String commentId String naturalLanguage String code NewsgroupArticleDetectingCode : String newsgroupName String articleNumber String naturalLanguage String code ForumPostsDetectingCode : String forumId String topicId String postId String naturalLanguage String code org.eclipse.scava.metricprovider.trans.emotionclassification Short name : trans.emotionclassification Friendly name : Emotion classifier This metric computes the emotions present in each bug comment, newsgroup article or forum post. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : EmotionClassificationTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerCommentEmotionClassification> newsgroupArticles List<NewsgroupArticleEmotionClassification> forumPosts List<ForumPostsEmotionClassification> Additional Information : BugTrackerCommentEmotionClassification : String bugTrackerId String bugId String commentId String emotions NewsgroupArticleEmotionClassification : String newsgroupName String articleNumber String emotions ForumPostsEmotionClassification : String forumId String topicId String postId String emotions org.eclipse.scava.metricprovider.trans.plaintextprocessing Short name : trans.plaintextprocessing Friendly name : Plain text processing This metric preprocess each bug comment, newsgroup article or forum post into a split plain text format. Depends-on : None Returns : PlainTextProcessingTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerCommentPlainTextProcessing> newsgroupArticles List<NewsgroupArticlePlainTextProcessing> forumPosts List<ForumPostsPlainTextProcessing> Additional Information : BugTrackerCommentPlainTextProcessing : String bugTrackerId String bugId String commentId String plainText boolean hadReplies NewsgroupArticlePlainTextProcessing : String newsgroupName String articleNumber String plainText boolean hadReplies ForumPostsPlainTextProcessing : String forumId String topicId String postId String plainText boolean hadReplies org.eclipse.scava.metricprovider.trans.requestreplyclassification Short name : trans.requestreplyclassification Friendly name : Request/Reply classification This metric computes if a bug comment, newsgroup article or forum post is a request of a reply. Depends-on : org.eclipse.scava.metricprovider.trans.plaintextprocessing , org.eclipse.scava.metricprovider.trans.detectingcode Returns : RequestReplyClassificationTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerComments> newsgroupArticles List<NewsgroupArticles> forumPosts List<ForumPosts> Additional Information : BugTrackerComments : String bugTrackerId String bugId String commentId String classificationResult String date NewsgroupArticles : String newsgroupName String articleNumber String classificationResult String date ForumPosts : String forumId String topicId String postId String classificationResult String date org.eclipse.scava.metricprovider.trans.sentimentclassification Short name : trans.sentimentclassification Friendly name : Sentiment classification This metric computes the sentiment of each bug comment, newsgroup article or forum post. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or 1 (positive sentiment). Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : SentimentClassificationTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerCommentsSentimentClassification> newsgroupArticles List<NewsgroupArticlesSentimentClassification> forumPosts List<ForumPostSentimentClassification> Additional Information : BugTrackerCommentsSentimentClassification : String bugTrackerId String bugId String commentId String polarity ( negative (-1) , neutral (0) or positive (1) ) NewsgroupArticlesSentimentClassification : String newsgroupName String articleNumber String polarity ( negative (-1) , neutral (0) or positive (1) ) ForumPostSentimentClassification : String forumId String topicId String postId String polarity ( negative (-1) , neutral (0) or positive (1) ) org.eclipse.scava.metricprovider.trans.severityclassification Short name : trans.severityclassification Friendly name : Severity classification This metric computes the severity of each bug comment, newsgroup article or forum post. Severity could be blocker, critical, major, minor, enhancement, normal). For bug comments, there is an additional severity level called unknown . A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode , org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : SeverityClassificationTransMetricProvider which contains: Variable Type bugTrackerBugs List<BugTrackerBugsData> newsgroupArticles List<NewsgroupArticleData> newsgroupThreads List<NewsgroupThreadData> forumPosts ForumPostData> Additional Information : BugTrackerBugsData : String bugTrackerId String bugId String severity int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams NewsgroupArticleData : String NewsgroupName long articleNumber int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams NewsgroupThreadData : String newsgroupName int threadId String severity BugTrackerBugsData : String forumId String topicId String severity int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams org.eclipse.scava.metricprovider.trans.topics Short name : trans.topics Friendly name : Topic clustering This metric computes topic clusters for each bug comment, newsgroup article or forum post in the last 30 days. Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : TopicsTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerCommentsData> bugTrackerTopics List<BugTrackerTopic> newsgroupArticles List<NewsgroupArticlesData> newsgroupTopics List<NewsgroupTopic> forumPosts List<ForumPostData> forumTopics List<ForumPostTopic> Additional Information : BugTrackerCommentsData : String bugTrackerId String bugId String commentId String subject String text String date NewsgroupArticlesData : String newsgroupName long articleNumber String subject String text String date ForumPostData : String forumId String topicId String postId String subject String text String date BugTrackerTopic : String bugTrackerId String label int numberOfDocuments NewsgroupTopic : String newsgroupName String label int numberOfDocuments ForumPostTopic : String forumId String label int numberOfDocuments","title":"Metrics Reference Guide"},{"location":"user-guide/metrics/#metrics-reference-guide","text":"This guide contains the historic and trans metric providers for for bug trackers and newsgroups (including eclipse forum).","title":"Metrics Reference Guide"},{"location":"user-guide/metrics/#historic-metric-providers","text":"Historic metrics maintain a record of various heuristics associated with a specific open source project over its lifetime. They typically depend on the results from one or more transient metrics.","title":"Historic Metric Providers"},{"location":"user-guide/metrics/#bug-trackers","text":"The following Historic Metric Providers are associated with Issue trackers","title":"Bug Trackers"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsbugs","text":"Short name : historic.bugs.bugs Friendly name : Number of bugs per day per bug tracker This metric computes the number of bugs per day for each bug tracker seperately. It also computes additional information such as average comments per bug, average comments per user, average requests and/or replies per user and bug. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata , org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsBugsHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackerData> numberOfBugs int averageCommentsPerBug float averageRequestsPerBug float averageRepliesPerBug float averageCommentsPerUser float averageRequestsPerUser float averageRepliesPerUser float Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfBugs","title":"org.eclipse.scava.metricprovider.historic.bugs.bugs"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugscomments","text":"Short name : historic.bugs.comments Friendly name : Number of bug comments per day per bug tracker This metric computes the number of bug comments submitted by the community (users) per day for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.comments Returns : BugsCommentsHistoricMetric which contains: Variable Type Bugs List<DailyBugData> numberOfComments int cumulativeNumberOfComments int Additional Information : DailyBugData : String bugTrackerID int numberOfComments int cumulativeNumberOfComments","title":"org.eclipse.scava.metricprovider.historic.bugs.comments"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsemotions","text":"Short name : historic.bugs.emotions Friendly name : Number of emotions per day per bug tracker This metric computes the emotional dimensions present in bug comments submitted by the community (users) per day for each bug tracker. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise). Depends-on : org.eclipse.scava.metricprovider.trans.bugs.emotions Returns : BugsEmotionsHistoricMetric which contains: Variable Type bugData List<BugData> Dimensions List<Dimensions> Additional Information : BugData : String bugTrackerID int numberOfComments int cumulativeNumberOfComments Dimensions : String bugTrackerId String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfComments int cumulativeNumberOfComments float percentage float cumulativePercentage","title":"org.eclipse.scava.metricprovider.historic.bugs.emotions"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsnewbugs","text":"Short name : historic.bugs.newbugs Friendly name : Number of new bugs per day per bug tracker This metric computes the number of new bugs reported by the community (users) per day for each bug tracker. A small number of bug reports can indicate either a bug-free, robust project or a project with a small/inactive user community. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.newbugs Returns : BugsNewBugsHistoricMetric which contains: Variable Type dailyBugData List<DailyBugData> numberOfBugs int cumulativeNumberOfBugs int Additional Information : DailyBugData : String bugTrackerId int numberOfBugs int cumulativeNumberOfBugs","title":"org.eclipse.scava.metricprovider.historic.bugs.newbugs"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsnewusers","text":"Short name : historic.bugs.newusers Friendly name : Number of new users per day per bug tracker This metric computes the number of new users per day for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsNewUsersHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackerData> numberOfNewUsers int cumulativeNumberOfNewUsers int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfNewUsers int cumulativeNumberOfNewUsers","title":"org.eclipse.scava.metricprovider.historic.bugs.newusers"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsopentime","text":"Short name : historic.bugs.opentime Friendly name : Average duration to close an open bug This metric computes the average duration between creating and closing bugs. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : OpenTimeHistoricMetricProvider which contains: Variable Type avgBugOpenTime String avgBugOpenTimeInDays double bugsConsidered int","title":"org.eclipse.scava.metricprovider.historic.bugs.opentime"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugspatches","text":"Short name : historic.bugs.patches Friendly name : Number of bug patches per day This class computes the number of bug patches per day, for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.patches Returns : PatchesHistoricMetricProvider which contains: Variable Type numberOfPatches int cumulativeNumberOfPatches int bugs List<DailyBugData> Additional Information : DailyBugData : String bugTrackerId int numberOfPatches int cumulativeNumberOfPatches","title":"org.eclipse.scava.metricprovider.historic.bugs.patches"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsrequestsreplies","text":"Short name : historic.bugs.requestsreplies Friendly name : Number of request and replies in bug comments per bug tracker This metric computes the number of requests and replies realting to comments posted to bugs by the community (users) per day for each bug tracker seperately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsRequestsRepliesHistoricMetric which contains: Variable Type Bugs List<DailyBugTrackerData> numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies","title":"org.eclipse.scava.metricprovider.historic.bugs.requestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsrequestsrepliesaverage","text":"Short name : historic.bugs.requestsreplies.average Friendly name : Average number of requests and replies in bug comments per bug tracker This metric computes the average number of bug comments considered as request and reply for each bug tracker per day. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsRequestsRepliesHistoricMetric which contains: Variable Type bugs List<DailyBugTrackerData> numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies int Additional Information : DailyBugTrackerData : String bugTrackerId int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies","title":"org.eclipse.scava.metricprovider.historic.bugs.requestsreplies.average"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsresponsetime","text":"Short name : historic.bugs.responsetime Friendly name : Average response time to open bugs per bug tracker This metric computes the average time in which the community (users) responds to open bugs per day for each bug tracker seperately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsResponseTimeHistoricMetric which contains: Variable Type bugTrackerId String avgResponseTimeFormatted String cumulativeAvgResponseTimeFormatted String avgResponseTime float cumulativeAvgResponseTime float bugsConsidered int cumulativeBugsConsidered int","title":"org.eclipse.scava.metricprovider.historic.bugs.responsetime"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugssentiment","text":"Short name : historic.bugs.sentiment Friendly name : Overall sentiment per bug tracker This metric computes the overall sentiment per bug tracker up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score for each bug contributes equally, regardless of it's size. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSentimentHistoricMetric which contains: Variable Type overallAverageSentiment float overallSentimentAtThreadBeggining float overallSentimentAtThreadEnd float Additional Information : The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment.","title":"org.eclipse.scava.metricprovider.historic.bugs.sentiment"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseverity","text":"Short name : historic.bugs.severity Friendly name : Number of bugs per severity level per bug tracker This metric computes the number of severity levels for bugs submitted by the community (users) every day for each bug tracker. Specifically, it calculates the number and percentage of bugs that have been categorised into 1 of 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification Returns : BugsSeveritiesHistoricMetric which contains: Variable Type bugData List<BugData> severityLevels List<ServerityLevel> Additional Information : BugData : String bugTrackerId int numberOfBugs SeverityLevel : String bugTrackerId String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs int percentage","title":"org.eclipse.scava.metricprovider.historic.bugs.severity"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseveritybugstatus","text":"Short name : historic.bugs.severitybugstatus Friendly name : Number of each bug status per bug severity level This metric computes the total number and percentage of each bug status per severity level, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate) and 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs int numberOfWontFixBugs int numberOfWorksForMeBugs int numberOfNonResolvedClosedBugs int numberOfInvalidBugs int numberOfFixedBugs int numberOfDuplicateBugs float percentageOfResolvedClosedBugs float percentageOfWontFixBugs float percentageOfWorksForMeBugs float percentageOfNonResolvedClosedBugs float percentageOfInvalidBugs float percentageOfFixedBugs float percentageOfDuplicateBugs","title":"org.eclipse.scava.metricprovider.historic.bugs.severitybugstatus"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseverityresponsetime","text":"Short name : historic.bugs.severityresponsetime Friendly name : Average response time to bugs per severity level per day This metric computes the average time in which the community (users) responds to open bugs per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) String avgResponseTimeFormatted int numberOfBugs long avgResponseTime","title":"org.eclipse.scava.metricprovider.historic.bugs.severityresponsetime"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsseveritysentiment","text":"Short name : historic.bugs.severitysentiment Friendly name : Average sentiment per bugs severity level per day This metric computes for each bug severity level, the average sentiment, sentiment at the begining and end of bug comments posted by the community (users) every day for each bug tracker. Sentiment score could be closer to -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). There are 8 severity levels (blocker, critical, major, minor, enhancement, normal, trivial, unknown). A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsSeverityBugStatusHistoricMetric which contains: Variable Type severityLevels List<SeverityLevel> Additional Information : SeverityLevel : String severityLevel ( blocker , critical , major , minor , enhancement , normal , trivial , unknown ) int numberOfBugs float averageSentiment float sentimentAtThreadBeggining float sentimentAtThreadEnd The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment.","title":"org.eclipse.scava.metricprovider.historic.bugs.severitysentiment"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsstatus","text":"Short name : historic.bugs.status Friendly name : Number of bugs per bug status per day This metric computes the total number of bugs that corresponds to each bug status, in bugs submitted every day, per bug tracker. There are 7 bug status (ResolvedClosed, WontFix, WorksForMe, NonResolvedClosed, Invalid, Fixed, Duplicate). Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsStatusHistoricMetric which contains: Variable Type numberOfBugs long numberOfResolvedClosedBugs int numberOfWontFixBugs int numberOfWorksForMeBugs int numberOfNonResolvedClosedBugs int numberOfInvalidBugs int numberOfFixedBugs int numberOfDuplicateBugs int","title":"org.eclipse.scava.metricprovider.historic.bugs.status"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugstopics","text":"Short name : historic.bugs.topics Friendly name : Labels of topics in bug comments per bug tracker This metric computes the labels of topics (thematic clusters) in bug comments submitted by the community (users), per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.topics Returns : BugsTopicsHistoricMetric which contains: Variable Type bugTopics List<BugTopic> Additional Information : SeverityLevel : String bugTrackerId String label float numberOfDocuments","title":"org.eclipse.scava.metricprovider.historic.bugs.topics"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsunansweredbugs","text":"Short name : historic.bugs.unansweredbugs Friendly name : Number of unanswered bugs per day This metric computes the number of unanswered bugs per day. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.requestsreplies Returns : BugsUnansweredBugsHistoricMetric which contains: Variable Type numberOfUnansweredBugs int","title":"org.eclipse.scava.metricprovider.historic.bugs.unansweredbugs"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricbugsusers","text":"Short name : historic.bugs.users Friendly name : Number of users, active and inactive per day per bug tracker This metric computes the number of users, number of active and inactive users per day for each bug tracker separately. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.activeusers Returns : BugsUsersHistoricMetric which contains: Variable Type bugTrackers List<DailyBugTrackingData> numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers int Additional Information : DailyBugTrackingData : String bugTrackerId int numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers","title":"org.eclipse.scava.metricprovider.historic.bugs.users"},{"location":"user-guide/metrics/#news-groups-and-forums","text":"The following Historic Metric Providers are associated with newsgroups.","title":"News Groups and Forums"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsarticles","text":"Short name : historic.newsgroups.articles Friendly name : Number of articles per day per news group This metric computes the number of articles submitted by the community (users) per day for each newsgroup separately Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.articles Returns : NewsgroupsArticlesHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfArticles int cummulativeNumberOfArticles","title":"org.eclipse.scava.metricprovider.historic.newsgroups.articles"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsemotions","text":"Short name : historic.newsgroups.emotions Friendly name : Number of emotions per day per newsgroup This metric computes the emotional dimensions present in newsgroup comments submitted by the community (users) per day for each newsgroup. Emotion can be 1 of 6 (anger, fear, joy, sadness, love or surprise). Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.emotions Returns : NewsgroupsEmotionsHistoricMetric which contains: Variable Type newsgroupsData List<Newsgroups> emotionDimension List<Emotion> Additional Information : NewsgroupsData : String newsgroupName int numberOfArticles int cummulativeNumberOfArticles EmotionDimension : String newsgroupName String emotionLabel ( anger , fear , joy , sadness , love , surprise ) int numberOfArticles int cumulativeNumberOfArticles float percentage float cumulativePercentage","title":"org.eclipse.scava.metricprovider.historic.newsgroups.emotions"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsnewthreads","text":"Short name : historic.newsgroups.newthreads Friendly name : Number of new threads per day per newsgroup This metric computes the number of new threads submitted by the community (users) per day for each newsgroup separately Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : NewsgroupsNewThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfNewThreads int cummulativeNumberOfNewThreads","title":"org.eclipse.scava.metricprovider.historic.newsgroups.newthreads"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsnewusers","text":"Short name : historic.newsgroups.newusers Friendly name : Number of new users per day per newsgroup This metric computes the number of new users per day for each newsgroup seperately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsNewUsersHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfNewUsers int cummulativeNumberOfNewUsers","title":"org.eclipse.scava.metricprovider.historic.newsgroups.newusers"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsrequestsreplies","text":"Short name : historic.newsgroups.requestsreplies Friendly name : Number of requests and replies in articles per day This metric computes the number of requests and replies in newsgroup articles submitted by the community (users) per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsRequestsRepliesHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfRequests int numberOfReplies int cumulativeNumberOfRequests int cumulativeNumberOfReplies","title":"org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsrequestsrepliesaverage","text":"Short name : historic.newsgroups.requestsreplies.average Friendly name : Average number of articles, requests and replies per day This metric computes the average number of newsgroup articles, including the number of requests and replies within the newsgroup articles per day. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsRequestsRepliesHistoricMetric which contains: Variable Type averageArticlesPerDay float averageRequestsPerDay float averageRepliesPerDay float","title":"org.eclipse.scava.metricprovider.historic.newsgroups.requestsreplies.average"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsresponsetime","text":"Short name : historic.newsgroups.responsetime Friendly name : Average response time to threads per day per newsgroup This metric computes the average time in which the community responds to open threads per day for each newsgroup separately. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsResponseTimeHistoricMetric which contains: Variable Type newsgroupName String avgResponseTime long avgResponseTimeFormatted String threadsConsidered int cumulativeAvgResponseTimeFormatted String cumulativeThreadsConsidered int","title":"org.eclipse.scava.metricprovider.historic.newsgroups.responsetime"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupssentiment","text":"Short name : historic.newsgroups.sentiment Friendly name : Overall sentiment of newsgroup articles This metric computes the overall sentiment per newsgroup repository up to the processing date. The overall sentiment score could be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). In the computation, the sentiment score of each thread contributes equally, irrespective of its size. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.sentiment Returns : SentimentClassificationTransMetric which contains: Variable Type overallAverageSentiment float overallSentimentAtThreadBegining float overallSentimentAtThreadEnd float The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment.","title":"org.eclipse.scava.metricprovider.historic.newsgroups.sentiment"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsseverity","text":"Short name : historic.newsgroups.severity Friendly name : Number of each severity level in newsgroup threads per day This metric computes the number of each severity levels in threads submitted every day, per newsgroup. There are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification Returns : NewsgroupsSeveritiesHistoricMetric which contains: Variable Type newsgroupData List<Newsgroups> severityLevel List<SeverityLevel> Additional Information : NewsgroupData : String newsgroupName int numberThreads SeverityLevel : String newsgroupName String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads float percentage","title":"org.eclipse.scava.metricprovider.historic.newsgroups.severity"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsseverityresponsetime","text":"Short name : historic.newsgroups.severityresponsetime Friendly name : Average response time to threads per severity level per day This metric computes the average time in which the community (users) responds to open threads per severity level per day for each bug tracker. Format: dd:HH:mm:ss:SS, where dd=days, HH:hours, mm=minutes, ss:seconds, SS=milliseconds. Note: there are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Average response time to threads per severity level This metric computes the average response time for newsgroup threads submitted every day, based on their severity levels. Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsSeverityResponseTimeHistoricMetric which contains: Variable Type severityLevel List<SeverityLevel> Additional Information : SeverityLevel : String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads long avgResponseTime String avgResponseTimeFormatted","title":"org.eclipse.scava.metricprovider.historic.newsgroups.severityresponsetime"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsseveritysentiment","text":"Short name : historic.newsgroups.severitysentiment Friendly name : Average sentiment in threads per severity level per day This metric computes the average sentiment, the sentiment at the beginning of threads and the sentiment at the end of threads; for each severity level in newsgroup threads submitted every day. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or +1 (positive sentiment). Note: there are 7 severity levels (blocker, critical, major, minor, enhancement, normal, trivial). Depends-on : org.eclipse.scava.metricprovider.trans.severityclassification , org.eclipse.scava.metricprovider.trans.newsgroups.sentiment Returns : NewsgroupsSeveritySentimentHistoricMetric which contains: Variable Type severityLevel List<SeverityLevel> Additional Information : SeverityLevel : String severityLabel ( blocker , critical , major , minor , enhancement , normal , trivial ) int numberOfThreads float avgSentiment float avgSentimentThreadBeginning float avgSentimentThreadEnd The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment.","title":"org.eclipse.scava.metricprovider.historic.newsgroups.severitysentiment"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsthreads","text":"Short name : historic.newsgroups.threads Friendly name : Number of threads per day per newsgroup This metric computes the number of threads per day for each newsgroup separately. The metric also computes average values for articles per thread, requests per thread, replies per thread, articles per user, requests per user and replies per user. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfThreads float averageArticlesPerThread float averageRequestsPerThread float averageRepliesPerThread float averageArticlesPerUser float averageRequestsPerUser float averageRepliesPerUser","title":"org.eclipse.scava.metricprovider.historic.newsgroups.threads"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupstopics","text":"Short name : historic.newsgroups.topics Friendly name : Labels of newsgroup topics per newsgroup This metric computes the labels of topics (thematic clusters) in articles submitted by the community (users), for each newsgroup seperately. Depends-on : org.eclipse.scava.metricprovider.trans.topics Returns : NewsgroupTopicsHistoricMetric which contains: Variable Type newsgroupTopic List<NewsgrpTopic> Additional Information : NewsgroupTopic : String newsgroupName String label int numberOfDocuments","title":"org.eclipse.scava.metricprovider.historic.newsgroups.topics"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsunansweredthreads","text":"Short name : historic.newsgroups.unansweredthreads Friendly name : Number of unanswered threads per day per newsgroup This metric computes the number of unanswered threads per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies Returns : NewsgroupsUnansweredThreadsHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfUnansweredThreads","title":"org.eclipse.scava.metricprovider.historic.newsgroups.unansweredthreads"},{"location":"user-guide/metrics/#orgeclipsescavametricproviderhistoricnewsgroupsusers","text":"Short name : historic.newsgroups.users Friendly name : Number of users, active and inactive per day per newsgroup This metric computes the number of users, including active and inactive users per day for each newsgroup separately. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.activeusers Returns : NewsgroupsUsersHistoricMetric which contains: Variable Type dailyNewsgroupData List<DailyNewsgroupData> Additional Information : DailyNewsgroupData : String newsgroupName int numberOfUsers int numberOfActiveUsers int numberOfInactiveUsers","title":"org.eclipse.scava.metricprovider.historic.newsgroups.users"},{"location":"user-guide/metrics/#transient-metric-providers","text":"Transient metrics are used to calculate heuristics that are associated with a particular period in time, i.e. a single day. Transient Metrics are stored temporarily within the knowledge base and their output is passed as parameters in the calculation of other transient and historic metrics. Depending on the complexity, a transient metric can depend on the output from other tools, other transient metircs or have no dependencies at all.","title":"Transient Metric Providers"},{"location":"user-guide/metrics/#bug-trackers_1","text":"The following Transient Metric Providers are associated with Issue trackers.","title":"Bug Trackers"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsactiveusers","text":"Short name : trans.bugs.activeusers Friendly name : Number of users with new bug comment in the last 15 days This metric computes the number of users that submitted new bug comments in the last 15 days, for each bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsActiveUsersTransMetric which contains: Variable Type bugs List<BugsData> users List<User> Additional Information : BugData : String bugTrackerId int activeUsers int inactiveUsers int previousUsers int users int days User : String bugTrackerId String userId String lastActivityDate int comments int requests int replies","title":"org.eclipse.scava.metricprovider.trans.bugs.activeusers"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsbugmetadata","text":"Short name : trans.bugs.bugmetadata Friendly name : Bug header metadata This metric computes various metadata in bug header, i.e. priority, status, operation system and resolution. Other values computed by this metric includes average sentiment, content class and requests/replies. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification , org.eclipse.scava.metricprovider.trans.sentimentclassification , org.eclipse.scava.metricprovider.trans.detectingcode Returns : BugsBugMetadataTransMetric which contains: Variable Type BugData List<BugData> CommentData List<CommentData> Additional Information : BugData : String bugTrackerId String bugId String status String resolution String operatingSystem String priority String creationTime String lastClosedTime String startSentiment String endSentiment float averageSentiment CommentData : String bugTrackerId String bugId String commentId String creationTime String creator String contentClass String requestReplyPrediction","title":"org.eclipse.scava.metricprovider.trans.bugs.bugmetadata"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugscomments","text":"Short name : trans.bugs.comments Friendly name : Number of bug comments This metric computes the number of bug comments, per bug tracker. Depends-on : None Returns : BugsCommentsTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> Additional Information : BugTrackerData: String bugTrackerId int numberOfComments int cumulativeNumberOfComments","title":"org.eclipse.scava.metricprovider.trans.bugs.comments"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugscontentclasses","text":"Short name : trans.bugs.contentclasses Friendly name : Content classes in bug comments This metric computes the frequency and percentage of content Classes in bug comments, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsContentClassesTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> contentClasses List<ContentClass> Additional Information : BugTrackerData : String bugTrackerId int numberOfComments ContentClass : String bugTrackerId String classLabel int numberOfComments float percentage","title":"org.eclipse.scava.metricprovider.trans.bugs.contentclasses"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsdailyrequestsreplies","text":"Short name : trans.bugs.dailyrequestsreplies Friendly name : Number of bug comments, requests and replies per day This metric computes the number of bug comments, including those regarded as requests and replies each day, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsDailyRequestsRepliesTransMetric which contains: Variable Type dayComments List<DayComments> Additional Information : DayComments : String name String bugTrackerId int numberOfComments int numberOfRequests int numberOfReplies float percentageOfComments float percentageOfRequests float percentageOfReplies","title":"org.eclipse.scava.metricprovider.trans.bugs.dailyrequestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsemotions","text":"Short name : trans.bugs.emotions Friendly name : Emotions in bug comments This metric computes the emotional dimensions in bug comments, per bug tracker. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.emotionclassification Returns : BugsEmotionsTransMetric which contains: Variable Type bugTrackerData List<BugTrackerData> dimensions List<EmotionDimension> Additional Information : BugTrackerData : String bugTrackerId int numberOfComments int cumulativeNumberOfComments EmotionDimension : String bugTrackerId String emotionLabel ( anger , fear , joy , sadness , love , surprise`) int numberOfComments int cumulativeNumberOfComments float percentage float cumulativePercentage","title":"org.eclipse.scava.metricprovider.trans.bugs.emotions"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugshourlyrequestsreplies","text":"Short name : trans.bugs.hourlyrequestsreplies Friendly name : Number of bug comments, requests and replies per hour This metric computes the number of bug comments, including those regarded as requests and replies, every hour of the day, per bug tracker. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : BugsHourlyRequestsRepliesTransMetric which contains: Variable Type hourComments List<HourComments> Additional Information : HourComments : type bugTrackerId type hour type numberOfComments type numberOfRequests type numberOfReplies type percentageOfComments type percentageOfRequests type percentageOfReplies","title":"org.eclipse.scava.metricprovider.trans.bugs.hourlyrequestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsnewbugs","text":"Short name : trans.bugs.newbugs Friendly name : Number of new bugs This metric computes the number of new bugs over time, per bug tracker. Depends-on : None Returns : BugsNewBugsTransMetric which contains: Variable Type bugTrackerData type Additional Information : BugTrackerData : String bugTrackerId int numberOfBugs int cumulativeNumberOfBugs","title":"org.eclipse.scava.metricprovider.trans.bugs.newbugs"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugspatches","text":"Short name : trans.bugs.patches Friendly name : Number of patches per bug This metric computes the number of patches submitted by the community (users) for each bug. Depends-on : None Returns : PatchesTransMetricProvider which contains: Variable Type bugTrackerData type Additional Information : BugTrackerData : String bugTrackerId int numberOfPatches int cumulativeNumberOfPatches","title":"org.eclipse.scava.metricprovider.trans.bugs.patches"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransbugsrequestsreplies","text":"Short name : trans.bugs.requestreplies Friendly name : Bug statistics (answered?, response time) This metric computes for each bug, whether it was answered. If so, it computes the time taken to respond. Depends-on : org.eclipse.scava.metricprovider.trans.bugs.bugmetadata Returns : BugsRequestsRepliesTransMetric which contains: Variable Type bugs List<BugStatistics> Additional Information : BugStatistics : String bugTrackerId String bugId boolean firstRequest boolean answered long responseDurationSec String responseDate","title":"org.eclipse.scava.metricprovider.trans.bugs.requestsreplies"},{"location":"user-guide/metrics/#news-groups-and-forums_1","text":"The following Transient Metric Providers are associated with newsgroups.","title":"News Groups and Forums"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsactiveusers","text":"Short name : trans.newsgroups.activeusers Friendly name : Number of users with new comment in the last 15 days This metric computes the number of users that submitted news comments in the last 15 days, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsActiveUsersTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> user List<User> Additional Information : NewsgroupData : String newsgroupName int activeUsers int inactiveUsers int previousUsers int users int days User : String newsgroupName String userId String lastActiveDate int articles int requests int replies","title":"org.eclipse.scava.metricprovider.trans.newsgroups.activeusers"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsarticles","text":"Short name : trans.newsgroups.articles Friendly name : Number of articles per newsgroup This metric computes the number of articles, per newsgroup. Depends-on : None Returns : NewsgroupsArticlesTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> Additional Information : NewsgroupData : String newsgroupName int numberOfArticles int cumulativeNumberOfArticles","title":"org.eclipse.scava.metricprovider.trans.newsgroups.articles"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupscontentclasses","text":"Short name : trans.newsgroups.contentclasses Friendly name : Content classes in newsgroup articles This metric computes the content classes in newgroup articles, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : NewsgroupsContentClassesTransMetric which contains: Variable Type newsGroupData List<NewsgroupData> contentClass List< ContentClass> Additional Information : NewsGroupData: String newsgroupName int numberOfArticles ContentClass: String newsgroupName String classLabel int numberOfArticles float percentage","title":"org.eclipse.scava.metricprovider.trans.newsgroups.contentclasses"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsdailyrequestsreplies","text":"Short name : trans.newsgroups.dailyrequestsreplies Friendly name : Number of articles, requests and replies per day This metric computes the number of articles, including those regarded as requests and replies for each day of the week, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsDailyRequestsRepliesTransMetric which contains: Variable Type dailyArticles List<DailyArticles> Additional Information : DailyArticles : String name int numberOfArticles int numberOfRequests int numberOfReplies float percentageOfArticles float percentageOfRequests float percentageOfReplies","title":"org.eclipse.scava.metricprovider.trans.newsgroups.dailyrequestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsemotions","text":"Short name : trans.newsgroups.emotions Friendly name : Emotions in newsgroup articles This metric computes the emotional dimensions in newsgroup articles, per newsgroup. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.emotionclassification Returns : NewsgroupsEmotionsTransMetric which contains: Variable Type newsgroupData List<NewsgroupData> emotionDimension List<EmotionDimension> Additional Information : NewsgroupData : String newsgroupName int numberOfArticles int cumulativeNumberOfArticles EmotionDimension : String newsgroupName String emotionLabel ( anger , fear , joy , sadness , love , surprise`) int numberOfArticles int cumulativeNumberOfArticles float percentage float cumulativePercentage","title":"org.eclipse.scava.metricprovider.trans.newsgroups.emotions"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupshourlyrequestsreplies","text":"Short name : trans.newsgroups.hourlyrequestsreplies Friendly name : Number of articles, requests and replies per hour This metric computes the number of articles, including those regarded as requests and replies for each hour of the day, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsHourlyRequestsRepliesTransMetric which contains: Variable Type hourArticles List<HourArticles> Additional Information : HourArticles : String hour int numberOfArticles int numberOfRequests int numberOfReplies float percentageOfArticles float percentageOfRequests float percentageOfReplies","title":"org.eclipse.scava.metricprovider.trans.newsgroups.hourlyrequestsreplies"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupssentiment","text":"Short name : trans.newsgroups.sentiment Friendly name : Average sentiment in newsgroup threads The metric computes the average sentiment, including sentiment at the beginning and end of each thread, per newsgroup. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.sentimentclassification Returns : NewsgroupsSentimentTransMetric which contains: Variable Type threadStatistics List<ThreadStatistics> Additional Information : ThreadStatistics : String newsgroupName int threadId float averageSentiment String startSentiment String endSentiment The sentiment related variables above all represent a Polarity value. A polarity value closer to: -1 indicates negative sentiment, closer to 0 indicates neutral sentiment and closer to 1 indicates positive sentiment.","title":"org.eclipse.scava.metricprovider.trans.newsgroups.sentiment"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsthreads","text":"Short name : trans.newsgroups.threads Friendly name : Assigns newsgroup articles to threads This metric holds information for assigning newsgroup articles to threads. The threading algorithm is executed from scratch every time. Depends-on : None , Returns : NewsgroupsThreadsTransMetric which contains: Variable Type articleData List<ArticleData> threadData List<ThreadData> newsgroupData List<NewsgroupData> currentDate List<CurrentDate> Additional Information : ArticleData : String newsgroupName int articleNumber String articlesId String date String from String subject String contentClass String references ThreadData : int threadId NewsgroupData : String newsgroupName int threads int previousThreads CurrentDate : String date","title":"org.eclipse.scava.metricprovider.trans.newsgroups.threads"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransnewsgroupsthreadsrequestsreplies","text":"Short name : trans.newsgroups.threadsrequestsreplies Friendly name : Thread statistics (answered?, response time) The metric computes for each thread whether it is answered. If so, it computes the response time. Depends-on : org.eclipse.scava.metricprovider.trans.newsgroups.threads , org.eclipse.scava.metricprovider.trans.requestreplyclassification Returns : NewsgroupsSentimentTransMetric which contains: Variable Type threadStatistics List<ThreadStatistics> Additional Information : ThreadStatistics : String newsgroupName int threadId boolean firstRequest boolean answered long responseDurationSec String responseDate","title":"org.eclipse.scava.metricprovider.trans.newsgroups.threadsrequestsreplies"},{"location":"user-guide/metrics/#natrual-language-processing","text":"The following Transient Metric Providers are associated with Natural Language Processing .","title":"Natrual Language Processing"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransdetectingcode","text":"Short name : trans.detectingcode Friendly name : Distinguishes between code and natural language This metric determines the parts of a bug comment or a newsgroup article that contains code or natural language. Depends-on : org.eclipse.scava.metricprovider.trans.plaintextprocessing Returns : DetectingCodeTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerCommentDetectingCode> newsgroupArticles List<NewsgroupArticleDetectingCode> forumPosts List<ForumPostsDetectingCode> Additional Information : BugTrackerCommentDetectingCode : String bugTrackerId String bugId String commentId String naturalLanguage String code NewsgroupArticleDetectingCode : String newsgroupName String articleNumber String naturalLanguage String code ForumPostsDetectingCode : String forumId String topicId String postId String naturalLanguage String code","title":"org.eclipse.scava.metricprovider.trans.detectingcode"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransemotionclassification","text":"Short name : trans.emotionclassification Friendly name : Emotion classifier This metric computes the emotions present in each bug comment, newsgroup article or forum post. There are 6 emotion labels (anger, fear, joy, sadness, love, surprise). Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : EmotionClassificationTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerCommentEmotionClassification> newsgroupArticles List<NewsgroupArticleEmotionClassification> forumPosts List<ForumPostsEmotionClassification> Additional Information : BugTrackerCommentEmotionClassification : String bugTrackerId String bugId String commentId String emotions NewsgroupArticleEmotionClassification : String newsgroupName String articleNumber String emotions ForumPostsEmotionClassification : String forumId String topicId String postId String emotions","title":"org.eclipse.scava.metricprovider.trans.emotionclassification"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransplaintextprocessing","text":"Short name : trans.plaintextprocessing Friendly name : Plain text processing This metric preprocess each bug comment, newsgroup article or forum post into a split plain text format. Depends-on : None Returns : PlainTextProcessingTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerCommentPlainTextProcessing> newsgroupArticles List<NewsgroupArticlePlainTextProcessing> forumPosts List<ForumPostsPlainTextProcessing> Additional Information : BugTrackerCommentPlainTextProcessing : String bugTrackerId String bugId String commentId String plainText boolean hadReplies NewsgroupArticlePlainTextProcessing : String newsgroupName String articleNumber String plainText boolean hadReplies ForumPostsPlainTextProcessing : String forumId String topicId String postId String plainText boolean hadReplies","title":"org.eclipse.scava.metricprovider.trans.plaintextprocessing"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransrequestreplyclassification","text":"Short name : trans.requestreplyclassification Friendly name : Request/Reply classification This metric computes if a bug comment, newsgroup article or forum post is a request of a reply. Depends-on : org.eclipse.scava.metricprovider.trans.plaintextprocessing , org.eclipse.scava.metricprovider.trans.detectingcode Returns : RequestReplyClassificationTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerComments> newsgroupArticles List<NewsgroupArticles> forumPosts List<ForumPosts> Additional Information : BugTrackerComments : String bugTrackerId String bugId String commentId String classificationResult String date NewsgroupArticles : String newsgroupName String articleNumber String classificationResult String date ForumPosts : String forumId String topicId String postId String classificationResult String date","title":"org.eclipse.scava.metricprovider.trans.requestreplyclassification"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertranssentimentclassification","text":"Short name : trans.sentimentclassification Friendly name : Sentiment classification This metric computes the sentiment of each bug comment, newsgroup article or forum post. Sentiment can be -1 (negative sentiment), 0 (neutral sentiment) or 1 (positive sentiment). Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : SentimentClassificationTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerCommentsSentimentClassification> newsgroupArticles List<NewsgroupArticlesSentimentClassification> forumPosts List<ForumPostSentimentClassification> Additional Information : BugTrackerCommentsSentimentClassification : String bugTrackerId String bugId String commentId String polarity ( negative (-1) , neutral (0) or positive (1) ) NewsgroupArticlesSentimentClassification : String newsgroupName String articleNumber String polarity ( negative (-1) , neutral (0) or positive (1) ) ForumPostSentimentClassification : String forumId String topicId String postId String polarity ( negative (-1) , neutral (0) or positive (1) )","title":"org.eclipse.scava.metricprovider.trans.sentimentclassification"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertransseverityclassification","text":"Short name : trans.severityclassification Friendly name : Severity classification This metric computes the severity of each bug comment, newsgroup article or forum post. Severity could be blocker, critical, major, minor, enhancement, normal). For bug comments, there is an additional severity level called unknown . A bug severity is considered unknown if there is not enough information for the classifier to make a decision. For example, an unanswered bug with no user comment to analyse. Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode , org.eclipse.scava.metricprovider.trans.newsgroups.threads Returns : SeverityClassificationTransMetricProvider which contains: Variable Type bugTrackerBugs List<BugTrackerBugsData> newsgroupArticles List<NewsgroupArticleData> newsgroupThreads List<NewsgroupThreadData> forumPosts ForumPostData> Additional Information : BugTrackerBugsData : String bugTrackerId String bugId String severity int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams NewsgroupArticleData : String NewsgroupName long articleNumber int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams NewsgroupThreadData : String newsgroupName int threadId String severity BugTrackerBugsData : String forumId String topicId String severity int unigrams int bigrams int trigrams int quadgrams int charTrigrams int charQuadgrams int charFivegrams","title":"org.eclipse.scava.metricprovider.trans.severityclassification"},{"location":"user-guide/metrics/#orgeclipsescavametricprovidertranstopics","text":"Short name : trans.topics Friendly name : Topic clustering This metric computes topic clusters for each bug comment, newsgroup article or forum post in the last 30 days. Depends-on : org.eclipse.scava.metricprovider.trans.detectingcode Returns : TopicsTransMetricProvider which contains: Variable Type bugTrackerComments List<BugTrackerCommentsData> bugTrackerTopics List<BugTrackerTopic> newsgroupArticles List<NewsgroupArticlesData> newsgroupTopics List<NewsgroupTopic> forumPosts List<ForumPostData> forumTopics List<ForumPostTopic> Additional Information : BugTrackerCommentsData : String bugTrackerId String bugId String commentId String subject String text String date NewsgroupArticlesData : String newsgroupName long articleNumber String subject String text String date ForumPostData : String forumId String topicId String postId String subject String text String date BugTrackerTopic : String bugTrackerId String label int numberOfDocuments NewsgroupTopic : String newsgroupName String label int numberOfDocuments ForumPostTopic : String forumId String label int numberOfDocuments","title":"org.eclipse.scava.metricprovider.trans.topics"},{"location":"user-guide/plugin/","text":"Eclipse Plugin","title":"Eclipse Plugin"},{"location":"user-guide/plugin/#eclipse-plugin","text":"","title":"Eclipse Plugin"},{"location":"user-guide/quickstart/","text":"Quick Start Guide \u201cHello world example\u201d with the aim of presenting an overall picture consisting of the steps presented here https://docs.google.com/presentation/d/1mugPrIqs6JqJhMmVt4x2cjKfpSxP-wwQFOCL9aWyV5I/edit#slide=id.g3a59249d20_2_211","title":"Quick Start Guide"},{"location":"user-guide/quickstart/#quick-start-guide","text":"\u201cHello world example\u201d with the aim of presenting an overall picture consisting of the steps presented here https://docs.google.com/presentation/d/1mugPrIqs6JqJhMmVt4x2cjKfpSxP-wwQFOCL9aWyV5I/edit#slide=id.g3a59249d20_2_211","title":"Quick Start Guide"},{"location":"user-guide/workflow/","text":"Workflow Engine","title":"Workflow Engine"},{"location":"user-guide/workflow/#workflow-engine","text":"","title":"Workflow Engine"}]}